{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2022 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdIzhk1eKt5k"
   },
   "source": [
    "# Intermediate classification with Kaggle data using TF-DF\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/decision_forests/tutorials/kaggle_intermediate_example_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/decision-forests/blob/main/documentation/tutorials/kaggle_intermediate_example_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/decision-forests/blob/main/documentation/tutorials/kaggle_intermediate_example_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/decision-forests/documentation/tutorials/kaggle_intermediate_example_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKaV2aZOT8Q0"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v1x0sj6k7nL"
   },
   "source": [
    "To run this notebook, you need to have a Kaggle account.\n",
    "\n",
    "If you do not have an account, create one here: [Kaggle Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F) \n",
    "\n",
    "Read through the [Authentication Section](https://www.kaggle.com/docs/api#authentication) of the Kaggle API documentation to get a key for the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "MY7To2jpMj_x"
   },
   "outputs": [],
   "source": [
    "#@title Enter your Kaggle token in order to fetch the dataset\n",
    "\n",
    "username = '' #@param {type:\"string\"}\n",
    "key = '' #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "W3qcScNeT1G2"
   },
   "outputs": [],
   "source": [
    "#@title Configure Kaggle\n",
    "try:\n",
    "  from google.colab import files, drive\n",
    "\n",
    "  # Install and Configure Kaggle\n",
    "  import json\n",
    "\n",
    "  token = {\n",
    "    \"username\":username,\n",
    "    \"key\":key\n",
    "  }\n",
    "\n",
    "  # Installing kaggle\n",
    "  !pip install kaggle &> /dev/null\n",
    "\n",
    "  # Creating .kaggle if necessary\n",
    "  !if [ -d .kaggle ]; then echo \".kaggle exists\"; else echo \".kaggle does not exist ... Creating it\"; mkdir .kaggle; if [ -d .kaggle ]; then echo \"Successfully created\"; else echo \"Error creating .kaggle\"; fi; fi\n",
    "\n",
    "  with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
    "      json.dump(token, file)\n",
    "\n",
    "  # Creating .kaggle if necessary\n",
    "  !if [ -d  ~/.kaggle ]; then echo \" ~/.kaggle exists\"; else echo \" ~/.kaggle does not exist ... Creating it\"; mkdir  ~/.kaggle; if [ -d  ~/.kaggle ]; then echo \"Successfully created\"; else echo \"Error creating  ~/.kaggle\"; fi; fi\n",
    "  !cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
    "\n",
    "  # kaggle configuration\n",
    "  !kaggle config set -n path -v{/content}\n",
    "\n",
    "  # Changing mode\n",
    "  !chmod 600 /root/.kaggle/kaggle.json\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "haNXIxSBUEXI"
   },
   "outputs": [],
   "source": [
    "#@title Download Dataset\n",
    "import os\n",
    "\n",
    "DOWNLOAD_LOCATION = \"/root/Downloads/\"\n",
    "\n",
    "if os.path.exists(DOWNLOAD_LOCATION):\n",
    "    if os.path.isdir(DOWNLOAD_LOCATION):\n",
    "        print(\"{} exists and is a directory\".format(DOWNLOAD_LOCATION))\n",
    "    else:\n",
    "        print(\"{} exists but is not a directory!!!\".format(DOWNLOAD_LOCATION))\n",
    "else:\n",
    "    print(\"{} does not exist ... Creating it\".format(DOWNLOAD_LOCATION))\n",
    "    os.makedirs(DOWNLOAD_LOCATION)\n",
    "\n",
    "# Downloading\n",
    "!kaggle competitions download -c tabular-playground-series-sep-2021 -p {DOWNLOAD_LOCATION}\n",
    "\n",
    "# Extracting archives\n",
    "!cd {DOWNLOAD_LOCATION}; unzip -qq \\*.zip; rm -f *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "X728nqEoHk8U"
   },
   "outputs": [],
   "source": [
    "#@title Install TensorFlow Decision Forests\n",
    "!pip install tensorflow_decision_forests -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "givUtNyHSVe_"
   },
   "outputs": [],
   "source": [
    "#@title User Input Configuration\n",
    "\n",
    "rnd_seed =  42#@param {type:\"number\"}\n",
    "validation_ratio = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jACDpm0GUyXQ"
   },
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx3jmgamUgS3",
    "outputId": "1b823264-e90e-468b-d5f5-bb523b6a7687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.9.1\n",
      "TensorFlow Decision Forests: 0.2.7\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "from IPython.display import Javascript\n",
    "\n",
    "print(\"TensorFlow Version: {}\".format(tf.__version__))\n",
    "print(\"TensorFlow Decision Forests: {}\".format(tfdf.__version__))\n",
    "\n",
    "# Some of the model training logs can cover the full\n",
    "# screen if not compressed to a smaller viewport.\n",
    "# This magic allows setting a max height for a cell.\n",
    "@register_line_magic\n",
    "def set_cell_height(size):\n",
    "  display(\n",
    "      Javascript(\"google.colab.output.setIframeHeight(0, true, {maxHeight: \" +\n",
    "                 str(size) + \"})\"))\n",
    "\n",
    "np.random.seed(rnd_seed)\n",
    "tf.random.set_seed(rnd_seed)\n",
    "\n",
    "VALID_RATIO = validation_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsqy7G9hU1zT"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyuhaIxQUuFk",
    "outputId": "41a5705b-b47a-4b47-e4ba-58dcdc992020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset shape is (957919, 120)\n"
     ]
    }
   ],
   "source": [
    "train_file_path = os.path.join(DOWNLOAD_LOCATION, \"train.csv\")\n",
    "train_full_data = pd.read_csv(train_file_path)\n",
    "print(\"Full train dataset shape is {}\".format(train_full_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "1o4jEv_LI5B8",
    "outputId": "9693cfb0-7bb1-43a2-ed42-70bc150e1378"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-25d7e119-269d-4efd-ac92-690e1cf059ec\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f110</th>\n",
       "      <th>f111</th>\n",
       "      <th>f112</th>\n",
       "      <th>f113</th>\n",
       "      <th>f114</th>\n",
       "      <th>f115</th>\n",
       "      <th>f116</th>\n",
       "      <th>f117</th>\n",
       "      <th>f118</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10859</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>-37.566</td>\n",
       "      <td>0.017364</td>\n",
       "      <td>0.28915</td>\n",
       "      <td>-10.25100</td>\n",
       "      <td>135.12</td>\n",
       "      <td>168900.0</td>\n",
       "      <td>3.992400e+14</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.2280</td>\n",
       "      <td>1.7482</td>\n",
       "      <td>1.90960</td>\n",
       "      <td>-7.11570</td>\n",
       "      <td>4378.80</td>\n",
       "      <td>1.2096</td>\n",
       "      <td>8.613400e+14</td>\n",
       "      <td>140.1</td>\n",
       "      <td>1.01770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.299610</td>\n",
       "      <td>11822.000</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.45970</td>\n",
       "      <td>-0.83733</td>\n",
       "      <td>1721.90</td>\n",
       "      <td>119810.0</td>\n",
       "      <td>3.874100e+15</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.7580</td>\n",
       "      <td>4.1684</td>\n",
       "      <td>0.34808</td>\n",
       "      <td>4.14200</td>\n",
       "      <td>913.23</td>\n",
       "      <td>1.2464</td>\n",
       "      <td>7.575100e+15</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>0.28359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.17803</td>\n",
       "      <td>-0.006980</td>\n",
       "      <td>907.270</td>\n",
       "      <td>0.272140</td>\n",
       "      <td>0.45948</td>\n",
       "      <td>0.17327</td>\n",
       "      <td>2298.00</td>\n",
       "      <td>360650.0</td>\n",
       "      <td>1.224500e+13</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.7688</td>\n",
       "      <td>1.2042</td>\n",
       "      <td>0.26290</td>\n",
       "      <td>8.13120</td>\n",
       "      <td>45119.00</td>\n",
       "      <td>1.1764</td>\n",
       "      <td>3.218100e+14</td>\n",
       "      <td>3838.2</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.15236</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>780.100</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.51947</td>\n",
       "      <td>7.49140</td>\n",
       "      <td>112.51</td>\n",
       "      <td>259490.0</td>\n",
       "      <td>7.781400e+13</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.8580</td>\n",
       "      <td>2.0694</td>\n",
       "      <td>0.79631</td>\n",
       "      <td>-16.33600</td>\n",
       "      <td>4952.40</td>\n",
       "      <td>1.1784</td>\n",
       "      <td>4.533000e+12</td>\n",
       "      <td>4889.1</td>\n",
       "      <td>0.51486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>-109.150</td>\n",
       "      <td>0.297910</td>\n",
       "      <td>0.34490</td>\n",
       "      <td>-0.40932</td>\n",
       "      <td>2538.90</td>\n",
       "      <td>65332.0</td>\n",
       "      <td>1.907200e+15</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.6410</td>\n",
       "      <td>1.5298</td>\n",
       "      <td>1.14640</td>\n",
       "      <td>-0.43124</td>\n",
       "      <td>3856.50</td>\n",
       "      <td>1.4830</td>\n",
       "      <td>-8.991300e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25d7e119-269d-4efd-ac92-690e1cf059ec')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-25d7e119-269d-4efd-ac92-690e1cf059ec button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-25d7e119-269d-4efd-ac92-690e1cf059ec');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   id       f1        f2         f3        f4       f5        f6       f7  \\\n",
       "0   0  0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n",
       "1   1  0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n",
       "2   2  0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n",
       "3   3  0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n",
       "4   4  0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n",
       "\n",
       "         f8            f9  ...     f110    f111     f112      f113      f114  \\\n",
       "0  168900.0  3.992400e+14  ... -12.2280  1.7482  1.90960  -7.11570   4378.80   \n",
       "1  119810.0  3.874100e+15  ... -56.7580  4.1684  0.34808   4.14200    913.23   \n",
       "2  360650.0  1.224500e+13  ...  -5.7688  1.2042  0.26290   8.13120  45119.00   \n",
       "3  259490.0  7.781400e+13  ... -34.8580  2.0694  0.79631 -16.33600   4952.40   \n",
       "4   65332.0  1.907200e+15  ... -13.6410  1.5298  1.14640  -0.43124   3856.50   \n",
       "\n",
       "     f115          f116    f117     f118  claim  \n",
       "0  1.2096  8.613400e+14   140.1  1.01770      1  \n",
       "1  1.2464  7.575100e+15  1861.0  0.28359      0  \n",
       "2  1.1764  3.218100e+14  3838.2  0.40690      1  \n",
       "3  1.1784  4.533000e+12  4889.1  0.51486      1  \n",
       "4  1.4830 -8.991300e+12     NaN  0.23049      1  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iTmy5KIU__x"
   },
   "source": [
    "The data is composed of 120 columns all of which are numerical:\n",
    "* 118 feature columns named `f1, f2, ... f118`\n",
    "* label column named `claim`\n",
    "* An `id` column that we will drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vkY2UJ3qi5y-"
   },
   "outputs": [],
   "source": [
    "train_full_data = train_full_data.drop('id', axis=1)\n",
    "features = [f'f{i}' for i in range(1, 119)]\n",
    "label = 'claim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDi4a-YSVNs5"
   },
   "source": [
    "Let's check if we have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaFgKdIEVTUi",
    "outputId": "ef2a9c21-a2d8-4da3-c28f-806d7d374fc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1      15247\n",
       "f2      15190\n",
       "f3      15491\n",
       "f4      15560\n",
       "f5      15405\n",
       "        ...  \n",
       "f114    15438\n",
       "f115    15559\n",
       "f116    15589\n",
       "f117    15407\n",
       "f118    15212\n",
       "Length: 118, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_data[features].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu3APw9xVX3a"
   },
   "source": [
    "We can see that the data contains a lot of missing values. Approximately 15000 for each feature. That's around 1.5%\n",
    "\n",
    "The approach we will take in this notebook is to keep missing values, but add 3 additional features:\n",
    "* `Number of missing values` in each sample\n",
    "    * For each sample out of the 957919, we will see how many values are missing across all features and then include this as a new feature. \n",
    "    * If there were n features missing, we will record the number n. \n",
    "* `Standard deviation` over axis=1\n",
    "    * Standard deviation for each sample.\n",
    "* `Unbiased Variance` over axis=1\n",
    "    * Variance for each sample.\n",
    "\n",
    "This preprocessing, and feature addition, will be implemented through the 2 methods mentioned previously. \n",
    "1. Preprocessing using pandas\n",
    "2. Tensorflow based Preprocessing\n",
    "\n",
    "Let's start with the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yygwiPJQJUlV"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, test_ratio=0.1):\n",
    "    test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "    return dataset[~test_indices], dataset[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfpWBa-JVffi"
   },
   "source": [
    "## First Approach: Preprocessing using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8ommX7yVm6y"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "In the approach that we will use in this notebook, we will keep the missing values but will add 3 additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QUyVn3nrVrFV"
   },
   "outputs": [],
   "source": [
    "train_full_data['nan'] = train_full_data[features].isnull().sum(axis=1)\n",
    "train_full_data['std'] = train_full_data[features].std(axis=1)\n",
    "train_full_data['var'] = train_full_data[features].var(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kBJd8s-Vxwx"
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OaM2nb61m7G"
   },
   "source": [
    "Split the dataframe into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qpdOQ0KKNoL",
    "outputId": "7067b30d-2f58-4965-b577-71caa1a7bba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862229 samples in training and 95690 in validation\n"
     ]
    }
   ],
   "source": [
    "train_ds_pd, valid_ds_pd = split_dataset(train_full_data, test_ratio=VALID_RATIO)\n",
    "print(\"{} samples in training and {} in validation\".format(train_ds_pd.shape[0], valid_ds_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5IDInwd1wV6"
   },
   "source": [
    "Create the training and validation datasets using TensorFlow Decision Forests `pd_dataframe_to_tf_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuwlnDUJKnuK"
   },
   "outputs": [],
   "source": [
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
    "valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL_GBdCBm4fA"
   },
   "source": [
    "### GradientBoostedTreesModel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLF4aRvkWKLY"
   },
   "source": [
    "For hyperparameter tuning, we did the following:\n",
    "* Tried the predefined hyperparameters which did not give good results \n",
    "    * Especially `benchmark_rank1` which gave very bad results not to mention that it takes longer time to fit the data. \n",
    "    * `better_default` on the other hand gave acceptable results.\n",
    "* Used Keras tuner in order to search for hyperparameters that maximise `AUC`. \n",
    "    * If you want to check how to do this, check out the following Kaggle [notebook](https://www.kaggle.com/ekaterinadranitsyna/kerastuner-tf-decision-forest?linkId=133421702) by Ekaterina Dranitsyna.\n",
    "\n",
    "Finally the hyperparameters that gave the best results where the below which are `better_default` with `l1_regularization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0odOG59vm4fI"
   },
   "outputs": [],
   "source": [
    "model_1 = tfdf.keras.GradientBoostedTreesModel(\n",
    "    growing_strategy = 'BEST_FIRST_GLOBAL',\n",
    "    l1_regularization = 0.8\n",
    ")\n",
    "\n",
    "model_1.compile(metrics=[keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTNqlB-Y2geP"
   },
   "source": [
    "The next cell will take some time to get executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "QEZlOOnUN_4q",
    "outputId": "966db7ee-7920-4033-a239-36b2cb786238"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: {'f1': <tf.Tensor 'data:0' shape=(None,) dtype=float64>, 'f2': <tf.Tensor 'data_30:0' shape=(None,) dtype=float64>, 'f3': <tf.Tensor 'data_41:0' shape=(None,) dtype=float64>, 'f4': <tf.Tensor 'data_52:0' shape=(None,) dtype=float64>, 'f5': <tf.Tensor 'data_63:0' shape=(None,) dtype=float64>, 'f6': <tf.Tensor 'data_74:0' shape=(None,) dtype=float64>, 'f7': <tf.Tensor 'data_85:0' shape=(None,) dtype=float64>, 'f8': <tf.Tensor 'data_96:0' shape=(None,) dtype=float64>, 'f9': <tf.Tensor 'data_107:0' shape=(None,) dtype=float64>, 'f10': <tf.Tensor 'data_1:0' shape=(None,) dtype=float64>, 'f11': <tf.Tensor 'data_12:0' shape=(None,) dtype=float64>, 'f12': <tf.Tensor 'data_22:0' shape=(None,) dtype=float64>, 'f13': <tf.Tensor 'data_23:0' shape=(None,) dtype=float64>, 'f14': <tf.Tensor 'data_24:0' shape=(None,) dtype=float64>, 'f15': <tf.Tensor 'data_25:0' shape=(None,) dtype=float64>, 'f16': <tf.Tensor 'data_26:0' shape=(None,) dtype=float64>, 'f17': <tf.Tensor 'data_27:0' shape=(None,) dtype=float64>, 'f18': <tf.Tensor 'data_28:0' shape=(None,) dtype=float64>, 'f19': <tf.Tensor 'data_29:0' shape=(None,) dtype=float64>, 'f20': <tf.Tensor 'data_31:0' shape=(None,) dtype=float64>, 'f21': <tf.Tensor 'data_32:0' shape=(None,) dtype=float64>, 'f22': <tf.Tensor 'data_33:0' shape=(None,) dtype=float64>, 'f23': <tf.Tensor 'data_34:0' shape=(None,) dtype=float64>, 'f24': <tf.Tensor 'data_35:0' shape=(None,) dtype=float64>, 'f25': <tf.Tensor 'data_36:0' shape=(None,) dtype=float64>, 'f26': <tf.Tensor 'data_37:0' shape=(None,) dtype=float64>, 'f27': <tf.Tensor 'data_38:0' shape=(None,) dtype=float64>, 'f28': <tf.Tensor 'data_39:0' shape=(None,) dtype=float64>, 'f29': <tf.Tensor 'data_40:0' shape=(None,) dtype=float64>, 'f30': <tf.Tensor 'data_42:0' shape=(None,) dtype=float64>, 'f31': <tf.Tensor 'data_43:0' shape=(None,) dtype=float64>, 'f32': <tf.Tensor 'data_44:0' shape=(None,) dtype=float64>, 'f33': <tf.Tensor 'data_45:0' shape=(None,) dtype=float64>, 'f34': <tf.Tensor 'data_46:0' shape=(None,) dtype=float64>, 'f35': <tf.Tensor 'data_47:0' shape=(None,) dtype=float64>, 'f36': <tf.Tensor 'data_48:0' shape=(None,) dtype=float64>, 'f37': <tf.Tensor 'data_49:0' shape=(None,) dtype=float64>, 'f38': <tf.Tensor 'data_50:0' shape=(None,) dtype=float64>, 'f39': <tf.Tensor 'data_51:0' shape=(None,) dtype=float64>, 'f40': <tf.Tensor 'data_53:0' shape=(None,) dtype=float64>, 'f41': <tf.Tensor 'data_54:0' shape=(None,) dtype=float64>, 'f42': <tf.Tensor 'data_55:0' shape=(None,) dtype=float64>, 'f43': <tf.Tensor 'data_56:0' shape=(None,) dtype=float64>, 'f44': <tf.Tensor 'data_57:0' shape=(None,) dtype=float64>, 'f45': <tf.Tensor 'data_58:0' shape=(None,) dtype=float64>, 'f46': <tf.Tensor 'data_59:0' shape=(None,) dtype=float64>, 'f47': <tf.Tensor 'data_60:0' shape=(None,) dtype=float64>, 'f48': <tf.Tensor 'data_61:0' shape=(None,) dtype=float64>, 'f49': <tf.Tensor 'data_62:0' shape=(None,) dtype=float64>, 'f50': <tf.Tensor 'data_64:0' shape=(None,) dtype=float64>, 'f51': <tf.Tensor 'data_65:0' shape=(None,) dtype=float64>, 'f52': <tf.Tensor 'data_66:0' shape=(None,) dtype=float64>, 'f53': <tf.Tensor 'data_67:0' shape=(None,) dtype=float64>, 'f54': <tf.Tensor 'data_68:0' shape=(None,) dtype=float64>, 'f55': <tf.Tensor 'data_69:0' shape=(None,) dtype=float64>, 'f56': <tf.Tensor 'data_70:0' shape=(None,) dtype=float64>, 'f57': <tf.Tensor 'data_71:0' shape=(None,) dtype=float64>, 'f58': <tf.Tensor 'data_72:0' shape=(None,) dtype=float64>, 'f59': <tf.Tensor 'data_73:0' shape=(None,) dtype=float64>, 'f60': <tf.Tensor 'data_75:0' shape=(None,) dtype=float64>, 'f61': <tf.Tensor 'data_76:0' shape=(None,) dtype=float64>, 'f62': <tf.Tensor 'data_77:0' shape=(None,) dtype=float64>, 'f63': <tf.Tensor 'data_78:0' shape=(None,) dtype=float64>, 'f64': <tf.Tensor 'data_79:0' shape=(None,) dtype=float64>, 'f65': <tf.Tensor 'data_80:0' shape=(None,) dtype=float64>, 'f66': <tf.Tensor 'data_81:0' shape=(None,) dtype=float64>, 'f67': <tf.Tensor 'data_82:0' shape=(None,) dtype=float64>, 'f68': <tf.Tensor 'data_83:0' shape=(None,) dtype=float64>, 'f69': <tf.Tensor 'data_84:0' shape=(None,) dtype=float64>, 'f70': <tf.Tensor 'data_86:0' shape=(None,) dtype=float64>, 'f71': <tf.Tensor 'data_87:0' shape=(None,) dtype=float64>, 'f72': <tf.Tensor 'data_88:0' shape=(None,) dtype=float64>, 'f73': <tf.Tensor 'data_89:0' shape=(None,) dtype=float64>, 'f74': <tf.Tensor 'data_90:0' shape=(None,) dtype=float64>, 'f75': <tf.Tensor 'data_91:0' shape=(None,) dtype=float64>, 'f76': <tf.Tensor 'data_92:0' shape=(None,) dtype=float64>, 'f77': <tf.Tensor 'data_93:0' shape=(None,) dtype=float64>, 'f78': <tf.Tensor 'data_94:0' shape=(None,) dtype=float64>, 'f79': <tf.Tensor 'data_95:0' shape=(None,) dtype=float64>, 'f80': <tf.Tensor 'data_97:0' shape=(None,) dtype=float64>, 'f81': <tf.Tensor 'data_98:0' shape=(None,) dtype=float64>, 'f82': <tf.Tensor 'data_99:0' shape=(None,) dtype=float64>, 'f83': <tf.Tensor 'data_100:0' shape=(None,) dtype=float64>, 'f84': <tf.Tensor 'data_101:0' shape=(None,) dtype=float64>, 'f85': <tf.Tensor 'data_102:0' shape=(None,) dtype=float64>, 'f86': <tf.Tensor 'data_103:0' shape=(None,) dtype=float64>, 'f87': <tf.Tensor 'data_104:0' shape=(None,) dtype=float64>, 'f88': <tf.Tensor 'data_105:0' shape=(None,) dtype=float64>, 'f89': <tf.Tensor 'data_106:0' shape=(None,) dtype=float64>, 'f90': <tf.Tensor 'data_108:0' shape=(None,) dtype=float64>, 'f91': <tf.Tensor 'data_109:0' shape=(None,) dtype=float64>, 'f92': <tf.Tensor 'data_110:0' shape=(None,) dtype=float64>, 'f93': <tf.Tensor 'data_111:0' shape=(None,) dtype=float64>, 'f94': <tf.Tensor 'data_112:0' shape=(None,) dtype=float64>, 'f95': <tf.Tensor 'data_113:0' shape=(None,) dtype=float64>, 'f96': <tf.Tensor 'data_114:0' shape=(None,) dtype=float64>, 'f97': <tf.Tensor 'data_115:0' shape=(None,) dtype=float64>, 'f98': <tf.Tensor 'data_116:0' shape=(None,) dtype=float64>, 'f99': <tf.Tensor 'data_117:0' shape=(None,) dtype=float64>, 'f100': <tf.Tensor 'data_2:0' shape=(None,) dtype=float64>, 'f101': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'f102': <tf.Tensor 'data_4:0' shape=(None,) dtype=float64>, 'f103': <tf.Tensor 'data_5:0' shape=(None,) dtype=float64>, 'f104': <tf.Tensor 'data_6:0' shape=(None,) dtype=float64>, 'f105': <tf.Tensor 'data_7:0' shape=(None,) dtype=float64>, 'f106': <tf.Tensor 'data_8:0' shape=(None,) dtype=float64>, 'f107': <tf.Tensor 'data_9:0' shape=(None,) dtype=float64>, 'f108': <tf.Tensor 'data_10:0' shape=(None,) dtype=float64>, 'f109': <tf.Tensor 'data_11:0' shape=(None,) dtype=float64>, 'f110': <tf.Tensor 'data_13:0' shape=(None,) dtype=float64>, 'f111': <tf.Tensor 'data_14:0' shape=(None,) dtype=float64>, 'f112': <tf.Tensor 'data_15:0' shape=(None,) dtype=float64>, 'f113': <tf.Tensor 'data_16:0' shape=(None,) dtype=float64>, 'f114': <tf.Tensor 'data_17:0' shape=(None,) dtype=float64>, 'f115': <tf.Tensor 'data_18:0' shape=(None,) dtype=float64>, 'f116': <tf.Tensor 'data_19:0' shape=(None,) dtype=float64>, 'f117': <tf.Tensor 'data_20:0' shape=(None,) dtype=float64>, 'f118': <tf.Tensor 'data_21:0' shape=(None,) dtype=float64>, 'nan': <tf.Tensor 'data_118:0' shape=(None,) dtype=int64>, 'std': <tf.Tensor 'data_119:0' shape=(None,) dtype=float64>, 'var': <tf.Tensor 'data_120:0' shape=(None,) dtype=float64>}\n",
      "Label: Tensor(\"data_121:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n",
      "Normalized tensor features:\n",
      " {'f1': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'f2': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'f3': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'f4': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'f5': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'f6': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_5:0' shape=(None,) dtype=float32>), 'f7': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_6:0' shape=(None,) dtype=float32>), 'f8': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_7:0' shape=(None,) dtype=float32>), 'f9': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_8:0' shape=(None,) dtype=float32>), 'f10': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_9:0' shape=(None,) dtype=float32>), 'f11': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_10:0' shape=(None,) dtype=float32>), 'f12': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_11:0' shape=(None,) dtype=float32>), 'f13': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_12:0' shape=(None,) dtype=float32>), 'f14': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_13:0' shape=(None,) dtype=float32>), 'f15': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_14:0' shape=(None,) dtype=float32>), 'f16': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_15:0' shape=(None,) dtype=float32>), 'f17': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_16:0' shape=(None,) dtype=float32>), 'f18': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_17:0' shape=(None,) dtype=float32>), 'f19': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_18:0' shape=(None,) dtype=float32>), 'f20': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_19:0' shape=(None,) dtype=float32>), 'f21': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_20:0' shape=(None,) dtype=float32>), 'f22': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_21:0' shape=(None,) dtype=float32>), 'f23': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_22:0' shape=(None,) dtype=float32>), 'f24': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_23:0' shape=(None,) dtype=float32>), 'f25': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_24:0' shape=(None,) dtype=float32>), 'f26': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_25:0' shape=(None,) dtype=float32>), 'f27': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_26:0' shape=(None,) dtype=float32>), 'f28': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_27:0' shape=(None,) dtype=float32>), 'f29': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_28:0' shape=(None,) dtype=float32>), 'f30': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_29:0' shape=(None,) dtype=float32>), 'f31': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_30:0' shape=(None,) dtype=float32>), 'f32': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_31:0' shape=(None,) dtype=float32>), 'f33': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_32:0' shape=(None,) dtype=float32>), 'f34': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_33:0' shape=(None,) dtype=float32>), 'f35': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_34:0' shape=(None,) dtype=float32>), 'f36': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_35:0' shape=(None,) dtype=float32>), 'f37': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_36:0' shape=(None,) dtype=float32>), 'f38': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_37:0' shape=(None,) dtype=float32>), 'f39': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_38:0' shape=(None,) dtype=float32>), 'f40': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_39:0' shape=(None,) dtype=float32>), 'f41': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_40:0' shape=(None,) dtype=float32>), 'f42': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_41:0' shape=(None,) dtype=float32>), 'f43': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_42:0' shape=(None,) dtype=float32>), 'f44': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_43:0' shape=(None,) dtype=float32>), 'f45': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_44:0' shape=(None,) dtype=float32>), 'f46': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_45:0' shape=(None,) dtype=float32>), 'f47': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_46:0' shape=(None,) dtype=float32>), 'f48': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_47:0' shape=(None,) dtype=float32>), 'f49': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_48:0' shape=(None,) dtype=float32>), 'f50': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_49:0' shape=(None,) dtype=float32>), 'f51': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_50:0' shape=(None,) dtype=float32>), 'f52': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_51:0' shape=(None,) dtype=float32>), 'f53': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_52:0' shape=(None,) dtype=float32>), 'f54': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_53:0' shape=(None,) dtype=float32>), 'f55': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_54:0' shape=(None,) dtype=float32>), 'f56': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_55:0' shape=(None,) dtype=float32>), 'f57': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_56:0' shape=(None,) dtype=float32>), 'f58': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_57:0' shape=(None,) dtype=float32>), 'f59': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_58:0' shape=(None,) dtype=float32>), 'f60': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_59:0' shape=(None,) dtype=float32>), 'f61': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_60:0' shape=(None,) dtype=float32>), 'f62': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_61:0' shape=(None,) dtype=float32>), 'f63': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_62:0' shape=(None,) dtype=float32>), 'f64': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_63:0' shape=(None,) dtype=float32>), 'f65': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_64:0' shape=(None,) dtype=float32>), 'f66': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_65:0' shape=(None,) dtype=float32>), 'f67': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_66:0' shape=(None,) dtype=float32>), 'f68': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_67:0' shape=(None,) dtype=float32>), 'f69': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_68:0' shape=(None,) dtype=float32>), 'f70': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_69:0' shape=(None,) dtype=float32>), 'f71': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_70:0' shape=(None,) dtype=float32>), 'f72': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_71:0' shape=(None,) dtype=float32>), 'f73': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_72:0' shape=(None,) dtype=float32>), 'f74': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_73:0' shape=(None,) dtype=float32>), 'f75': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_74:0' shape=(None,) dtype=float32>), 'f76': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_75:0' shape=(None,) dtype=float32>), 'f77': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_76:0' shape=(None,) dtype=float32>), 'f78': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_77:0' shape=(None,) dtype=float32>), 'f79': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_78:0' shape=(None,) dtype=float32>), 'f80': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_79:0' shape=(None,) dtype=float32>), 'f81': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_80:0' shape=(None,) dtype=float32>), 'f82': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_81:0' shape=(None,) dtype=float32>), 'f83': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_82:0' shape=(None,) dtype=float32>), 'f84': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_83:0' shape=(None,) dtype=float32>), 'f85': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_84:0' shape=(None,) dtype=float32>), 'f86': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_85:0' shape=(None,) dtype=float32>), 'f87': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_86:0' shape=(None,) dtype=float32>), 'f88': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_87:0' shape=(None,) dtype=float32>), 'f89': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_88:0' shape=(None,) dtype=float32>), 'f90': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_89:0' shape=(None,) dtype=float32>), 'f91': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_90:0' shape=(None,) dtype=float32>), 'f92': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_91:0' shape=(None,) dtype=float32>), 'f93': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_92:0' shape=(None,) dtype=float32>), 'f94': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_93:0' shape=(None,) dtype=float32>), 'f95': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_94:0' shape=(None,) dtype=float32>), 'f96': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_95:0' shape=(None,) dtype=float32>), 'f97': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_96:0' shape=(None,) dtype=float32>), 'f98': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_97:0' shape=(None,) dtype=float32>), 'f99': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_98:0' shape=(None,) dtype=float32>), 'f100': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_99:0' shape=(None,) dtype=float32>), 'f101': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_100:0' shape=(None,) dtype=float32>), 'f102': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_101:0' shape=(None,) dtype=float32>), 'f103': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_102:0' shape=(None,) dtype=float32>), 'f104': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_103:0' shape=(None,) dtype=float32>), 'f105': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_104:0' shape=(None,) dtype=float32>), 'f106': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_105:0' shape=(None,) dtype=float32>), 'f107': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_106:0' shape=(None,) dtype=float32>), 'f108': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_107:0' shape=(None,) dtype=float32>), 'f109': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_108:0' shape=(None,) dtype=float32>), 'f110': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_109:0' shape=(None,) dtype=float32>), 'f111': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_110:0' shape=(None,) dtype=float32>), 'f112': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_111:0' shape=(None,) dtype=float32>), 'f113': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_112:0' shape=(None,) dtype=float32>), 'f114': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_113:0' shape=(None,) dtype=float32>), 'f115': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_114:0' shape=(None,) dtype=float32>), 'f116': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_115:0' shape=(None,) dtype=float32>), 'f117': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_116:0' shape=(None,) dtype=float32>), 'f118': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_117:0' shape=(None,) dtype=float32>), 'nan': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_118:0' shape=(None,) dtype=float32>), 'std': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_119:0' shape=(None,) dtype=float32>), 'var': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_120:0' shape=(None,) dtype=float32>)}\n",
      "Training dataset read in 0:01:23.204459. Found 862229 examples.\n",
      "Training model...\n",
      "Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training get stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO kernel.cc:813] Start Yggdrasil model training\n",
      "[INFO kernel.cc:814] Collect training examples\n",
      "[INFO kernel.cc:422] Number of batches: 863\n",
      "[INFO kernel.cc:423] Number of examples: 862229\n",
      "[INFO kernel.cc:836] Training dataset:\n",
      "Number of records: 862229\n",
      "Number of columns: 122\n",
      "\n",
      "Number of columns by type:\n",
      "\tNUMERICAL: 121 (99.1803%)\n",
      "\tCATEGORICAL: 1 (0.819672%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "NUMERICAL: 121 (99.1803%)\n",
      "\t0: \"f1\" NUMERICAL num-nas:13707 (1.58972%) mean:0.0901929 min:-0.14991 max:0.41517 sd:0.0435975\n",
      "\t1: \"f10\" NUMERICAL num-nas:13738 (1.59331%) mean:5323.89 min:-26404 max:85622 sd:10071.6\n",
      "\t2: \"f100\" NUMERICAL num-nas:13925 (1.615%) mean:0.425797 min:-0.034559 max:1.0613 sd:0.283504\n",
      "\t3: \"f101\" NUMERICAL num-nas:13822 (1.60305%) mean:20.2106 min:-4.2949 max:105.62 sd:19.6196\n",
      "\t4: \"f102\" NUMERICAL num-nas:13631 (1.5809%) mean:321610 min:-227770 max:2.3379e+06 sd:327719\n",
      "\t5: \"f103\" NUMERICAL num-nas:14039 (1.62822%) mean:548.513 min:-222.21 max:3260.9 sd:864.005\n",
      "\t6: \"f104\" NUMERICAL num-nas:13696 (1.58844%) mean:3852.5 min:-11581 max:46876 sd:6665.54\n",
      "\t7: \"f105\" NUMERICAL num-nas:13793 (1.59969%) mean:0.17808 min:-0.029027 max:0.49156 sd:0.123318\n",
      "\t8: \"f106\" NUMERICAL num-nas:13963 (1.61941%) mean:0.160874 min:-0.066726 max:0.84855 sd:0.141664\n",
      "\t9: \"f107\" NUMERICAL num-nas:13882 (1.61001%) mean:0.0142058 min:-0.0075354 max:0.089019 sd:0.0162637\n",
      "\t10: \"f108\" NUMERICAL num-nas:13818 (1.60259%) mean:1.67072e+09 min:-5.877e+08 max:7.5565e+09 sd:1.87488e+09\n",
      "\t11: \"f109\" NUMERICAL num-nas:13968 (1.61999%) mean:0.398682 min:-0.042355 max:1.1193 sd:0.298112\n",
      "\t12: \"f11\" NUMERICAL num-nas:13919 (1.6143%) mean:0.729508 min:-8.0863 max:8.6505 sd:1.49535\n",
      "\t13: \"f110\" NUMERICAL num-nas:13776 (1.59772%) mean:-19.932 min:-105.86 max:1.6134 sd:18.5835\n",
      "\t14: \"f111\" NUMERICAL num-nas:13960 (1.61906%) mean:2.07482 min:0.27704 max:4.5659 sd:0.896048\n",
      "\t15: \"f112\" NUMERICAL num-nas:13848 (1.60607%) mean:23.8863 min:-27.691 max:217.84 sd:45.5769\n",
      "\t16: \"f113\" NUMERICAL num-nas:13726 (1.59192%) mean:1.74858 min:-26.589 max:47.757 sd:10.0811\n",
      "\t17: \"f114\" NUMERICAL num-nas:13846 (1.60584%) mean:63165.4 min:-81977 max:526050 sd:92466.8\n",
      "\t18: \"f115\" NUMERICAL num-nas:13975 (1.6208%) mean:1.20889 min:0.90527 max:1.8867 sd:0.11494\n",
      "\t19: \"f116\" NUMERICAL num-nas:14061 (1.63077%) mean:4.27699e+16 min:-8.9444e+15 max:3.2499e+17 sd:6.73321e+16\n",
      "\t20: \"f117\" NUMERICAL num-nas:13937 (1.61639%) mean:3959.89 min:-415.24 max:13151 sd:3155.47\n",
      "\t21: \"f118\" NUMERICAL num-nas:13615 (1.57905%) mean:0.559327 min:-0.15124 max:2.7436 sd:0.408718\n",
      "\t22: \"f12\" NUMERICAL num-nas:14007 (1.62451%) mean:1.84406e+09 min:-4.081e+08 max:8.4736e+09 sd:2.12516e+09\n",
      "\t23: \"f13\" NUMERICAL num-nas:13846 (1.60584%) mean:0.247734 min:-0.1038 max:0.58977 sd:0.101186\n",
      "\t24: \"f14\" NUMERICAL num-nas:13775 (1.5976%) mean:7.00363 min:-0.85376 max:36.951 sd:6.62398\n",
      "\t25: \"f15\" NUMERICAL num-nas:13969 (1.6201%) mean:0.0193789 min:-0.3346 max:0.50963 sd:0.101871\n",
      "\t26: \"f16\" NUMERICAL num-nas:13904 (1.61256%) mean:444.931 min:-116.88 max:2335.4 sd:631.439\n",
      "\t27: \"f17\" NUMERICAL num-nas:13864 (1.60793%) mean:6.89241 min:-3.4329 max:19.189 sd:1.71432\n",
      "\t28: \"f18\" NUMERICAL num-nas:13811 (1.60178%) mean:4.49316 min:-0.066527 max:25.458 sd:3.90024\n",
      "\t29: \"f19\" NUMERICAL num-nas:13930 (1.61558%) mean:22.4495 min:-4.4225 max:80.154 sd:14.6142\n",
      "\t30: \"f2\" NUMERICAL num-nas:13675 (1.58601%) mean:0.34593 min:-0.019044 max:0.51899 sd:0.146284\n",
      "\t31: \"f20\" NUMERICAL num-nas:13848 (1.60607%) mean:203.728 min:-58.834 max:1032.2 sd:281.026\n",
      "\t32: \"f21\" NUMERICAL num-nas:13874 (1.60909%) mean:61101.7 min:-84079 max:523590 sd:89924.3\n",
      "\t33: \"f22\" NUMERICAL num-nas:13752 (1.59494%) mean:2.26956 min:-6.0094 max:11.306 sd:0.897182\n",
      "\t34: \"f23\" NUMERICAL num-nas:13801 (1.60062%) mean:87.148 min:-20.514 max:160.45 sd:37.3688\n",
      "\t35: \"f24\" NUMERICAL num-nas:14068 (1.63159%) mean:0.340863 min:-5.7352 max:6.96 sd:1.64295\n",
      "\t36: \"f25\" NUMERICAL num-nas:13952 (1.61813%) mean:414.98 min:-71.502 max:1220.8 sd:314.903\n",
      "\t37: \"f26\" NUMERICAL num-nas:13814 (1.60213%) mean:3.3804e+12 min:-6.9567e+11 max:2.5805e+13 sd:5.65703e+12\n",
      "\t38: \"f27\" NUMERICAL num-nas:13919 (1.6143%) mean:1.25442e+12 min:-9.3842e+11 max:5.4471e+12 sd:1.64273e+12\n",
      "\t39: \"f28\" NUMERICAL num-nas:13754 (1.59517%) mean:2.25671e+06 min:-470600 max:8.9606e+06 sd:2.30347e+06\n",
      "\t40: \"f29\" NUMERICAL num-nas:13914 (1.61372%) mean:0.329003 min:-0.0056592 max:1.0958 sd:0.433842\n",
      "\t41: \"f3\" NUMERICAL num-nas:13964 (1.61952%) mean:4065.51 min:-9421.7 max:39544 sd:6412.95\n",
      "\t42: \"f30\" NUMERICAL num-nas:13901 (1.61222%) mean:7.8799 min:-0.52999 max:36.744 sd:5.93746\n",
      "\t43: \"f31\" NUMERICAL num-nas:14134 (1.63924%) mean:0.394193 min:-3.8135 max:3.7531 sd:0.781709\n",
      "\t44: \"f32\" NUMERICAL num-nas:13971 (1.62034%) mean:134456 min:-349650 max:1.154e+06 sd:203634\n",
      "\t45: \"f33\" NUMERICAL num-nas:13912 (1.61349%) mean:357944 min:-605590 max:2.8732e+06 sd:462666\n",
      "\t46: \"f34\" NUMERICAL num-nas:13708 (1.58983%) mean:-4.78463e-06 min:-0.0038813 max:0.0039186 sd:0.00153391\n",
      "\t47: \"f35\" NUMERICAL num-nas:13770 (1.59702%) mean:2.78379e+16 min:-2.0689e+16 max:1.5905e+17 sd:3.4545e+16\n",
      "\t48: \"f36\" NUMERICAL num-nas:13803 (1.60085%) mean:185.569 min:-2414.3 max:3728.5 sd:702.078\n",
      "\t49: \"f37\" NUMERICAL num-nas:13803 (1.60085%) mean:406.165 min:-40.881 max:1218 sd:314.802\n",
      "\t50: \"f38\" NUMERICAL num-nas:13922 (1.61465%) mean:1.76866 min:0.5461 max:4.084 sd:0.588839\n",
      "\t51: \"f39\" NUMERICAL num-nas:13983 (1.62173%) mean:1980.87 min:-433.7 max:11195 sd:1958.97\n",
      "\t52: \"f4\" NUMERICAL num-nas:14029 (1.62706%) mean:0.201242 min:-0.073832 max:1.3199 sd:0.21246\n",
      "\t53: \"f40\" NUMERICAL num-nas:13770 (1.59702%) mean:0.35927 min:-0.007641 max:1.0435 sd:0.441704\n",
      "\t54: \"f41\" NUMERICAL num-nas:13846 (1.60584%) mean:446.603 min:-107.38 max:2335.4 sd:620.556\n",
      "\t55: \"f42\" NUMERICAL num-nas:13901 (1.61222%) mean:0.359519 min:-0.05771 max:1.0287 sd:0.407484\n",
      "\t56: \"f43\" NUMERICAL num-nas:13927 (1.61523%) mean:6.94542 min:-4.4214 max:19.978 sd:1.83219\n",
      "\t57: \"f44\" NUMERICAL num-nas:13840 (1.60514%) mean:29.7649 min:-8.1892 max:180.97 sd:28.778\n",
      "\t58: \"f45\" NUMERICAL num-nas:13920 (1.61442%) mean:0.0134514 min:-0.01026 max:0.066794 sd:0.0146629\n",
      "\t59: \"f46\" NUMERICAL num-nas:14072 (1.63205%) mean:4.27767 min:-3.5615 max:10.066 sd:1.13971\n",
      "\t60: \"f47\" NUMERICAL num-nas:13971 (1.62034%) mean:0.0292488 min:-2.6172 max:3.0153 sd:0.676685\n",
      "\t61: \"f48\" NUMERICAL num-nas:13930 (1.61558%) mean:6.37869 min:1.0564 max:16.87 sd:2.10741\n",
      "\t62: \"f49\" NUMERICAL num-nas:13834 (1.60445%) mean:-0.425687 min:-1.7306 max:1.799 sd:0.729042\n",
      "\t63: \"f5\" NUMERICAL num-nas:13880 (1.60978%) mean:0.304889 min:-0.0069898 max:0.55443 sd:0.145297\n",
      "\t64: \"f50\" NUMERICAL num-nas:13981 (1.62149%) mean:0.299902 min:-0.006924 max:0.54832 sd:0.14613\n",
      "\t65: \"f51\" NUMERICAL num-nas:13919 (1.6143%) mean:56.6528 min:-131.95 max:503.17 sd:88.1947\n",
      "\t66: \"f52\" NUMERICAL num-nas:13784 (1.59865%) mean:2682.19 min:-721.61 max:14553 sd:2525.62\n",
      "\t67: \"f53\" NUMERICAL num-nas:13883 (1.61013%) mean:12.2015 min:-26.637 max:131.75 sd:21.6551\n",
      "\t68: \"f54\" NUMERICAL num-nas:13856 (1.607%) mean:137.378 min:98.868 max:175.16 sd:16.0393\n",
      "\t69: \"f55\" NUMERICAL num-nas:13830 (1.60398%) mean:0.250612 min:-0.033956 max:0.49607 sd:0.11\n",
      "\t70: \"f56\" NUMERICAL num-nas:13887 (1.61059%) mean:0.410944 min:-0.052052 max:1.1866 sd:0.323773\n",
      "\t71: \"f57\" NUMERICAL num-nas:14081 (1.63309%) mean:1.10913e-05 min:-0.003899 max:0.0039055 sd:0.00151991\n",
      "\t72: \"f58\" NUMERICAL num-nas:13914 (1.61372%) mean:-0.329402 min:-1.179 max:0.069581 sd:0.281571\n",
      "\t73: \"f59\" NUMERICAL num-nas:13789 (1.59923%) mean:3.05892 min:0.68364 max:7.7346 sd:1.73436\n",
      "\t74: \"f6\" NUMERICAL num-nas:14003 (1.62405%) mean:-0.0704013 min:-12.791 max:11.202 sd:2.12258\n",
      "\t75: \"f60\" NUMERICAL num-nas:14033 (1.62753%) mean:0.54868 min:-0.15099 max:1.0141 sd:0.268411\n",
      "\t76: \"f61\" NUMERICAL num-nas:13827 (1.60363%) mean:0.273509 min:-0.19692 max:1.0751 sd:0.256388\n",
      "\t77: \"f62\" NUMERICAL num-nas:13993 (1.62289%) mean:2.46808e+09 min:-1.8256e+09 max:1.8289e+10 sd:2.90255e+09\n",
      "\t78: \"f63\" NUMERICAL num-nas:13860 (1.60746%) mean:36.8443 min:-11.941 max:210.43 sd:34.7187\n",
      "\t79: \"f64\" NUMERICAL num-nas:14115 (1.63704%) mean:0.21284 min:-0.13478 max:1.3421 sd:0.225064\n",
      "\t80: \"f65\" NUMERICAL num-nas:13908 (1.61303%) mean:47829 min:-3206.2 max:91871 sd:36007.2\n",
      "\t81: \"f66\" NUMERICAL num-nas:13905 (1.61268%) mean:84.1 min:-22.021 max:161.75 sd:36.0355\n",
      "\t82: \"f67\" NUMERICAL num-nas:13968 (1.61999%) mean:607.92 min:-68.682 max:1996.7 sd:527.344\n",
      "\t83: \"f68\" NUMERICAL num-nas:14137 (1.63959%) mean:28.9942 min:-2.1598 max:167.66 sd:27.3526\n",
      "\t84: \"f69\" NUMERICAL num-nas:14052 (1.62973%) mean:1.21243 min:0.84922 max:1.8917 sd:0.129288\n",
      "\t85: \"f7\" NUMERICAL num-nas:13933 (1.61593%) mean:1619.97 min:-224.8 max:5426.6 sd:1275.97\n",
      "\t86: \"f70\" NUMERICAL num-nas:13733 (1.59273%) mean:0.418386 min:-0.0092006 max:1.0178 sd:0.493429\n",
      "\t87: \"f71\" NUMERICAL num-nas:13904 (1.61256%) mean:1.5449 min:0.7742 max:3.7999 sd:0.441675\n",
      "\t88: \"f72\" NUMERICAL num-nas:13733 (1.59273%) mean:482.113 min:-64.669 max:1453.9 sd:378.652\n",
      "\t89: \"f73\" NUMERICAL num-nas:13961 (1.61918%) mean:7.96462e+14 min:-2.8028e+14 max:6.0879e+15 sd:1.1913e+15\n",
      "\t90: \"f74\" NUMERICAL num-nas:13989 (1.62242%) mean:1.06311e+12 min:-6.1067e+11 max:6.6946e+12 sd:2.00358e+12\n",
      "\t91: \"f75\" NUMERICAL num-nas:13890 (1.61094%) mean:0.376598 min:-0.013163 max:1.0304 sd:0.444947\n",
      "\t92: \"f76\" NUMERICAL num-nas:14005 (1.62428%) mean:6.87533 min:-2.9862 max:18.366 sd:1.70836\n",
      "\t93: \"f77\" NUMERICAL num-nas:13739 (1.59343%) mean:10725.2 min:-1546 max:56889 sd:15111.3\n",
      "\t94: \"f78\" NUMERICAL num-nas:13887 (1.61059%) mean:10526.7 min:-1284.2 max:47503 sd:10413.8\n",
      "\t95: \"f79\" NUMERICAL num-nas:13892 (1.61117%) mean:1.55549 min:-24.288 max:43.552 sd:9.08008\n",
      "\t96: \"f8\" NUMERICAL num-nas:13815 (1.60224%) mean:376922 min:-29843 max:1.9137e+06 sd:345276\n",
      "\t97: \"f80\" NUMERICAL num-nas:13767 (1.59668%) mean:0.194294 min:-0.017615 max:1.3572 sd:0.162351\n",
      "\t98: \"f81\" NUMERICAL num-nas:13806 (1.6012%) mean:3.23966 min:0.96422 max:7.2883 sd:1.99154\n",
      "\t99: \"f82\" NUMERICAL num-nas:13922 (1.61465%) mean:1.0542e+11 min:-7.3457e+10 max:7.3897e+11 sd:9.8988e+10\n",
      "\t100: \"f83\" NUMERICAL num-nas:14055 (1.63008%) mean:152.778 min:-28.752 max:950.53 sd:227.829\n",
      "\t101: \"f84\" NUMERICAL num-nas:13840 (1.60514%) mean:6.12848e+06 min:-2.992e+06 max:3.4511e+07 sd:8.76732e+06\n",
      "\t102: \"f85\" NUMERICAL num-nas:13992 (1.62277%) mean:635.326 min:-74.545 max:2307.5 sd:583.413\n",
      "\t103: \"f86\" NUMERICAL num-nas:14011 (1.62497%) mean:3.25123e+10 min:-5.9495e+09 max:1.3097e+11 sd:3.06885e+10\n",
      "\t104: \"f87\" NUMERICAL num-nas:13759 (1.59575%) mean:26.5965 min:-7.6164 max:147.08 sd:25.4476\n",
      "\t105: \"f88\" NUMERICAL num-nas:13967 (1.61987%) mean:207.287 min:-22.576 max:618.13 sd:158.215\n",
      "\t106: \"f89\" NUMERICAL num-nas:13940 (1.61674%) mean:3806.01 min:-296.78 max:20675 sd:3533.47\n",
      "\t107: \"f9\" NUMERICAL num-nas:13742 (1.59378%) mean:1.80445e+15 min:-1.1533e+15 max:1.0424e+16 sd:2.33523e+15\n",
      "\t108: \"f90\" NUMERICAL num-nas:13898 (1.61187%) mean:6.73295 min:-0.25757 max:21.994 sd:3.15821\n",
      "\t109: \"f91\" NUMERICAL num-nas:13965 (1.61964%) mean:0.366894 min:-0.012238 max:0.51629 sd:0.146292\n",
      "\t110: \"f92\" NUMERICAL num-nas:13935 (1.61616%) mean:4869.17 min:-12829 max:55362 sd:8430.69\n",
      "\t111: \"f93\" NUMERICAL num-nas:13961 (1.61918%) mean:132.273 min:-12.922 max:448.78 sd:110.077\n",
      "\t112: \"f94\" NUMERICAL num-nas:13862 (1.60769%) mean:0.82094 min:-3.2933 max:3.9251 sd:0.712834\n",
      "\t113: \"f95\" NUMERICAL num-nas:14062 (1.63089%) mean:13.118 min:-1.3413 max:65.215 sd:12.7377\n",
      "\t114: \"f96\" NUMERICAL num-nas:13744 (1.59401%) mean:3848.2 min:-7641.3 max:38704 sd:6433.14\n",
      "\t115: \"f97\" NUMERICAL num-nas:13740 (1.59354%) mean:0.99997 min:0.9961 max:1.0039 sd:0.00153284\n",
      "\t116: \"f98\" NUMERICAL num-nas:13754 (1.59517%) mean:1.41573e+13 min:-5.7146e+12 max:7.1701e+13 sd:1.64018e+13\n",
      "\t117: \"f99\" NUMERICAL num-nas:13876 (1.60932%) mean:1.68305 min:0.6082 max:4.1691 sd:0.711925\n",
      "\t118: \"nan\" NUMERICAL mean:1.90056 min:0 max:14 sd:2.02682\n",
      "\t119: \"std\" NUMERICAL mean:5.7502e+15 min:1.77089e+11 max:3.2399e+16 sd:6.1175e+15\n",
      "\t120: \"var\" NUMERICAL mean:7.04887e+31 min:3.13606e+22 max:1.0497e+33 sd:nan\n",
      "\n",
      "CATEGORICAL: 1 (0.819672%)\n",
      "\t121: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO kernel.cc:882] Configure learner\n",
      "[WARNING gradient_boosted_trees.cc:1671] Subsample hyperparameter given but sampling method does not match.\n",
      "[WARNING gradient_boosted_trees.cc:1684] GOSS alpha hyperparameter given but GOSS is disabled.\n",
      "[WARNING gradient_boosted_trees.cc:1693] GOSS beta hyperparameter given but GOSS is disabled.\n",
      "[WARNING gradient_boosted_trees.cc:1705] SelGB ratio hyperparameter given but SelGB is disabled.\n",
      "[INFO kernel.cc:912] Training config:\n",
      "learner: \"GRADIENT_BOOSTED_TREES\"\n",
      "features: \"f1\"\n",
      "features: \"f10\"\n",
      "features: \"f100\"\n",
      "features: \"f101\"\n",
      "features: \"f102\"\n",
      "features: \"f103\"\n",
      "features: \"f104\"\n",
      "features: \"f105\"\n",
      "features: \"f106\"\n",
      "features: \"f107\"\n",
      "features: \"f108\"\n",
      "features: \"f109\"\n",
      "features: \"f11\"\n",
      "features: \"f110\"\n",
      "features: \"f111\"\n",
      "features: \"f112\"\n",
      "features: \"f113\"\n",
      "features: \"f114\"\n",
      "features: \"f115\"\n",
      "features: \"f116\"\n",
      "features: \"f117\"\n",
      "features: \"f118\"\n",
      "features: \"f12\"\n",
      "features: \"f13\"\n",
      "features: \"f14\"\n",
      "features: \"f15\"\n",
      "features: \"f16\"\n",
      "features: \"f17\"\n",
      "features: \"f18\"\n",
      "features: \"f19\"\n",
      "features: \"f2\"\n",
      "features: \"f20\"\n",
      "features: \"f21\"\n",
      "features: \"f22\"\n",
      "features: \"f23\"\n",
      "features: \"f24\"\n",
      "features: \"f25\"\n",
      "features: \"f26\"\n",
      "features: \"f27\"\n",
      "features: \"f28\"\n",
      "features: \"f29\"\n",
      "features: \"f3\"\n",
      "features: \"f30\"\n",
      "features: \"f31\"\n",
      "features: \"f32\"\n",
      "features: \"f33\"\n",
      "features: \"f34\"\n",
      "features: \"f35\"\n",
      "features: \"f36\"\n",
      "features: \"f37\"\n",
      "features: \"f38\"\n",
      "features: \"f39\"\n",
      "features: \"f4\"\n",
      "features: \"f40\"\n",
      "features: \"f41\"\n",
      "features: \"f42\"\n",
      "features: \"f43\"\n",
      "features: \"f44\"\n",
      "features: \"f45\"\n",
      "features: \"f46\"\n",
      "features: \"f47\"\n",
      "features: \"f48\"\n",
      "features: \"f49\"\n",
      "features: \"f5\"\n",
      "features: \"f50\"\n",
      "features: \"f51\"\n",
      "features: \"f52\"\n",
      "features: \"f53\"\n",
      "features: \"f54\"\n",
      "features: \"f55\"\n",
      "features: \"f56\"\n",
      "features: \"f57\"\n",
      "features: \"f58\"\n",
      "features: \"f59\"\n",
      "features: \"f6\"\n",
      "features: \"f60\"\n",
      "features: \"f61\"\n",
      "features: \"f62\"\n",
      "features: \"f63\"\n",
      "features: \"f64\"\n",
      "features: \"f65\"\n",
      "features: \"f66\"\n",
      "features: \"f67\"\n",
      "features: \"f68\"\n",
      "features: \"f69\"\n",
      "features: \"f7\"\n",
      "features: \"f70\"\n",
      "features: \"f71\"\n",
      "features: \"f72\"\n",
      "features: \"f73\"\n",
      "features: \"f74\"\n",
      "features: \"f75\"\n",
      "features: \"f76\"\n",
      "features: \"f77\"\n",
      "features: \"f78\"\n",
      "features: \"f79\"\n",
      "features: \"f8\"\n",
      "features: \"f80\"\n",
      "features: \"f81\"\n",
      "features: \"f82\"\n",
      "features: \"f83\"\n",
      "features: \"f84\"\n",
      "features: \"f85\"\n",
      "features: \"f86\"\n",
      "features: \"f87\"\n",
      "features: \"f88\"\n",
      "features: \"f89\"\n",
      "features: \"f9\"\n",
      "features: \"f90\"\n",
      "features: \"f91\"\n",
      "features: \"f92\"\n",
      "features: \"f93\"\n",
      "features: \"f94\"\n",
      "features: \"f95\"\n",
      "features: \"f96\"\n",
      "features: \"f97\"\n",
      "features: \"f98\"\n",
      "features: \"f99\"\n",
      "features: \"nan\"\n",
      "features: \"std\"\n",
      "features: \"var\"\n",
      "label: \"__LABEL\"\n",
      "task: CLASSIFICATION\n",
      "random_seed: 123456\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "pure_serving_model: false\n",
      "[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n",
      "  num_trees: 300\n",
      "  decision_tree {\n",
      "    max_depth: 6\n",
      "    min_examples: 5\n",
      "    in_split_min_examples_check: true\n",
      "    keep_non_leaf_label_distribution: true\n",
      "    num_candidate_attributes: -1\n",
      "    missing_value_policy: GLOBAL_IMPUTATION\n",
      "    allow_na_conditions: false\n",
      "    categorical_set_greedy_forward {\n",
      "      sampling: 0.1\n",
      "      max_num_items: -1\n",
      "      min_item_frequency: 1\n",
      "    }\n",
      "    growing_strategy_best_first_global {\n",
      "    }\n",
      "    categorical {\n",
      "      cart {\n",
      "      }\n",
      "    }\n",
      "    axis_aligned_split {\n",
      "    }\n",
      "    internal {\n",
      "      sorting_strategy: PRESORTED\n",
      "    }\n",
      "    uplift {\n",
      "      min_examples_in_treatment: 5\n",
      "      split_score: KULLBACK_LEIBLER\n",
      "    }\n",
      "  }\n",
      "  shrinkage: 0.1\n",
      "  loss: DEFAULT\n",
      "  validation_set_ratio: 0.1\n",
      "  validation_interval_in_trees: 1\n",
      "  early_stopping: VALIDATION_LOSS_INCREASE\n",
      "  early_stopping_num_trees_look_ahead: 30\n",
      "  l2_regularization: 0\n",
      "  lambda_loss: 1\n",
      "  mart {\n",
      "  }\n",
      "  adapt_subsample_for_maximum_training_duration: false\n",
      "  l1_regularization: 0.8\n",
      "  use_hessian_gain: false\n",
      "  l2_regularization_categorical: 1\n",
      "  apply_link_function: true\n",
      "  compute_permutation_variable_importance: false\n",
      "  binary_focal_loss_options {\n",
      "    misprediction_exponent: 2\n",
      "    positive_sample_coefficient: 0.5\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO kernel.cc:915] Deployment config:\n",
      "cache_path: \"/tmp/tmplq1cufhx/working_cache\"\n",
      "num_threads: 2\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO kernel.cc:944] Train model\n",
      "[INFO gradient_boosted_trees.cc:405] Default loss set to BINOMIAL_LOG_LIKELIHOOD\n",
      "[INFO gradient_boosted_trees.cc:1008] Training gradient boosted tree on 862229 example(s) and 121 feature(s).\n",
      "[INFO gradient_boosted_trees.cc:1051] 775945 examples used for training and 86284 examples used for validation\n",
      "[INFO gradient_boosted_trees.cc:1434] \tnum-trees:1 train-loss:1.323354 train-accuracy:0.772251 valid-loss:1.323255 valid-accuracy:0.772797\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:2 train-loss:1.272105 train-accuracy:0.772221 valid-loss:1.271954 valid-accuracy:0.772785\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:3 train-loss:1.230052 train-accuracy:0.772289 valid-loss:1.229858 valid-accuracy:0.772785\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:4 train-loss:1.195351 train-accuracy:0.772218 valid-loss:1.195131 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:5 train-loss:1.166590 train-accuracy:0.772208 valid-loss:1.166355 valid-accuracy:0.772797\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:6 train-loss:1.142686 train-accuracy:0.772189 valid-loss:1.142473 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:7 train-loss:1.122765 train-accuracy:0.772240 valid-loss:1.122598 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:8 train-loss:1.106148 train-accuracy:0.772218 valid-loss:1.106002 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:9 train-loss:1.092271 train-accuracy:0.772216 valid-loss:1.092191 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:10 train-loss:1.080673 train-accuracy:0.772203 valid-loss:1.080651 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:11 train-loss:1.070979 train-accuracy:0.772185 valid-loss:1.071007 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:12 train-loss:1.062878 train-accuracy:0.772176 valid-loss:1.062977 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:13 train-loss:1.056097 train-accuracy:0.772211 valid-loss:1.056236 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:14 train-loss:1.050424 train-accuracy:0.772207 valid-loss:1.050613 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:15 train-loss:1.045684 train-accuracy:0.772204 valid-loss:1.045969 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:16 train-loss:1.041719 train-accuracy:0.772202 valid-loss:1.042084 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:17 train-loss:1.038399 train-accuracy:0.772198 valid-loss:1.038855 valid-accuracy:0.772843\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:18 train-loss:1.035626 train-accuracy:0.772191 valid-loss:1.036170 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:19 train-loss:1.033293 train-accuracy:0.772198 valid-loss:1.033949 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:21 train-loss:1.029708 train-accuracy:0.772195 valid-loss:1.030564 valid-accuracy:0.772797\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:22 train-loss:1.028334 train-accuracy:0.772235 valid-loss:1.029263 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:23 train-loss:1.027148 train-accuracy:0.772242 valid-loss:1.028188 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:24 train-loss:1.026164 train-accuracy:0.772249 valid-loss:1.027287 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:25 train-loss:1.025316 train-accuracy:0.772248 valid-loss:1.026544 valid-accuracy:0.772843\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:26 train-loss:1.024593 train-accuracy:0.772265 valid-loss:1.025916 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:27 train-loss:1.023946 train-accuracy:0.772257 valid-loss:1.025334 valid-accuracy:0.772843\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:28 train-loss:1.023394 train-accuracy:0.772257 valid-loss:1.024856 valid-accuracy:0.772866\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:29 train-loss:1.022918 train-accuracy:0.772267 valid-loss:1.024464 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:30 train-loss:1.022483 train-accuracy:0.772287 valid-loss:1.024164 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:31 train-loss:1.022108 train-accuracy:0.772298 valid-loss:1.023890 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:32 train-loss:1.021748 train-accuracy:0.772300 valid-loss:1.023592 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:33 train-loss:1.021432 train-accuracy:0.772302 valid-loss:1.023407 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:34 train-loss:1.021150 train-accuracy:0.772311 valid-loss:1.023261 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:35 train-loss:1.020896 train-accuracy:0.772318 valid-loss:1.023093 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:36 train-loss:1.020643 train-accuracy:0.772347 valid-loss:1.022931 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:37 train-loss:1.020391 train-accuracy:0.772352 valid-loss:1.022806 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:38 train-loss:1.020174 train-accuracy:0.772392 valid-loss:1.022723 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:39 train-loss:1.019942 train-accuracy:0.772423 valid-loss:1.022603 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:40 train-loss:1.019699 train-accuracy:0.772423 valid-loss:1.022452 valid-accuracy:0.772843\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:41 train-loss:1.019495 train-accuracy:0.772461 valid-loss:1.022355 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:42 train-loss:1.019301 train-accuracy:0.772495 valid-loss:1.022278 valid-accuracy:0.772797\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:43 train-loss:1.019105 train-accuracy:0.772515 valid-loss:1.022198 valid-accuracy:0.772820\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:44 train-loss:1.018919 train-accuracy:0.772557 valid-loss:1.022107 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:45 train-loss:1.018687 train-accuracy:0.772570 valid-loss:1.021980 valid-accuracy:0.772808\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:46 train-loss:1.018494 train-accuracy:0.772591 valid-loss:1.021910 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:47 train-loss:1.018299 train-accuracy:0.772617 valid-loss:1.021839 valid-accuracy:0.772843\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:48 train-loss:1.018092 train-accuracy:0.772640 valid-loss:1.021744 valid-accuracy:0.772901\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:49 train-loss:1.017878 train-accuracy:0.772658 valid-loss:1.021678 valid-accuracy:0.772913\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:50 train-loss:1.017664 train-accuracy:0.772678 valid-loss:1.021584 valid-accuracy:0.772901\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:51 train-loss:1.017476 train-accuracy:0.772703 valid-loss:1.021460 valid-accuracy:0.772913\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 50\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:52 train-loss:1.017303 train-accuracy:0.772721 valid-loss:1.021418 valid-accuracy:0.772901\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:53 train-loss:1.017131 train-accuracy:0.772748 valid-loss:1.021334 valid-accuracy:0.772924\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:54 train-loss:1.016942 train-accuracy:0.772794 valid-loss:1.021299 valid-accuracy:0.772936\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:55 train-loss:1.016765 train-accuracy:0.772811 valid-loss:1.021203 valid-accuracy:0.772866\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:56 train-loss:1.016548 train-accuracy:0.772825 valid-loss:1.021112 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:57 train-loss:1.016346 train-accuracy:0.772850 valid-loss:1.021021 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:58 train-loss:1.016096 train-accuracy:0.772886 valid-loss:1.020890 valid-accuracy:0.772866\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:59 train-loss:1.015934 train-accuracy:0.772914 valid-loss:1.020853 valid-accuracy:0.772866\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:60 train-loss:1.015759 train-accuracy:0.772943 valid-loss:1.020771 valid-accuracy:0.772901\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:61 train-loss:1.015588 train-accuracy:0.772948 valid-loss:1.020687 valid-accuracy:0.772878\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:62 train-loss:1.015417 train-accuracy:0.772966 valid-loss:1.020648 valid-accuracy:0.772832\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:63 train-loss:1.015248 train-accuracy:0.772983 valid-loss:1.020610 valid-accuracy:0.772866\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:64 train-loss:1.015086 train-accuracy:0.773008 valid-loss:1.020541 valid-accuracy:0.772947\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:65 train-loss:1.014931 train-accuracy:0.773050 valid-loss:1.020522 valid-accuracy:0.772878\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:66 train-loss:1.014771 train-accuracy:0.773081 valid-loss:1.020461 valid-accuracy:0.772855\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:67 train-loss:1.014599 train-accuracy:0.773119 valid-loss:1.020401 valid-accuracy:0.772913\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:68 train-loss:1.014443 train-accuracy:0.773154 valid-loss:1.020353 valid-accuracy:0.772890\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:69 train-loss:1.014288 train-accuracy:0.773204 valid-loss:1.020305 valid-accuracy:0.772924\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:70 train-loss:1.014097 train-accuracy:0.773203 valid-loss:1.020254 valid-accuracy:0.772959\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:71 train-loss:1.013940 train-accuracy:0.773222 valid-loss:1.020219 valid-accuracy:0.773005\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:72 train-loss:1.013776 train-accuracy:0.773258 valid-loss:1.020153 valid-accuracy:0.772971\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:73 train-loss:1.013614 train-accuracy:0.773278 valid-loss:1.020101 valid-accuracy:0.772947\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:74 train-loss:1.013460 train-accuracy:0.773314 valid-loss:1.020031 valid-accuracy:0.772971\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:75 train-loss:1.013280 train-accuracy:0.773318 valid-loss:1.019981 valid-accuracy:0.772959\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:76 train-loss:1.013096 train-accuracy:0.773368 valid-loss:1.019922 valid-accuracy:0.772878\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:77 train-loss:1.012950 train-accuracy:0.773401 valid-loss:1.019867 valid-accuracy:0.773005\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:78 train-loss:1.012798 train-accuracy:0.773422 valid-loss:1.019841 valid-accuracy:0.773063\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:79 train-loss:1.012641 train-accuracy:0.773448 valid-loss:1.019778 valid-accuracy:0.772959\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:80 train-loss:1.012497 train-accuracy:0.773498 valid-loss:1.019732 valid-accuracy:0.772971\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:81 train-loss:1.012344 train-accuracy:0.773537 valid-loss:1.019696 valid-accuracy:0.773005\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:82 train-loss:1.012115 train-accuracy:0.773566 valid-loss:1.019543 valid-accuracy:0.773052\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:83 train-loss:1.011968 train-accuracy:0.773610 valid-loss:1.019501 valid-accuracy:0.773075\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:84 train-loss:1.011835 train-accuracy:0.773658 valid-loss:1.019465 valid-accuracy:0.773087\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:85 train-loss:1.011693 train-accuracy:0.773682 valid-loss:1.019427 valid-accuracy:0.773052\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:86 train-loss:1.011532 train-accuracy:0.773706 valid-loss:1.019387 valid-accuracy:0.772994\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:87 train-loss:1.011389 train-accuracy:0.773722 valid-loss:1.019352 valid-accuracy:0.773005\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:88 train-loss:1.011245 train-accuracy:0.773760 valid-loss:1.019293 valid-accuracy:0.773063\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:89 train-loss:1.011098 train-accuracy:0.773789 valid-loss:1.019232 valid-accuracy:0.773052\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:90 train-loss:1.010958 train-accuracy:0.773823 valid-loss:1.019208 valid-accuracy:0.773052\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:91 train-loss:1.010779 train-accuracy:0.773880 valid-loss:1.019126 valid-accuracy:0.773087\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:92 train-loss:1.010628 train-accuracy:0.773916 valid-loss:1.019089 valid-accuracy:0.773121\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:93 train-loss:1.010467 train-accuracy:0.773954 valid-loss:1.019011 valid-accuracy:0.773168\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:94 train-loss:1.010337 train-accuracy:0.773979 valid-loss:1.018961 valid-accuracy:0.773110\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:95 train-loss:1.010199 train-accuracy:0.774012 valid-loss:1.018922 valid-accuracy:0.773144\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:96 train-loss:1.010073 train-accuracy:0.774024 valid-loss:1.018887 valid-accuracy:0.773144\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:97 train-loss:1.009916 train-accuracy:0.774032 valid-loss:1.018849 valid-accuracy:0.773144\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:98 train-loss:1.009758 train-accuracy:0.774085 valid-loss:1.018836 valid-accuracy:0.773226\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:99 train-loss:1.009606 train-accuracy:0.774136 valid-loss:1.018784 valid-accuracy:0.773168\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:100 train-loss:1.009460 train-accuracy:0.774145 valid-loss:1.018753 valid-accuracy:0.773110\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 99\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:101 train-loss:1.009329 train-accuracy:0.774168 valid-loss:1.018713 valid-accuracy:0.773040\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:102 train-loss:1.009166 train-accuracy:0.774184 valid-loss:1.018660 valid-accuracy:0.773063\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:103 train-loss:1.008998 train-accuracy:0.774211 valid-loss:1.018596 valid-accuracy:0.773110\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:104 train-loss:1.008836 train-accuracy:0.774225 valid-loss:1.018548 valid-accuracy:0.773202\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:105 train-loss:1.008713 train-accuracy:0.774249 valid-loss:1.018523 valid-accuracy:0.773226\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:106 train-loss:1.008565 train-accuracy:0.774277 valid-loss:1.018493 valid-accuracy:0.773214\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:107 train-loss:1.008433 train-accuracy:0.774291 valid-loss:1.018429 valid-accuracy:0.773237\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:108 train-loss:1.008282 train-accuracy:0.774320 valid-loss:1.018383 valid-accuracy:0.773133\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:109 train-loss:1.008145 train-accuracy:0.774368 valid-loss:1.018334 valid-accuracy:0.773121\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:110 train-loss:1.008005 train-accuracy:0.774391 valid-loss:1.018288 valid-accuracy:0.773144\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:111 train-loss:1.007872 train-accuracy:0.774430 valid-loss:1.018253 valid-accuracy:0.773226\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:112 train-loss:1.007736 train-accuracy:0.774454 valid-loss:1.018206 valid-accuracy:0.773179\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:113 train-loss:1.007605 train-accuracy:0.774465 valid-loss:1.018147 valid-accuracy:0.773237\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:114 train-loss:1.007461 train-accuracy:0.774481 valid-loss:1.018080 valid-accuracy:0.773342\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:115 train-loss:1.007326 train-accuracy:0.774506 valid-loss:1.018038 valid-accuracy:0.773307\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:116 train-loss:1.007194 train-accuracy:0.774506 valid-loss:1.017981 valid-accuracy:0.773388\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:117 train-loss:1.007057 train-accuracy:0.774550 valid-loss:1.017938 valid-accuracy:0.773388\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:118 train-loss:1.006939 train-accuracy:0.774579 valid-loss:1.017903 valid-accuracy:0.773399\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:119 train-loss:1.006823 train-accuracy:0.774573 valid-loss:1.017889 valid-accuracy:0.773353\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:120 train-loss:1.006687 train-accuracy:0.774590 valid-loss:1.017915 valid-accuracy:0.773307\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:121 train-loss:1.006542 train-accuracy:0.774623 valid-loss:1.017905 valid-accuracy:0.773342\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:122 train-loss:1.006414 train-accuracy:0.774630 valid-loss:1.017894 valid-accuracy:0.773365\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:123 train-loss:1.006307 train-accuracy:0.774667 valid-loss:1.017870 valid-accuracy:0.773365\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:124 train-loss:1.006189 train-accuracy:0.774713 valid-loss:1.017836 valid-accuracy:0.773353\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:125 train-loss:1.006072 train-accuracy:0.774730 valid-loss:1.017813 valid-accuracy:0.773330\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:126 train-loss:1.005934 train-accuracy:0.774765 valid-loss:1.017756 valid-accuracy:0.773330\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:127 train-loss:1.005817 train-accuracy:0.774797 valid-loss:1.017762 valid-accuracy:0.773365\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:128 train-loss:1.005683 train-accuracy:0.774832 valid-loss:1.017772 valid-accuracy:0.773342\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:129 train-loss:1.005566 train-accuracy:0.774878 valid-loss:1.017759 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:130 train-loss:1.005446 train-accuracy:0.774905 valid-loss:1.017744 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:131 train-loss:1.005335 train-accuracy:0.774922 valid-loss:1.017728 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:132 train-loss:1.005199 train-accuracy:0.774989 valid-loss:1.017703 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:133 train-loss:1.005069 train-accuracy:0.775012 valid-loss:1.017666 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:134 train-loss:1.004950 train-accuracy:0.775020 valid-loss:1.017639 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:135 train-loss:1.004790 train-accuracy:0.775030 valid-loss:1.017579 valid-accuracy:0.773423\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:136 train-loss:1.004665 train-accuracy:0.775067 valid-loss:1.017558 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:137 train-loss:1.004525 train-accuracy:0.775115 valid-loss:1.017487 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:138 train-loss:1.004402 train-accuracy:0.775132 valid-loss:1.017459 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:139 train-loss:1.004278 train-accuracy:0.775121 valid-loss:1.017442 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:140 train-loss:1.004169 train-accuracy:0.775154 valid-loss:1.017424 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:141 train-loss:1.004045 train-accuracy:0.775168 valid-loss:1.017403 valid-accuracy:0.773539\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:142 train-loss:1.003925 train-accuracy:0.775172 valid-loss:1.017375 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:143 train-loss:1.003814 train-accuracy:0.775179 valid-loss:1.017358 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:144 train-loss:1.003688 train-accuracy:0.775213 valid-loss:1.017326 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:145 train-loss:1.003552 train-accuracy:0.775235 valid-loss:1.017275 valid-accuracy:0.773562\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:146 train-loss:1.003437 train-accuracy:0.775260 valid-loss:1.017274 valid-accuracy:0.773620\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:147 train-loss:1.003317 train-accuracy:0.775302 valid-loss:1.017254 valid-accuracy:0.773620\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:148 train-loss:1.003215 train-accuracy:0.775280 valid-loss:1.017250 valid-accuracy:0.773596\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:149 train-loss:1.003105 train-accuracy:0.775317 valid-loss:1.017226 valid-accuracy:0.773562\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 148\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:150 train-loss:1.002970 train-accuracy:0.775336 valid-loss:1.017172 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:151 train-loss:1.002844 train-accuracy:0.775376 valid-loss:1.017147 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:152 train-loss:1.002716 train-accuracy:0.775411 valid-loss:1.017104 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:153 train-loss:1.002613 train-accuracy:0.775449 valid-loss:1.017091 valid-accuracy:0.773469\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:154 train-loss:1.002508 train-accuracy:0.775451 valid-loss:1.017051 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:155 train-loss:1.002378 train-accuracy:0.775457 valid-loss:1.017018 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:156 train-loss:1.002251 train-accuracy:0.775511 valid-loss:1.016977 valid-accuracy:0.773539\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:157 train-loss:1.002141 train-accuracy:0.775546 valid-loss:1.016941 valid-accuracy:0.773539\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:158 train-loss:1.002033 train-accuracy:0.775567 valid-loss:1.016931 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:159 train-loss:1.001915 train-accuracy:0.775622 valid-loss:1.016918 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:160 train-loss:1.001795 train-accuracy:0.775659 valid-loss:1.016900 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:161 train-loss:1.001673 train-accuracy:0.775670 valid-loss:1.016895 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:162 train-loss:1.001563 train-accuracy:0.775701 valid-loss:1.016913 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:163 train-loss:1.001439 train-accuracy:0.775715 valid-loss:1.016870 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:164 train-loss:1.001293 train-accuracy:0.775726 valid-loss:1.016826 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:165 train-loss:1.001173 train-accuracy:0.775761 valid-loss:1.016807 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:166 train-loss:1.001058 train-accuracy:0.775796 valid-loss:1.016772 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:167 train-loss:1.000955 train-accuracy:0.775824 valid-loss:1.016748 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:168 train-loss:1.000844 train-accuracy:0.775831 valid-loss:1.016712 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:169 train-loss:1.000715 train-accuracy:0.775864 valid-loss:1.016672 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:170 train-loss:1.000610 train-accuracy:0.775857 valid-loss:1.016669 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:171 train-loss:1.000496 train-accuracy:0.775893 valid-loss:1.016599 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:172 train-loss:1.000392 train-accuracy:0.775873 valid-loss:1.016602 valid-accuracy:0.773469\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:173 train-loss:1.000287 train-accuracy:0.775902 valid-loss:1.016623 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:174 train-loss:1.000177 train-accuracy:0.775936 valid-loss:1.016629 valid-accuracy:0.773399\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:175 train-loss:1.000063 train-accuracy:0.775953 valid-loss:1.016596 valid-accuracy:0.773423\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:176 train-loss:0.999934 train-accuracy:0.775983 valid-loss:1.016576 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:177 train-loss:0.999823 train-accuracy:0.775998 valid-loss:1.016588 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:178 train-loss:0.999719 train-accuracy:0.776001 valid-loss:1.016544 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:179 train-loss:0.999605 train-accuracy:0.776015 valid-loss:1.016509 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:180 train-loss:0.999470 train-accuracy:0.776059 valid-loss:1.016471 valid-accuracy:0.773539\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:181 train-loss:0.999385 train-accuracy:0.776074 valid-loss:1.016460 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:182 train-loss:0.999284 train-accuracy:0.776127 valid-loss:1.016450 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:183 train-loss:0.999176 train-accuracy:0.776131 valid-loss:1.016446 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:184 train-loss:0.999065 train-accuracy:0.776135 valid-loss:1.016433 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:185 train-loss:0.998944 train-accuracy:0.776131 valid-loss:1.016437 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:186 train-loss:0.998857 train-accuracy:0.776134 valid-loss:1.016427 valid-accuracy:0.773573\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:187 train-loss:0.998778 train-accuracy:0.776140 valid-loss:1.016407 valid-accuracy:0.773608\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:188 train-loss:0.998659 train-accuracy:0.776174 valid-loss:1.016401 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:189 train-loss:0.998556 train-accuracy:0.776215 valid-loss:1.016387 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:190 train-loss:0.998431 train-accuracy:0.776207 valid-loss:1.016335 valid-accuracy:0.773492\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:191 train-loss:0.998328 train-accuracy:0.776230 valid-loss:1.016331 valid-accuracy:0.773596\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:192 train-loss:0.998208 train-accuracy:0.776269 valid-loss:1.016321 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:193 train-loss:0.998090 train-accuracy:0.776284 valid-loss:1.016303 valid-accuracy:0.773573\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:194 train-loss:0.997981 train-accuracy:0.776299 valid-loss:1.016281 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:195 train-loss:0.997910 train-accuracy:0.776327 valid-loss:1.016269 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:196 train-loss:0.997801 train-accuracy:0.776324 valid-loss:1.016233 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:197 train-loss:0.997678 train-accuracy:0.776350 valid-loss:1.016225 valid-accuracy:0.773596\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:198 train-loss:0.997581 train-accuracy:0.776389 valid-loss:1.016216 valid-accuracy:0.773643\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 197\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:200 train-loss:0.997369 train-accuracy:0.776394 valid-loss:1.016210 valid-accuracy:0.773573\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:201 train-loss:0.997283 train-accuracy:0.776411 valid-loss:1.016215 valid-accuracy:0.773620\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:202 train-loss:0.997166 train-accuracy:0.776434 valid-loss:1.016221 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:203 train-loss:0.997057 train-accuracy:0.776456 valid-loss:1.016208 valid-accuracy:0.773469\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:204 train-loss:0.996945 train-accuracy:0.776447 valid-loss:1.016207 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:205 train-loss:0.996828 train-accuracy:0.776473 valid-loss:1.016196 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:207 train-loss:0.996640 train-accuracy:0.776532 valid-loss:1.016202 valid-accuracy:0.773562\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:208 train-loss:0.996532 train-accuracy:0.776545 valid-loss:1.016200 valid-accuracy:0.773654\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:209 train-loss:0.996431 train-accuracy:0.776569 valid-loss:1.016191 valid-accuracy:0.773678\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:211 train-loss:0.996258 train-accuracy:0.776591 valid-loss:1.016206 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:212 train-loss:0.996139 train-accuracy:0.776604 valid-loss:1.016178 valid-accuracy:0.773631\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:213 train-loss:0.996034 train-accuracy:0.776601 valid-loss:1.016159 valid-accuracy:0.773678\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:214 train-loss:0.995944 train-accuracy:0.776612 valid-loss:1.016155 valid-accuracy:0.773654\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:215 train-loss:0.995840 train-accuracy:0.776635 valid-loss:1.016159 valid-accuracy:0.773643\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:217 train-loss:0.995620 train-accuracy:0.776645 valid-loss:1.016123 valid-accuracy:0.773573\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:218 train-loss:0.995523 train-accuracy:0.776666 valid-loss:1.016154 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:219 train-loss:0.995401 train-accuracy:0.776661 valid-loss:1.016121 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:220 train-loss:0.995341 train-accuracy:0.776699 valid-loss:1.016103 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:221 train-loss:0.995240 train-accuracy:0.776725 valid-loss:1.016097 valid-accuracy:0.773411\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:222 train-loss:0.995131 train-accuracy:0.776738 valid-loss:1.016104 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:224 train-loss:0.994955 train-accuracy:0.776757 valid-loss:1.016126 valid-accuracy:0.773469\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:226 train-loss:0.994758 train-accuracy:0.776804 valid-loss:1.016098 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:227 train-loss:0.994650 train-accuracy:0.776857 valid-loss:1.016104 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:228 train-loss:0.994551 train-accuracy:0.776850 valid-loss:1.016080 valid-accuracy:0.773411\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:229 train-loss:0.994426 train-accuracy:0.776866 valid-loss:1.016049 valid-accuracy:0.773434\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:231 train-loss:0.994230 train-accuracy:0.776939 valid-loss:1.016057 valid-accuracy:0.773434\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:233 train-loss:0.994047 train-accuracy:0.776956 valid-loss:1.016028 valid-accuracy:0.773399\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:235 train-loss:0.993860 train-accuracy:0.777006 valid-loss:1.016003 valid-accuracy:0.773365\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:236 train-loss:0.993753 train-accuracy:0.777046 valid-loss:1.016017 valid-accuracy:0.773307\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:237 train-loss:0.993668 train-accuracy:0.777085 valid-loss:1.015999 valid-accuracy:0.773330\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:238 train-loss:0.993546 train-accuracy:0.777094 valid-loss:1.015973 valid-accuracy:0.773399\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:240 train-loss:0.993331 train-accuracy:0.777175 valid-loss:1.015956 valid-accuracy:0.773446\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:242 train-loss:0.993150 train-accuracy:0.777194 valid-loss:1.015984 valid-accuracy:0.773411\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:244 train-loss:0.992957 train-accuracy:0.777207 valid-loss:1.015964 valid-accuracy:0.773399\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:245 train-loss:0.992892 train-accuracy:0.777232 valid-loss:1.015966 valid-accuracy:0.773411\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:246 train-loss:0.992785 train-accuracy:0.777247 valid-loss:1.015946 valid-accuracy:0.773457\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:247 train-loss:0.992699 train-accuracy:0.777257 valid-loss:1.015933 valid-accuracy:0.773469\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:248 train-loss:0.992601 train-accuracy:0.777260 valid-loss:1.015926 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:250 train-loss:0.992396 train-accuracy:0.777290 valid-loss:1.015959 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:251 train-loss:0.992314 train-accuracy:0.777309 valid-loss:1.015968 valid-accuracy:0.773562\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:252 train-loss:0.992234 train-accuracy:0.777322 valid-loss:1.016004 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:253 train-loss:0.992156 train-accuracy:0.777326 valid-loss:1.016020 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 252\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:254 train-loss:0.992043 train-accuracy:0.777348 valid-loss:1.016008 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:256 train-loss:0.991864 train-accuracy:0.777368 valid-loss:1.015978 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:257 train-loss:0.991780 train-accuracy:0.777354 valid-loss:1.015960 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:259 train-loss:0.991600 train-accuracy:0.777403 valid-loss:1.015919 valid-accuracy:0.773481\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:260 train-loss:0.991521 train-accuracy:0.777417 valid-loss:1.015919 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:261 train-loss:0.991414 train-accuracy:0.777456 valid-loss:1.015908 valid-accuracy:0.773515\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:262 train-loss:0.991329 train-accuracy:0.777470 valid-loss:1.015903 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:264 train-loss:0.991157 train-accuracy:0.777510 valid-loss:1.015899 valid-accuracy:0.773504\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:266 train-loss:0.990957 train-accuracy:0.777545 valid-loss:1.015901 valid-accuracy:0.773550\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:267 train-loss:0.990866 train-accuracy:0.777572 valid-loss:1.015902 valid-accuracy:0.773562\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:268 train-loss:0.990771 train-accuracy:0.777604 valid-loss:1.015894 valid-accuracy:0.773527\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:269 train-loss:0.990692 train-accuracy:0.777620 valid-loss:1.015897 valid-accuracy:0.773585\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:271 train-loss:0.990535 train-accuracy:0.777641 valid-loss:1.015860 valid-accuracy:0.773620\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:272 train-loss:0.990427 train-accuracy:0.777603 valid-loss:1.015853 valid-accuracy:0.773573\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:273 train-loss:0.990329 train-accuracy:0.777614 valid-loss:1.015842 valid-accuracy:0.773689\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:274 train-loss:0.990243 train-accuracy:0.777649 valid-loss:1.015829 valid-accuracy:0.773608\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:276 train-loss:0.990062 train-accuracy:0.777680 valid-loss:1.015820 valid-accuracy:0.773539\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:278 train-loss:0.989899 train-accuracy:0.777727 valid-loss:1.015813 valid-accuracy:0.773608\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:279 train-loss:0.989817 train-accuracy:0.777738 valid-loss:1.015825 valid-accuracy:0.773678\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:280 train-loss:0.989719 train-accuracy:0.777766 valid-loss:1.015807 valid-accuracy:0.773712\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:281 train-loss:0.989662 train-accuracy:0.777769 valid-loss:1.015790 valid-accuracy:0.773759\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:282 train-loss:0.989560 train-accuracy:0.777786 valid-loss:1.015753 valid-accuracy:0.773724\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:283 train-loss:0.989446 train-accuracy:0.777817 valid-loss:1.015736 valid-accuracy:0.773805\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:285 train-loss:0.989232 train-accuracy:0.777879 valid-loss:1.015745 valid-accuracy:0.773736\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:287 train-loss:0.989052 train-accuracy:0.777922 valid-loss:1.015741 valid-accuracy:0.773736\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:289 train-loss:0.988842 train-accuracy:0.777948 valid-loss:1.015707 valid-accuracy:0.773863\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:291 train-loss:0.988671 train-accuracy:0.777984 valid-loss:1.015721 valid-accuracy:0.773817\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:292 train-loss:0.988599 train-accuracy:0.778017 valid-loss:1.015748 valid-accuracy:0.773805\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:293 train-loss:0.988508 train-accuracy:0.778032 valid-loss:1.015756 valid-accuracy:0.773875\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:294 train-loss:0.988434 train-accuracy:0.778050 valid-loss:1.015750 valid-accuracy:0.773886\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:296 train-loss:0.988245 train-accuracy:0.778064 valid-loss:1.015705 valid-accuracy:0.773817\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:298 train-loss:0.988072 train-accuracy:0.778095 valid-loss:1.015688 valid-accuracy:0.773828\n",
      "[INFO gradient_boosted_trees.cc:1434] \tnum-trees:300 train-loss:0.987876 train-accuracy:0.778162 valid-loss:1.015676 valid-accuracy:0.773770\n",
      "[INFO gradient_boosted_trees.cc:1479] Create final snapshot of the model at iteration 300\n",
      "[INFO gradient_boosted_trees.cc:230] Truncates the model to 300 tree(s) i.e. 300  iteration(s).\n",
      "[INFO gradient_boosted_trees.cc:264] Final model num-trees:300 valid-loss:1.015676 valid-accuracy:0.773770\n",
      "[INFO kernel.cc:961] Export model in log directory: /tmp/tmplq1cufhx with prefix e12c0651ee784e8d\n",
      "[INFO kernel.cc:978] Save model in resources\n",
      "[INFO kernel.cc:1176] Loading model from path /tmp/tmplq1cufhx/model/ with prefix e12c0651ee784e8d\n",
      "[INFO abstract_model.cc:1248] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n",
      "[INFO kernel.cc:1022] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 2:55:37.440718\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa5a21db320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fa5a21db320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5a0f7be50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "model_1.fit(train_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IILLr0vCfBjo"
   },
   "source": [
    "### Post Training Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAMJJZNXffRz"
   },
   "source": [
    "`model.summary()` shows us the overall structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "XcdRUiuym4fI",
    "outputId": "57d75059-c1f2-40e0-cce0-f10357962e7a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gradient_boosted_trees_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Type: \"GRADIENT_BOOSTED_TREES\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (121):\n",
      "\tf1\n",
      "\tf10\n",
      "\tf100\n",
      "\tf101\n",
      "\tf102\n",
      "\tf103\n",
      "\tf104\n",
      "\tf105\n",
      "\tf106\n",
      "\tf107\n",
      "\tf108\n",
      "\tf109\n",
      "\tf11\n",
      "\tf110\n",
      "\tf111\n",
      "\tf112\n",
      "\tf113\n",
      "\tf114\n",
      "\tf115\n",
      "\tf116\n",
      "\tf117\n",
      "\tf118\n",
      "\tf12\n",
      "\tf13\n",
      "\tf14\n",
      "\tf15\n",
      "\tf16\n",
      "\tf17\n",
      "\tf18\n",
      "\tf19\n",
      "\tf2\n",
      "\tf20\n",
      "\tf21\n",
      "\tf22\n",
      "\tf23\n",
      "\tf24\n",
      "\tf25\n",
      "\tf26\n",
      "\tf27\n",
      "\tf28\n",
      "\tf29\n",
      "\tf3\n",
      "\tf30\n",
      "\tf31\n",
      "\tf32\n",
      "\tf33\n",
      "\tf34\n",
      "\tf35\n",
      "\tf36\n",
      "\tf37\n",
      "\tf38\n",
      "\tf39\n",
      "\tf4\n",
      "\tf40\n",
      "\tf41\n",
      "\tf42\n",
      "\tf43\n",
      "\tf44\n",
      "\tf45\n",
      "\tf46\n",
      "\tf47\n",
      "\tf48\n",
      "\tf49\n",
      "\tf5\n",
      "\tf50\n",
      "\tf51\n",
      "\tf52\n",
      "\tf53\n",
      "\tf54\n",
      "\tf55\n",
      "\tf56\n",
      "\tf57\n",
      "\tf58\n",
      "\tf59\n",
      "\tf6\n",
      "\tf60\n",
      "\tf61\n",
      "\tf62\n",
      "\tf63\n",
      "\tf64\n",
      "\tf65\n",
      "\tf66\n",
      "\tf67\n",
      "\tf68\n",
      "\tf69\n",
      "\tf7\n",
      "\tf70\n",
      "\tf71\n",
      "\tf72\n",
      "\tf73\n",
      "\tf74\n",
      "\tf75\n",
      "\tf76\n",
      "\tf77\n",
      "\tf78\n",
      "\tf79\n",
      "\tf8\n",
      "\tf80\n",
      "\tf81\n",
      "\tf82\n",
      "\tf83\n",
      "\tf84\n",
      "\tf85\n",
      "\tf86\n",
      "\tf87\n",
      "\tf88\n",
      "\tf89\n",
      "\tf9\n",
      "\tf90\n",
      "\tf91\n",
      "\tf92\n",
      "\tf93\n",
      "\tf94\n",
      "\tf95\n",
      "\tf96\n",
      "\tf97\n",
      "\tf98\n",
      "\tf99\n",
      "\tnan\n",
      "\tstd\n",
      "\tvar\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: MEAN_MIN_DEPTH:\n",
      "    1. \"__LABEL\"  5.419134 ################\n",
      "    2.     \"std\"  5.405801 ###############\n",
      "    3.     \"f85\"  5.397147 ###############\n",
      "    4.     \"var\"  5.396876 ###############\n",
      "    5.     \"f58\"  5.391392 ###############\n",
      "    6.     \"f64\"  5.391118 ###############\n",
      "    7.     \"f26\"  5.390855 ###############\n",
      "    8.     \"f76\"  5.388381 ###############\n",
      "    9.     \"f91\"  5.385801 ###############\n",
      "   10.     \"f51\"  5.385586 ###############\n",
      "   11.     \"f93\"  5.382145 ###############\n",
      "   12.     \"f41\"  5.381233 ###############\n",
      "   13.     \"f66\"  5.378381 ###############\n",
      "   14.     \"f63\"  5.378322 ###############\n",
      "   15.     \"f20\"  5.377414 ###############\n",
      "   16.    \"f105\"  5.372074 ###############\n",
      "   17.     \"f14\"  5.372037 ###############\n",
      "   18.    \"f100\"  5.371285 ###############\n",
      "   19.     \"f72\"  5.369851 ###############\n",
      "   20.     \"f89\"  5.366769 ###############\n",
      "   21.    \"f115\"  5.366769 ###############\n",
      "   22.     \"f59\"  5.365964 ###############\n",
      "   23.     \"f22\"  5.364295 ###############\n",
      "   24.    \"f104\"  5.361607 ###############\n",
      "   25.     \"f19\"  5.359994 ###############\n",
      "   26.    \"f116\"  5.359242 ###############\n",
      "   27.     \"f56\"  5.358596 ###############\n",
      "   28.     \"f55\"  5.358059 ###############\n",
      "   29.     \"f83\"  5.356186 ###############\n",
      "   30.    \"f112\"  5.355375 ###############\n",
      "   31.     \"f94\"  5.354833 ##############\n",
      "   32.     \"f84\"  5.353743 ##############\n",
      "   33.     \"f15\"  5.351392 ##############\n",
      "   34.     \"f68\"  5.350625 ##############\n",
      "   35.     \"f82\"  5.348166 ##############\n",
      "   36.     \"f74\"  5.347521 ##############\n",
      "   37.     \"f90\"  5.347521 ##############\n",
      "   38.     \"f10\"  5.346553 ##############\n",
      "   39.     \"f69\"  5.346123 ##############\n",
      "   40.     \"f33\"  5.343538 ##############\n",
      "   41.     \"f73\"  5.342724 ##############\n",
      "   42.    \"f118\"  5.342683 ##############\n",
      "   43.    \"f103\"  5.341285 ##############\n",
      "   44.     \"f44\"  5.341233 ##############\n",
      "   45.      \"f4\"  5.338627 ##############\n",
      "   46.     \"f88\"  5.338028 ##############\n",
      "   47.     \"f97\"  5.335371 ##############\n",
      "   48.     \"f80\"  5.331392 ##############\n",
      "   49.     \"f18\"  5.331177 ##############\n",
      "   50.    \"f101\"  5.330832 ##############\n",
      "   51.     \"f46\"  5.330640 ##############\n",
      "   52.     \"f30\"  5.328786 ##############\n",
      "   53.     \"f60\"  5.327091 ##############\n",
      "   54.     \"f43\"  5.327091 ##############\n",
      "   55.    \"f108\"  5.327040 ##############\n",
      "   56.     \"f13\"  5.325893 ##############\n",
      "   57.    \"f110\"  5.322898 ##############\n",
      "   58.     \"f17\"  5.321270 ##############\n",
      "   59.     \"f54\"  5.319027 ##############\n",
      "   60.     \"f49\"  5.312575 ##############\n",
      "   61.    \"f109\"  5.311607 ##############\n",
      "   62.     \"f87\"  5.310427 ##############\n",
      "   63.    \"f111\"  5.310317 ##############\n",
      "   64.     \"f98\"  5.309991 ##############\n",
      "   65.     \"f67\"  5.308596 ##############\n",
      "   66.     \"f39\"  5.308020 ##############\n",
      "   67.     \"f81\"  5.305048 ##############\n",
      "   68.      \"f9\"  5.304403 ##############\n",
      "   69.     \"f53\"  5.303543 ##############\n",
      "   70.     \"f23\"  5.302560 ##############\n",
      "   71.    \"f117\"  5.300747 ##############\n",
      "   72.     \"f75\"  5.295048 ##############\n",
      "   73.     \"f27\"  5.290625 #############\n",
      "   74.     \"f42\"  5.290424 #############\n",
      "   75.     \"f38\"  5.289442 #############\n",
      "   76.     \"f86\"  5.284554 #############\n",
      "   77.    \"f113\"  5.282145 #############\n",
      "   78.     \"f12\"  5.280509 #############\n",
      "   79.     \"f79\"  5.279950 #############\n",
      "   80.    \"f114\"  5.276123 #############\n",
      "   81.     \"f62\"  5.275586 #############\n",
      "   82.     \"f36\"  5.270424 #############\n",
      "   83.     \"f52\"  5.269020 #############\n",
      "   84.     \"f99\"  5.263981 #############\n",
      "   85.      \"f6\"  5.262700 #############\n",
      "   86.     \"f31\"  5.262683 #############\n",
      "   87.     \"f78\"  5.262659 #############\n",
      "   88.     \"f29\"  5.261930 #############\n",
      "   89.     \"f61\"  5.255693 #############\n",
      "   90.     \"f48\"  5.255025 #############\n",
      "   91.      \"f3\"  5.252683 #############\n",
      "   92.     \"f37\"  5.241822 #############\n",
      "   93.     \"f96\"  5.234403 #############\n",
      "   94.     \"f50\"  5.233881 #############\n",
      "   95.      \"f7\"  5.232683 #############\n",
      "   96.     \"f71\"  5.227002 #############\n",
      "   97.    \"f106\"  5.225610 ############\n",
      "   98.    \"f107\"  5.214618 ############\n",
      "   99.      \"f2\"  5.212790 ############\n",
      "  100.    \"f102\"  5.211822 ############\n",
      "  101.      \"f5\"  5.211070 ############\n",
      "  102.     \"f11\"  5.207097 ############\n",
      "  103.     \"f92\"  5.200473 ############\n",
      "  104.     \"f32\"  5.197665 ############\n",
      "  105.     \"f95\"  5.196483 ############\n",
      "  106.     \"f34\"  5.196354 ############\n",
      "  107.     \"f77\"  5.195970 ############\n",
      "  108.     \"f57\"  5.195763 ############\n",
      "  109.     \"f24\"  5.194080 ############\n",
      "  110.     \"f16\"  5.193650 ############\n",
      "  111.     \"f28\"  5.189134 ############\n",
      "  112.     \"f25\"  5.162675 ############\n",
      "  113.      \"f8\"  5.155117 ###########\n",
      "  114.     \"f21\"  5.150121 ###########\n",
      "  115.      \"f1\"  5.143343 ###########\n",
      "  116.     \"f65\"  5.128397 ###########\n",
      "  117.     \"f47\"  5.117736 ###########\n",
      "  118.     \"f35\"  5.114403 ###########\n",
      "  119.     \"f45\"  5.101616 ###########\n",
      "  120.     \"f70\"  5.043220 ##########\n",
      "  121.     \"f40\"  5.019672 #########\n",
      "  122.     \"nan\"  4.391541 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1.  \"nan\" 53.000000 ################\n",
      "    2.  \"f40\"  9.000000 ##\n",
      "    3.  \"f70\"  9.000000 ##\n",
      "    4.   \"f1\"  8.000000 ##\n",
      "    5.  \"f25\"  8.000000 ##\n",
      "    6.  \"f28\"  8.000000 ##\n",
      "    7.  \"f11\"  7.000000 #\n",
      "    8.  \"f24\"  7.000000 #\n",
      "    9.  \"f47\"  7.000000 #\n",
      "   10.  \"f65\"  7.000000 #\n",
      "   11. \"f102\"  6.000000 #\n",
      "   12.   \"f2\"  6.000000 #\n",
      "   13.  \"f21\"  6.000000 #\n",
      "   14.   \"f8\"  6.000000 #\n",
      "   15.  \"f32\"  5.000000 #\n",
      "   16.  \"f37\"  5.000000 #\n",
      "   17.  \"f45\"  5.000000 #\n",
      "   18. \"f106\"  4.000000 \n",
      "   19. \"f107\"  4.000000 \n",
      "   20. \"f113\"  4.000000 \n",
      "   21.  \"f16\"  4.000000 \n",
      "   22.  \"f35\"  4.000000 \n",
      "   23.  \"f57\"  4.000000 \n",
      "   24.  \"f67\"  4.000000 \n",
      "   25.  \"f92\"  4.000000 \n",
      "   26.  \"f95\"  4.000000 \n",
      "   27.  \"f99\"  4.000000 \n",
      "   28. \"f111\"  3.000000 \n",
      "   29. \"f114\"  3.000000 \n",
      "   30.  \"f23\"  3.000000 \n",
      "   31.  \"f29\"  3.000000 \n",
      "   32.  \"f34\"  3.000000 \n",
      "   33.  \"f36\"  3.000000 \n",
      "   34.  \"f39\"  3.000000 \n",
      "   35.   \"f5\"  3.000000 \n",
      "   36.  \"f50\"  3.000000 \n",
      "   37.   \"f6\"  3.000000 \n",
      "   38.  \"f61\"  3.000000 \n",
      "   39.  \"f71\"  3.000000 \n",
      "   40.  \"f75\"  3.000000 \n",
      "   41.  \"f77\"  3.000000 \n",
      "   42. \"f108\"  2.000000 \n",
      "   43. \"f117\"  2.000000 \n",
      "   44.  \"f18\"  2.000000 \n",
      "   45.  \"f27\"  2.000000 \n",
      "   46.   \"f3\"  2.000000 \n",
      "   47.  \"f38\"  2.000000 \n",
      "   48.   \"f4\"  2.000000 \n",
      "   49.  \"f42\"  2.000000 \n",
      "   50.  \"f52\"  2.000000 \n",
      "   51.  \"f54\"  2.000000 \n",
      "   52.   \"f7\"  2.000000 \n",
      "   53.  \"f78\"  2.000000 \n",
      "   54.   \"f9\"  2.000000 \n",
      "   55.  \"f94\"  2.000000 \n",
      "   56.  \"f96\"  2.000000 \n",
      "   57.  \"f98\"  2.000000 \n",
      "   58. \"f110\"  1.000000 \n",
      "   59. \"f112\"  1.000000 \n",
      "   60. \"f115\"  1.000000 \n",
      "   61. \"f116\"  1.000000 \n",
      "   62. \"f118\"  1.000000 \n",
      "   63.  \"f12\"  1.000000 \n",
      "   64.  \"f13\"  1.000000 \n",
      "   65.  \"f17\"  1.000000 \n",
      "   66.  \"f22\"  1.000000 \n",
      "   67.  \"f30\"  1.000000 \n",
      "   68.  \"f31\"  1.000000 \n",
      "   69.  \"f43\"  1.000000 \n",
      "   70.  \"f46\"  1.000000 \n",
      "   71.  \"f48\"  1.000000 \n",
      "   72.  \"f49\"  1.000000 \n",
      "   73.  \"f59\"  1.000000 \n",
      "   74.  \"f60\"  1.000000 \n",
      "   75.  \"f62\"  1.000000 \n",
      "   76.  \"f81\"  1.000000 \n",
      "   77.  \"f82\"  1.000000 \n",
      "   78.  \"f86\"  1.000000 \n",
      "   79.  \"f87\"  1.000000 \n",
      "   80.  \"f88\"  1.000000 \n",
      "   81.  \"f89\"  1.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1.  \"f40\" 225.000000 ################\n",
      "    2.  \"f70\" 158.000000 ##########\n",
      "    3.  \"nan\" 152.000000 ##########\n",
      "    4.  \"f35\" 150.000000 ##########\n",
      "    5.  \"f47\" 147.000000 ##########\n",
      "    6.  \"f45\" 137.000000 #########\n",
      "    7.   \"f1\" 134.000000 #########\n",
      "    8.   \"f5\" 122.000000 ########\n",
      "    9.  \"f21\" 121.000000 ########\n",
      "   10.  \"f34\" 120.000000 ########\n",
      "   11.   \"f8\" 117.000000 #######\n",
      "   12.   \"f3\" 116.000000 #######\n",
      "   13.  \"f57\" 115.000000 #######\n",
      "   14.  \"f95\" 115.000000 #######\n",
      "   15.  \"f92\" 113.000000 #######\n",
      "   16.  \"f71\" 111.000000 #######\n",
      "   17. \"f106\" 110.000000 #######\n",
      "   18.  \"f96\" 104.000000 ######\n",
      "   19.  \"f77\" 103.000000 ######\n",
      "   20. \"f107\" 102.000000 ######\n",
      "   21.  \"f65\" 99.000000 ######\n",
      "   22.  \"f48\" 96.000000 ######\n",
      "   23.   \"f7\" 96.000000 ######\n",
      "   24.  \"f31\" 95.000000 ######\n",
      "   25.  \"f32\" 95.000000 ######\n",
      "   26.  \"f52\" 94.000000 ######\n",
      "   27.  \"f62\" 93.000000 #####\n",
      "   28.  \"f86\" 93.000000 #####\n",
      "   29.  \"f79\" 92.000000 #####\n",
      "   30.   \"f2\" 91.000000 #####\n",
      "   31.  \"f50\" 91.000000 #####\n",
      "   32.  \"f16\" 88.000000 #####\n",
      "   33.  \"f25\" 88.000000 #####\n",
      "   34. \"f102\" 86.000000 #####\n",
      "   35.  \"f28\" 86.000000 #####\n",
      "   36.  \"f61\" 85.000000 #####\n",
      "   37.  \"f27\" 83.000000 #####\n",
      "   38.  \"f99\" 83.000000 #####\n",
      "   39.   \"f9\" 82.000000 #####\n",
      "   40.  \"f53\" 81.000000 #####\n",
      "   41.  \"f24\" 80.000000 ####\n",
      "   42.  \"f38\" 79.000000 ####\n",
      "   43.   \"f6\" 78.000000 ####\n",
      "   44.  \"f75\" 78.000000 ####\n",
      "   45.  \"f36\" 77.000000 ####\n",
      "   46.  \"f15\" 76.000000 ####\n",
      "   47.  \"f12\" 75.000000 ####\n",
      "   48.  \"f78\" 75.000000 ####\n",
      "   49. \"f114\" 74.000000 ####\n",
      "   50.  \"f13\" 74.000000 ####\n",
      "   51.  \"f33\" 73.000000 ####\n",
      "   52.  \"f37\" 73.000000 ####\n",
      "   53.  \"f42\" 73.000000 ####\n",
      "   54.  \"f29\" 71.000000 ####\n",
      "   55. \"f113\" 70.000000 ####\n",
      "   56.  \"f17\" 70.000000 ####\n",
      "   57.  \"f81\" 69.000000 ####\n",
      "   58. \"f108\" 67.000000 ###\n",
      "   59.  \"f11\" 66.000000 ###\n",
      "   60.  \"f46\" 66.000000 ###\n",
      "   61.  \"f74\" 66.000000 ###\n",
      "   62.  \"f10\" 65.000000 ###\n",
      "   63. \"f110\" 65.000000 ###\n",
      "   64. \"f111\" 65.000000 ###\n",
      "   65.  \"f18\" 65.000000 ###\n",
      "   66.  \"f87\" 65.000000 ###\n",
      "   67.  \"f80\" 64.000000 ###\n",
      "   68.  \"f98\" 64.000000 ###\n",
      "   69.  \"f23\" 63.000000 ###\n",
      "   70.  \"f30\" 63.000000 ###\n",
      "   71.  \"f39\" 63.000000 ###\n",
      "   72.  \"f54\" 63.000000 ###\n",
      "   73.  \"f56\" 63.000000 ###\n",
      "   74.  \"f97\" 63.000000 ###\n",
      "   75. \"f109\" 62.000000 ###\n",
      "   76. \"f112\" 61.000000 ###\n",
      "   77.  \"f55\" 61.000000 ###\n",
      "   78.  \"f90\" 61.000000 ###\n",
      "   79.  \"f44\" 60.000000 ###\n",
      "   80.  \"f49\" 60.000000 ###\n",
      "   81. \"f101\" 59.000000 ###\n",
      "   82. \"f103\" 58.000000 ###\n",
      "   83. \"f117\" 58.000000 ###\n",
      "   84. \"f104\" 57.000000 ###\n",
      "   85.  \"f82\" 57.000000 ###\n",
      "   86. \"f100\" 56.000000 ###\n",
      "   87.  \"f22\" 55.000000 ###\n",
      "   88.  \"f68\" 55.000000 ###\n",
      "   89.  \"f73\" 55.000000 ###\n",
      "   90.  \"f93\" 55.000000 ###\n",
      "   91.   \"f4\" 54.000000 ##\n",
      "   92.  \"f60\" 54.000000 ##\n",
      "   93.  \"f69\" 54.000000 ##\n",
      "   94.  \"f14\" 53.000000 ##\n",
      "   95.  \"f84\" 53.000000 ##\n",
      "   96.  \"f43\" 51.000000 ##\n",
      "   97.  \"f19\" 50.000000 ##\n",
      "   98.  \"f63\" 50.000000 ##\n",
      "   99.  \"f91\" 49.000000 ##\n",
      "  100.  \"f72\" 48.000000 ##\n",
      "  101.  \"f89\" 48.000000 ##\n",
      "  102. \"f118\" 47.000000 ##\n",
      "  103.  \"f51\" 47.000000 ##\n",
      "  104.  \"f59\" 47.000000 ##\n",
      "  105.  \"f67\" 47.000000 ##\n",
      "  106.  \"f64\" 46.000000 ##\n",
      "  107. \"f105\" 43.000000 ##\n",
      "  108.  \"f66\" 42.000000 ##\n",
      "  109.  \"f83\" 42.000000 ##\n",
      "  110. \"f115\" 41.000000 #\n",
      "  111. \"f116\" 41.000000 #\n",
      "  112.  \"f20\" 41.000000 #\n",
      "  113.  \"f94\" 41.000000 #\n",
      "  114.  \"f41\" 40.000000 #\n",
      "  115.  \"f26\" 39.000000 #\n",
      "  116.  \"f88\" 36.000000 #\n",
      "  117.  \"f76\" 35.000000 #\n",
      "  118.  \"f58\" 30.000000 #\n",
      "  119.  \"f85\" 24.000000 \n",
      "  120.  \"std\" 21.000000 \n",
      "  121.  \"var\" 15.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1.  \"nan\" 339729.131455 ################\n",
      "    2.  \"f40\" 1275.032112 \n",
      "    3.  \"f70\" 828.326178 \n",
      "    4.  \"f35\" 668.515474 \n",
      "    5.  \"f47\" 656.645281 \n",
      "    6.  \"f45\" 587.477810 \n",
      "    7.  \"f21\" 534.892854 \n",
      "    8.  \"f34\" 497.126827 \n",
      "    9.   \"f1\" 488.247322 \n",
      "   10.   \"f3\" 481.624669 \n",
      "   11.  \"f95\" 450.799528 \n",
      "   12.   \"f8\" 435.375450 \n",
      "   13.  \"f96\" 434.942763 \n",
      "   14.  \"f65\" 422.534807 \n",
      "   15.  \"f77\" 412.082688 \n",
      "   16.  \"f57\" 406.463234 \n",
      "   17.   \"f5\" 399.811087 \n",
      "   18. \"f107\" 382.663092 \n",
      "   19. \"f106\" 373.604813 \n",
      "   20.  \"f31\" 356.016556 \n",
      "   21.  \"f16\" 354.506589 \n",
      "   22.  \"f71\" 348.994833 \n",
      "   23.  \"f92\" 348.239843 \n",
      "   24.  \"f32\" 347.860486 \n",
      "   25.   \"f7\" 323.404810 \n",
      "   26.  \"f62\" 309.814241 \n",
      "   27.  \"f52\" 305.301958 \n",
      "   28.  \"f28\" 287.737795 \n",
      "   29.  \"f48\" 286.528622 \n",
      "   30.  \"f86\" 280.101689 \n",
      "   31.  \"f79\" 279.267752 \n",
      "   32.   \"f9\" 275.825616 \n",
      "   33.   \"f2\" 275.095458 \n",
      "   34.  \"f38\" 274.339969 \n",
      "   35.  \"f50\" 273.046553 \n",
      "   36.  \"f36\" 272.337490 \n",
      "   37.  \"f75\" 270.030947 \n",
      "   38.  \"f25\" 269.694930 \n",
      "   39.  \"f78\" 260.562802 \n",
      "   40. \"f102\" 259.696100 \n",
      "   41.  \"f61\" 257.002002 \n",
      "   42.  \"f99\" 248.110396 \n",
      "   43.  \"f27\" 244.794958 \n",
      "   44.  \"f53\" 244.430645 \n",
      "   45.   \"f6\" 244.195287 \n",
      "   46.  \"f24\" 234.050095 \n",
      "   47.  \"f12\" 225.163524 \n",
      "   48.  \"f15\" 220.931082 \n",
      "   49. \"f114\" 217.632671 \n",
      "   50.  \"f42\" 216.261944 \n",
      "   51.  \"f37\" 215.665299 \n",
      "   52.  \"f13\" 204.200276 \n",
      "   53.  \"f97\" 195.252143 \n",
      "   54. \"f113\" 193.399866 \n",
      "   55. \"f103\" 191.406155 \n",
      "   56.  \"f46\" 190.105021 \n",
      "   57. \"f109\" 189.649373 \n",
      "   58. \"f108\" 189.458123 \n",
      "   59.  \"f81\" 188.877942 \n",
      "   60. \"f111\" 187.871641 \n",
      "   61.  \"f10\" 186.426219 \n",
      "   62.  \"f11\" 184.935740 \n",
      "   63.  \"f18\" 183.983296 \n",
      "   64. \"f110\" 182.483445 \n",
      "   65.  \"f17\" 181.832488 \n",
      "   66.  \"f39\" 181.666209 \n",
      "   67.  \"f44\" 180.866146 \n",
      "   68.  \"f30\" 180.041644 \n",
      "   69.  \"f98\" 178.683092 \n",
      "   70.  \"f33\" 177.347421 \n",
      "   71.  \"f74\" 176.794153 \n",
      "   72.  \"f80\" 176.703463 \n",
      "   73.  \"f23\" 174.036244 \n",
      "   74. \"f104\" 173.868913 \n",
      "   75.  \"f56\" 171.807773 \n",
      "   76.  \"f90\" 170.854750 \n",
      "   77.  \"f29\" 168.368193 \n",
      "   78. \"f112\" 167.591877 \n",
      "   79.  \"f54\" 167.369258 \n",
      "   80.  \"f87\" 165.339540 \n",
      "   81.  \"f60\" 163.302077 \n",
      "   82. \"f101\" 162.242264 \n",
      "   83.  \"f49\" 160.248446 \n",
      "   84.  \"f93\" 155.831335 \n",
      "   85.  \"f55\" 155.723034 \n",
      "   86.   \"f4\" 154.004584 \n",
      "   87. \"f117\" 154.001758 \n",
      "   88.  \"f14\" 152.273868 \n",
      "   89. \"f100\" 148.472961 \n",
      "   90.  \"f82\" 146.464903 \n",
      "   91.  \"f22\" 146.066311 \n",
      "   92.  \"f68\" 145.752284 \n",
      "   93.  \"f69\" 144.849960 \n",
      "   94.  \"f73\" 143.478092 \n",
      "   95.  \"f84\" 138.064273 \n",
      "   96.  \"f91\" 134.569955 \n",
      "   97.  \"f89\" 132.703584 \n",
      "   98. \"f118\" 130.611026 \n",
      "   99.  \"f63\" 128.395279 \n",
      "  100.  \"f19\" 126.770139 \n",
      "  101.  \"f72\" 124.020610 \n",
      "  102.  \"f43\" 123.535756 \n",
      "  103.  \"f67\" 118.657166 \n",
      "  104.  \"f51\" 118.571496 \n",
      "  105.  \"f59\" 114.945570 \n",
      "  106.  \"f64\" 114.370702 \n",
      "  107. \"f116\" 111.724941 \n",
      "  108. \"f105\" 110.737343 \n",
      "  109.  \"f83\" 106.836458 \n",
      "  110.  \"f66\" 106.770746 \n",
      "  111.  \"f20\" 105.002925 \n",
      "  112. \"f115\" 102.436139 \n",
      "  113.  \"f41\" 100.243984 \n",
      "  114.  \"f94\" 96.174136 \n",
      "  115.  \"f76\" 93.663482 \n",
      "  116.  \"f26\" 92.516459 \n",
      "  117.  \"f88\" 91.342290 \n",
      "  118.  \"f58\" 75.742553 \n",
      "  119.  \"f85\" 59.416610 \n",
      "  120.  \"std\" 48.477471 \n",
      "  121.  \"var\" 37.711125 \n",
      "\n",
      "\n",
      "\n",
      "Loss: BINOMIAL_LOG_LIKELIHOOD\n",
      "Validation loss value: 1.01568\n",
      "Number of trees per iteration: 1\n",
      "Node format: NOT_SET\n",
      "Number of trees: 300\n",
      "Total number of nodes: 18278\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 300 Average: 60.9267 StdDev: 0.804957\n",
      "Min: 49 Max: 61 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 49, 50)   1   0.33%   0.33%\n",
      "[ 50, 51)   0   0.00%   0.33%\n",
      "[ 51, 52)   0   0.00%   0.33%\n",
      "[ 52, 53)   0   0.00%   0.33%\n",
      "[ 53, 54)   0   0.00%   0.33%\n",
      "[ 54, 55)   0   0.00%   0.33%\n",
      "[ 55, 56)   1   0.33%   0.67%\n",
      "[ 56, 57)   0   0.00%   0.67%\n",
      "[ 57, 58)   1   0.33%   1.00%\n",
      "[ 58, 59)   0   0.00%   1.00%\n",
      "[ 59, 60)   0   0.00%   1.00%\n",
      "[ 60, 61)   0   0.00%   1.00%\n",
      "[ 61, 61] 297  99.00% 100.00% ##########\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 9289 Average: 5.41899 StdDev: 0.968849\n",
      "Min: 1 Max: 6 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 1, 2)    7   0.08%   0.08%\n",
      "[ 2, 3)  148   1.59%   1.67%\n",
      "[ 3, 4)  433   4.66%   6.33% #\n",
      "[ 4, 5)  972  10.46%  16.79% ##\n",
      "[ 5, 6) 1527  16.44%  33.23% ##\n",
      "[ 6, 6] 6202  66.77% 100.00% ##########\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 9289 Average: 0 StdDev: 0\n",
      "Min: 0 Max: 0 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 0, 0] 9289 100.00% 100.00% ##########\n",
      "\n",
      "Attribute in nodes:\n",
      "\t225 : f40 [NUMERICAL]\n",
      "\t158 : f70 [NUMERICAL]\n",
      "\t152 : nan [NUMERICAL]\n",
      "\t150 : f35 [NUMERICAL]\n",
      "\t147 : f47 [NUMERICAL]\n",
      "\t137 : f45 [NUMERICAL]\n",
      "\t134 : f1 [NUMERICAL]\n",
      "\t122 : f5 [NUMERICAL]\n",
      "\t121 : f21 [NUMERICAL]\n",
      "\t120 : f34 [NUMERICAL]\n",
      "\t117 : f8 [NUMERICAL]\n",
      "\t116 : f3 [NUMERICAL]\n",
      "\t115 : f95 [NUMERICAL]\n",
      "\t115 : f57 [NUMERICAL]\n",
      "\t113 : f92 [NUMERICAL]\n",
      "\t111 : f71 [NUMERICAL]\n",
      "\t110 : f106 [NUMERICAL]\n",
      "\t104 : f96 [NUMERICAL]\n",
      "\t103 : f77 [NUMERICAL]\n",
      "\t102 : f107 [NUMERICAL]\n",
      "\t99 : f65 [NUMERICAL]\n",
      "\t96 : f7 [NUMERICAL]\n",
      "\t96 : f48 [NUMERICAL]\n",
      "\t95 : f32 [NUMERICAL]\n",
      "\t95 : f31 [NUMERICAL]\n",
      "\t94 : f52 [NUMERICAL]\n",
      "\t93 : f86 [NUMERICAL]\n",
      "\t93 : f62 [NUMERICAL]\n",
      "\t92 : f79 [NUMERICAL]\n",
      "\t91 : f50 [NUMERICAL]\n",
      "\t91 : f2 [NUMERICAL]\n",
      "\t88 : f25 [NUMERICAL]\n",
      "\t88 : f16 [NUMERICAL]\n",
      "\t86 : f28 [NUMERICAL]\n",
      "\t86 : f102 [NUMERICAL]\n",
      "\t85 : f61 [NUMERICAL]\n",
      "\t83 : f99 [NUMERICAL]\n",
      "\t83 : f27 [NUMERICAL]\n",
      "\t82 : f9 [NUMERICAL]\n",
      "\t81 : f53 [NUMERICAL]\n",
      "\t80 : f24 [NUMERICAL]\n",
      "\t79 : f38 [NUMERICAL]\n",
      "\t78 : f75 [NUMERICAL]\n",
      "\t78 : f6 [NUMERICAL]\n",
      "\t77 : f36 [NUMERICAL]\n",
      "\t76 : f15 [NUMERICAL]\n",
      "\t75 : f78 [NUMERICAL]\n",
      "\t75 : f12 [NUMERICAL]\n",
      "\t74 : f13 [NUMERICAL]\n",
      "\t74 : f114 [NUMERICAL]\n",
      "\t73 : f42 [NUMERICAL]\n",
      "\t73 : f37 [NUMERICAL]\n",
      "\t73 : f33 [NUMERICAL]\n",
      "\t71 : f29 [NUMERICAL]\n",
      "\t70 : f17 [NUMERICAL]\n",
      "\t70 : f113 [NUMERICAL]\n",
      "\t69 : f81 [NUMERICAL]\n",
      "\t67 : f108 [NUMERICAL]\n",
      "\t66 : f74 [NUMERICAL]\n",
      "\t66 : f46 [NUMERICAL]\n",
      "\t66 : f11 [NUMERICAL]\n",
      "\t65 : f87 [NUMERICAL]\n",
      "\t65 : f18 [NUMERICAL]\n",
      "\t65 : f111 [NUMERICAL]\n",
      "\t65 : f110 [NUMERICAL]\n",
      "\t65 : f10 [NUMERICAL]\n",
      "\t64 : f98 [NUMERICAL]\n",
      "\t64 : f80 [NUMERICAL]\n",
      "\t63 : f97 [NUMERICAL]\n",
      "\t63 : f56 [NUMERICAL]\n",
      "\t63 : f54 [NUMERICAL]\n",
      "\t63 : f39 [NUMERICAL]\n",
      "\t63 : f30 [NUMERICAL]\n",
      "\t63 : f23 [NUMERICAL]\n",
      "\t62 : f109 [NUMERICAL]\n",
      "\t61 : f90 [NUMERICAL]\n",
      "\t61 : f55 [NUMERICAL]\n",
      "\t61 : f112 [NUMERICAL]\n",
      "\t60 : f49 [NUMERICAL]\n",
      "\t60 : f44 [NUMERICAL]\n",
      "\t59 : f101 [NUMERICAL]\n",
      "\t58 : f117 [NUMERICAL]\n",
      "\t58 : f103 [NUMERICAL]\n",
      "\t57 : f82 [NUMERICAL]\n",
      "\t57 : f104 [NUMERICAL]\n",
      "\t56 : f100 [NUMERICAL]\n",
      "\t55 : f93 [NUMERICAL]\n",
      "\t55 : f73 [NUMERICAL]\n",
      "\t55 : f68 [NUMERICAL]\n",
      "\t55 : f22 [NUMERICAL]\n",
      "\t54 : f69 [NUMERICAL]\n",
      "\t54 : f60 [NUMERICAL]\n",
      "\t54 : f4 [NUMERICAL]\n",
      "\t53 : f84 [NUMERICAL]\n",
      "\t53 : f14 [NUMERICAL]\n",
      "\t51 : f43 [NUMERICAL]\n",
      "\t50 : f63 [NUMERICAL]\n",
      "\t50 : f19 [NUMERICAL]\n",
      "\t49 : f91 [NUMERICAL]\n",
      "\t48 : f89 [NUMERICAL]\n",
      "\t48 : f72 [NUMERICAL]\n",
      "\t47 : f67 [NUMERICAL]\n",
      "\t47 : f59 [NUMERICAL]\n",
      "\t47 : f51 [NUMERICAL]\n",
      "\t47 : f118 [NUMERICAL]\n",
      "\t46 : f64 [NUMERICAL]\n",
      "\t43 : f105 [NUMERICAL]\n",
      "\t42 : f83 [NUMERICAL]\n",
      "\t42 : f66 [NUMERICAL]\n",
      "\t41 : f94 [NUMERICAL]\n",
      "\t41 : f20 [NUMERICAL]\n",
      "\t41 : f116 [NUMERICAL]\n",
      "\t41 : f115 [NUMERICAL]\n",
      "\t40 : f41 [NUMERICAL]\n",
      "\t39 : f26 [NUMERICAL]\n",
      "\t36 : f88 [NUMERICAL]\n",
      "\t35 : f76 [NUMERICAL]\n",
      "\t30 : f58 [NUMERICAL]\n",
      "\t24 : f85 [NUMERICAL]\n",
      "\t21 : std [NUMERICAL]\n",
      "\t15 : var [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t53 : nan [NUMERICAL]\n",
      "\t9 : f70 [NUMERICAL]\n",
      "\t9 : f40 [NUMERICAL]\n",
      "\t8 : f28 [NUMERICAL]\n",
      "\t8 : f25 [NUMERICAL]\n",
      "\t8 : f1 [NUMERICAL]\n",
      "\t7 : f65 [NUMERICAL]\n",
      "\t7 : f47 [NUMERICAL]\n",
      "\t7 : f24 [NUMERICAL]\n",
      "\t7 : f11 [NUMERICAL]\n",
      "\t6 : f8 [NUMERICAL]\n",
      "\t6 : f21 [NUMERICAL]\n",
      "\t6 : f2 [NUMERICAL]\n",
      "\t6 : f102 [NUMERICAL]\n",
      "\t5 : f45 [NUMERICAL]\n",
      "\t5 : f37 [NUMERICAL]\n",
      "\t5 : f32 [NUMERICAL]\n",
      "\t4 : f99 [NUMERICAL]\n",
      "\t4 : f95 [NUMERICAL]\n",
      "\t4 : f92 [NUMERICAL]\n",
      "\t4 : f67 [NUMERICAL]\n",
      "\t4 : f57 [NUMERICAL]\n",
      "\t4 : f35 [NUMERICAL]\n",
      "\t4 : f16 [NUMERICAL]\n",
      "\t4 : f113 [NUMERICAL]\n",
      "\t4 : f107 [NUMERICAL]\n",
      "\t4 : f106 [NUMERICAL]\n",
      "\t3 : f77 [NUMERICAL]\n",
      "\t3 : f75 [NUMERICAL]\n",
      "\t3 : f71 [NUMERICAL]\n",
      "\t3 : f61 [NUMERICAL]\n",
      "\t3 : f6 [NUMERICAL]\n",
      "\t3 : f50 [NUMERICAL]\n",
      "\t3 : f5 [NUMERICAL]\n",
      "\t3 : f39 [NUMERICAL]\n",
      "\t3 : f36 [NUMERICAL]\n",
      "\t3 : f34 [NUMERICAL]\n",
      "\t3 : f29 [NUMERICAL]\n",
      "\t3 : f23 [NUMERICAL]\n",
      "\t3 : f114 [NUMERICAL]\n",
      "\t3 : f111 [NUMERICAL]\n",
      "\t2 : f98 [NUMERICAL]\n",
      "\t2 : f96 [NUMERICAL]\n",
      "\t2 : f94 [NUMERICAL]\n",
      "\t2 : f9 [NUMERICAL]\n",
      "\t2 : f78 [NUMERICAL]\n",
      "\t2 : f7 [NUMERICAL]\n",
      "\t2 : f54 [NUMERICAL]\n",
      "\t2 : f52 [NUMERICAL]\n",
      "\t2 : f42 [NUMERICAL]\n",
      "\t2 : f4 [NUMERICAL]\n",
      "\t2 : f38 [NUMERICAL]\n",
      "\t2 : f3 [NUMERICAL]\n",
      "\t2 : f27 [NUMERICAL]\n",
      "\t2 : f18 [NUMERICAL]\n",
      "\t2 : f117 [NUMERICAL]\n",
      "\t2 : f108 [NUMERICAL]\n",
      "\t1 : f89 [NUMERICAL]\n",
      "\t1 : f88 [NUMERICAL]\n",
      "\t1 : f87 [NUMERICAL]\n",
      "\t1 : f86 [NUMERICAL]\n",
      "\t1 : f82 [NUMERICAL]\n",
      "\t1 : f81 [NUMERICAL]\n",
      "\t1 : f62 [NUMERICAL]\n",
      "\t1 : f60 [NUMERICAL]\n",
      "\t1 : f59 [NUMERICAL]\n",
      "\t1 : f49 [NUMERICAL]\n",
      "\t1 : f48 [NUMERICAL]\n",
      "\t1 : f46 [NUMERICAL]\n",
      "\t1 : f43 [NUMERICAL]\n",
      "\t1 : f31 [NUMERICAL]\n",
      "\t1 : f30 [NUMERICAL]\n",
      "\t1 : f22 [NUMERICAL]\n",
      "\t1 : f17 [NUMERICAL]\n",
      "\t1 : f13 [NUMERICAL]\n",
      "\t1 : f12 [NUMERICAL]\n",
      "\t1 : f118 [NUMERICAL]\n",
      "\t1 : f116 [NUMERICAL]\n",
      "\t1 : f115 [NUMERICAL]\n",
      "\t1 : f112 [NUMERICAL]\n",
      "\t1 : f110 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t91 : nan [NUMERICAL]\n",
      "\t30 : f70 [NUMERICAL]\n",
      "\t29 : f40 [NUMERICAL]\n",
      "\t22 : f1 [NUMERICAL]\n",
      "\t20 : f35 [NUMERICAL]\n",
      "\t18 : f45 [NUMERICAL]\n",
      "\t18 : f34 [NUMERICAL]\n",
      "\t18 : f106 [NUMERICAL]\n",
      "\t17 : f65 [NUMERICAL]\n",
      "\t16 : f47 [NUMERICAL]\n",
      "\t16 : f32 [NUMERICAL]\n",
      "\t15 : f24 [NUMERICAL]\n",
      "\t14 : f95 [NUMERICAL]\n",
      "\t14 : f25 [NUMERICAL]\n",
      "\t14 : f21 [NUMERICAL]\n",
      "\t13 : f96 [NUMERICAL]\n",
      "\t13 : f5 [NUMERICAL]\n",
      "\t13 : f29 [NUMERICAL]\n",
      "\t12 : f8 [NUMERICAL]\n",
      "\t12 : f57 [NUMERICAL]\n",
      "\t12 : f16 [NUMERICAL]\n",
      "\t11 : f92 [NUMERICAL]\n",
      "\t11 : f77 [NUMERICAL]\n",
      "\t11 : f37 [NUMERICAL]\n",
      "\t11 : f28 [NUMERICAL]\n",
      "\t11 : f11 [NUMERICAL]\n",
      "\t11 : f102 [NUMERICAL]\n",
      "\t10 : f7 [NUMERICAL]\n",
      "\t10 : f12 [NUMERICAL]\n",
      "\t10 : f107 [NUMERICAL]\n",
      "\t9 : f71 [NUMERICAL]\n",
      "\t9 : f50 [NUMERICAL]\n",
      "\t9 : f114 [NUMERICAL]\n",
      "\t8 : f80 [NUMERICAL]\n",
      "\t8 : f52 [NUMERICAL]\n",
      "\t8 : f42 [NUMERICAL]\n",
      "\t8 : f31 [NUMERICAL]\n",
      "\t8 : f2 [NUMERICAL]\n",
      "\t7 : f99 [NUMERICAL]\n",
      "\t7 : f78 [NUMERICAL]\n",
      "\t7 : f49 [NUMERICAL]\n",
      "\t7 : f48 [NUMERICAL]\n",
      "\t7 : f17 [NUMERICAL]\n",
      "\t7 : f117 [NUMERICAL]\n",
      "\t7 : f113 [NUMERICAL]\n",
      "\t7 : f103 [NUMERICAL]\n",
      "\t6 : f98 [NUMERICAL]\n",
      "\t6 : f86 [NUMERICAL]\n",
      "\t6 : f79 [NUMERICAL]\n",
      "\t6 : f73 [NUMERICAL]\n",
      "\t6 : f67 [NUMERICAL]\n",
      "\t6 : f6 [NUMERICAL]\n",
      "\t6 : f53 [NUMERICAL]\n",
      "\t6 : f43 [NUMERICAL]\n",
      "\t6 : f33 [NUMERICAL]\n",
      "\t6 : f3 [NUMERICAL]\n",
      "\t5 : f87 [NUMERICAL]\n",
      "\t5 : f81 [NUMERICAL]\n",
      "\t5 : f75 [NUMERICAL]\n",
      "\t5 : f62 [NUMERICAL]\n",
      "\t5 : f61 [NUMERICAL]\n",
      "\t5 : f56 [NUMERICAL]\n",
      "\t5 : f54 [NUMERICAL]\n",
      "\t5 : f4 [NUMERICAL]\n",
      "\t5 : f38 [NUMERICAL]\n",
      "\t5 : f36 [NUMERICAL]\n",
      "\t5 : f30 [NUMERICAL]\n",
      "\t5 : f27 [NUMERICAL]\n",
      "\t5 : f118 [NUMERICAL]\n",
      "\t5 : f10 [NUMERICAL]\n",
      "\t4 : f97 [NUMERICAL]\n",
      "\t4 : f94 [NUMERICAL]\n",
      "\t4 : f88 [NUMERICAL]\n",
      "\t4 : f83 [NUMERICAL]\n",
      "\t4 : f82 [NUMERICAL]\n",
      "\t4 : f74 [NUMERICAL]\n",
      "\t4 : f46 [NUMERICAL]\n",
      "\t4 : f39 [NUMERICAL]\n",
      "\t4 : f23 [NUMERICAL]\n",
      "\t4 : f18 [NUMERICAL]\n",
      "\t4 : f13 [NUMERICAL]\n",
      "\t4 : f111 [NUMERICAL]\n",
      "\t4 : f110 [NUMERICAL]\n",
      "\t4 : f109 [NUMERICAL]\n",
      "\t4 : f101 [NUMERICAL]\n",
      "\t3 : f91 [NUMERICAL]\n",
      "\t3 : f9 [NUMERICAL]\n",
      "\t3 : f68 [NUMERICAL]\n",
      "\t3 : f64 [NUMERICAL]\n",
      "\t3 : f60 [NUMERICAL]\n",
      "\t3 : f44 [NUMERICAL]\n",
      "\t3 : f115 [NUMERICAL]\n",
      "\t2 : f90 [NUMERICAL]\n",
      "\t2 : f89 [NUMERICAL]\n",
      "\t2 : f84 [NUMERICAL]\n",
      "\t2 : f69 [NUMERICAL]\n",
      "\t2 : f59 [NUMERICAL]\n",
      "\t2 : f55 [NUMERICAL]\n",
      "\t2 : f41 [NUMERICAL]\n",
      "\t2 : f26 [NUMERICAL]\n",
      "\t2 : f20 [NUMERICAL]\n",
      "\t2 : f116 [NUMERICAL]\n",
      "\t2 : f112 [NUMERICAL]\n",
      "\t2 : f108 [NUMERICAL]\n",
      "\t2 : f105 [NUMERICAL]\n",
      "\t2 : f104 [NUMERICAL]\n",
      "\t1 : var [NUMERICAL]\n",
      "\t1 : f93 [NUMERICAL]\n",
      "\t1 : f85 [NUMERICAL]\n",
      "\t1 : f72 [NUMERICAL]\n",
      "\t1 : f58 [NUMERICAL]\n",
      "\t1 : f22 [NUMERICAL]\n",
      "\t1 : f19 [NUMERICAL]\n",
      "\t1 : f15 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t98 : nan [NUMERICAL]\n",
      "\t65 : f40 [NUMERICAL]\n",
      "\t59 : f70 [NUMERICAL]\n",
      "\t47 : f1 [NUMERICAL]\n",
      "\t42 : f47 [NUMERICAL]\n",
      "\t41 : f45 [NUMERICAL]\n",
      "\t40 : f35 [NUMERICAL]\n",
      "\t36 : f65 [NUMERICAL]\n",
      "\t35 : f21 [NUMERICAL]\n",
      "\t34 : f5 [NUMERICAL]\n",
      "\t31 : f106 [NUMERICAL]\n",
      "\t30 : f8 [NUMERICAL]\n",
      "\t27 : f96 [NUMERICAL]\n",
      "\t27 : f57 [NUMERICAL]\n",
      "\t27 : f42 [NUMERICAL]\n",
      "\t27 : f32 [NUMERICAL]\n",
      "\t26 : f34 [NUMERICAL]\n",
      "\t24 : f16 [NUMERICAL]\n",
      "\t23 : f7 [NUMERICAL]\n",
      "\t22 : f95 [NUMERICAL]\n",
      "\t22 : f71 [NUMERICAL]\n",
      "\t22 : f29 [NUMERICAL]\n",
      "\t22 : f11 [NUMERICAL]\n",
      "\t21 : f77 [NUMERICAL]\n",
      "\t21 : f3 [NUMERICAL]\n",
      "\t21 : f28 [NUMERICAL]\n",
      "\t20 : f92 [NUMERICAL]\n",
      "\t20 : f78 [NUMERICAL]\n",
      "\t20 : f52 [NUMERICAL]\n",
      "\t20 : f50 [NUMERICAL]\n",
      "\t20 : f48 [NUMERICAL]\n",
      "\t20 : f25 [NUMERICAL]\n",
      "\t19 : f86 [NUMERICAL]\n",
      "\t19 : f107 [NUMERICAL]\n",
      "\t18 : f33 [NUMERICAL]\n",
      "\t18 : f24 [NUMERICAL]\n",
      "\t17 : f79 [NUMERICAL]\n",
      "\t17 : f31 [NUMERICAL]\n",
      "\t17 : f2 [NUMERICAL]\n",
      "\t17 : f12 [NUMERICAL]\n",
      "\t17 : f114 [NUMERICAL]\n",
      "\t17 : f102 [NUMERICAL]\n",
      "\t16 : f49 [NUMERICAL]\n",
      "\t16 : f37 [NUMERICAL]\n",
      "\t16 : f117 [NUMERICAL]\n",
      "\t15 : f87 [NUMERICAL]\n",
      "\t15 : f80 [NUMERICAL]\n",
      "\t15 : f62 [NUMERICAL]\n",
      "\t15 : f61 [NUMERICAL]\n",
      "\t15 : f6 [NUMERICAL]\n",
      "\t15 : f36 [NUMERICAL]\n",
      "\t14 : f99 [NUMERICAL]\n",
      "\t14 : f9 [NUMERICAL]\n",
      "\t14 : f27 [NUMERICAL]\n",
      "\t14 : f18 [NUMERICAL]\n",
      "\t13 : f97 [NUMERICAL]\n",
      "\t13 : f81 [NUMERICAL]\n",
      "\t13 : f75 [NUMERICAL]\n",
      "\t13 : f53 [NUMERICAL]\n",
      "\t13 : f101 [NUMERICAL]\n",
      "\t12 : f98 [NUMERICAL]\n",
      "\t12 : f90 [NUMERICAL]\n",
      "\t12 : f68 [NUMERICAL]\n",
      "\t12 : f55 [NUMERICAL]\n",
      "\t12 : f43 [NUMERICAL]\n",
      "\t12 : f23 [NUMERICAL]\n",
      "\t12 : f15 [NUMERICAL]\n",
      "\t12 : f118 [NUMERICAL]\n",
      "\t12 : f111 [NUMERICAL]\n",
      "\t11 : f54 [NUMERICAL]\n",
      "\t11 : f17 [NUMERICAL]\n",
      "\t11 : f13 [NUMERICAL]\n",
      "\t11 : f109 [NUMERICAL]\n",
      "\t11 : f103 [NUMERICAL]\n",
      "\t11 : f10 [NUMERICAL]\n",
      "\t10 : f82 [NUMERICAL]\n",
      "\t10 : f73 [NUMERICAL]\n",
      "\t10 : f44 [NUMERICAL]\n",
      "\t10 : f38 [NUMERICAL]\n",
      "\t10 : f113 [NUMERICAL]\n",
      "\t9 : f84 [NUMERICAL]\n",
      "\t9 : f83 [NUMERICAL]\n",
      "\t9 : f74 [NUMERICAL]\n",
      "\t9 : f67 [NUMERICAL]\n",
      "\t9 : f60 [NUMERICAL]\n",
      "\t9 : f56 [NUMERICAL]\n",
      "\t9 : f110 [NUMERICAL]\n",
      "\t8 : f94 [NUMERICAL]\n",
      "\t8 : f88 [NUMERICAL]\n",
      "\t8 : f46 [NUMERICAL]\n",
      "\t8 : f39 [NUMERICAL]\n",
      "\t8 : f30 [NUMERICAL]\n",
      "\t8 : f19 [NUMERICAL]\n",
      "\t8 : f112 [NUMERICAL]\n",
      "\t8 : f108 [NUMERICAL]\n",
      "\t8 : f100 [NUMERICAL]\n",
      "\t7 : f93 [NUMERICAL]\n",
      "\t7 : f91 [NUMERICAL]\n",
      "\t7 : f69 [NUMERICAL]\n",
      "\t7 : f66 [NUMERICAL]\n",
      "\t7 : f64 [NUMERICAL]\n",
      "\t7 : f116 [NUMERICAL]\n",
      "\t7 : f104 [NUMERICAL]\n",
      "\t6 : f72 [NUMERICAL]\n",
      "\t6 : f115 [NUMERICAL]\n",
      "\t5 : f89 [NUMERICAL]\n",
      "\t5 : f85 [NUMERICAL]\n",
      "\t5 : f76 [NUMERICAL]\n",
      "\t5 : f63 [NUMERICAL]\n",
      "\t5 : f41 [NUMERICAL]\n",
      "\t5 : f4 [NUMERICAL]\n",
      "\t5 : f26 [NUMERICAL]\n",
      "\t5 : f22 [NUMERICAL]\n",
      "\t5 : f20 [NUMERICAL]\n",
      "\t4 : var [NUMERICAL]\n",
      "\t4 : f59 [NUMERICAL]\n",
      "\t4 : f105 [NUMERICAL]\n",
      "\t3 : f51 [NUMERICAL]\n",
      "\t3 : f14 [NUMERICAL]\n",
      "\t2 : f58 [NUMERICAL]\n",
      "\t1 : std [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t111 : nan [NUMERICAL]\n",
      "\t107 : f40 [NUMERICAL]\n",
      "\t84 : f70 [NUMERICAL]\n",
      "\t83 : f47 [NUMERICAL]\n",
      "\t76 : f1 [NUMERICAL]\n",
      "\t70 : f45 [NUMERICAL]\n",
      "\t63 : f35 [NUMERICAL]\n",
      "\t61 : f5 [NUMERICAL]\n",
      "\t58 : f65 [NUMERICAL]\n",
      "\t58 : f21 [NUMERICAL]\n",
      "\t53 : f34 [NUMERICAL]\n",
      "\t50 : f8 [NUMERICAL]\n",
      "\t47 : f92 [NUMERICAL]\n",
      "\t46 : f106 [NUMERICAL]\n",
      "\t45 : f57 [NUMERICAL]\n",
      "\t44 : f96 [NUMERICAL]\n",
      "\t44 : f77 [NUMERICAL]\n",
      "\t44 : f50 [NUMERICAL]\n",
      "\t44 : f3 [NUMERICAL]\n",
      "\t43 : f7 [NUMERICAL]\n",
      "\t43 : f32 [NUMERICAL]\n",
      "\t42 : f95 [NUMERICAL]\n",
      "\t41 : f48 [NUMERICAL]\n",
      "\t41 : f25 [NUMERICAL]\n",
      "\t40 : f71 [NUMERICAL]\n",
      "\t40 : f2 [NUMERICAL]\n",
      "\t38 : f16 [NUMERICAL]\n",
      "\t37 : f62 [NUMERICAL]\n",
      "\t37 : f28 [NUMERICAL]\n",
      "\t37 : f107 [NUMERICAL]\n",
      "\t36 : f61 [NUMERICAL]\n",
      "\t36 : f52 [NUMERICAL]\n",
      "\t36 : f24 [NUMERICAL]\n",
      "\t35 : f42 [NUMERICAL]\n",
      "\t35 : f102 [NUMERICAL]\n",
      "\t34 : f75 [NUMERICAL]\n",
      "\t33 : f86 [NUMERICAL]\n",
      "\t33 : f29 [NUMERICAL]\n",
      "\t33 : f114 [NUMERICAL]\n",
      "\t32 : f36 [NUMERICAL]\n",
      "\t31 : f99 [NUMERICAL]\n",
      "\t31 : f79 [NUMERICAL]\n",
      "\t31 : f78 [NUMERICAL]\n",
      "\t31 : f53 [NUMERICAL]\n",
      "\t31 : f49 [NUMERICAL]\n",
      "\t31 : f12 [NUMERICAL]\n",
      "\t30 : f87 [NUMERICAL]\n",
      "\t30 : f31 [NUMERICAL]\n",
      "\t30 : f17 [NUMERICAL]\n",
      "\t29 : f33 [NUMERICAL]\n",
      "\t29 : f27 [NUMERICAL]\n",
      "\t29 : f11 [NUMERICAL]\n",
      "\t28 : f6 [NUMERICAL]\n",
      "\t28 : f38 [NUMERICAL]\n",
      "\t26 : f97 [NUMERICAL]\n",
      "\t26 : f81 [NUMERICAL]\n",
      "\t26 : f80 [NUMERICAL]\n",
      "\t26 : f37 [NUMERICAL]\n",
      "\t26 : f117 [NUMERICAL]\n",
      "\t26 : f113 [NUMERICAL]\n",
      "\t25 : f9 [NUMERICAL]\n",
      "\t25 : f46 [NUMERICAL]\n",
      "\t25 : f15 [NUMERICAL]\n",
      "\t25 : f109 [NUMERICAL]\n",
      "\t24 : f55 [NUMERICAL]\n",
      "\t24 : f18 [NUMERICAL]\n",
      "\t24 : f111 [NUMERICAL]\n",
      "\t23 : f23 [NUMERICAL]\n",
      "\t22 : f98 [NUMERICAL]\n",
      "\t22 : f44 [NUMERICAL]\n",
      "\t22 : f13 [NUMERICAL]\n",
      "\t22 : f110 [NUMERICAL]\n",
      "\t22 : f108 [NUMERICAL]\n",
      "\t22 : f100 [NUMERICAL]\n",
      "\t21 : f73 [NUMERICAL]\n",
      "\t21 : f10 [NUMERICAL]\n",
      "\t20 : f90 [NUMERICAL]\n",
      "\t20 : f74 [NUMERICAL]\n",
      "\t20 : f67 [NUMERICAL]\n",
      "\t20 : f60 [NUMERICAL]\n",
      "\t20 : f104 [NUMERICAL]\n",
      "\t20 : f103 [NUMERICAL]\n",
      "\t19 : f82 [NUMERICAL]\n",
      "\t19 : f54 [NUMERICAL]\n",
      "\t18 : f84 [NUMERICAL]\n",
      "\t18 : f69 [NUMERICAL]\n",
      "\t18 : f68 [NUMERICAL]\n",
      "\t18 : f56 [NUMERICAL]\n",
      "\t18 : f19 [NUMERICAL]\n",
      "\t17 : f83 [NUMERICAL]\n",
      "\t17 : f4 [NUMERICAL]\n",
      "\t17 : f118 [NUMERICAL]\n",
      "\t17 : f101 [NUMERICAL]\n",
      "\t16 : f91 [NUMERICAL]\n",
      "\t16 : f88 [NUMERICAL]\n",
      "\t16 : f43 [NUMERICAL]\n",
      "\t16 : f14 [NUMERICAL]\n",
      "\t16 : f112 [NUMERICAL]\n",
      "\t16 : f105 [NUMERICAL]\n",
      "\t15 : f93 [NUMERICAL]\n",
      "\t15 : f39 [NUMERICAL]\n",
      "\t15 : f30 [NUMERICAL]\n",
      "\t14 : f89 [NUMERICAL]\n",
      "\t14 : f59 [NUMERICAL]\n",
      "\t14 : f22 [NUMERICAL]\n",
      "\t13 : f94 [NUMERICAL]\n",
      "\t13 : f64 [NUMERICAL]\n",
      "\t13 : f41 [NUMERICAL]\n",
      "\t13 : f20 [NUMERICAL]\n",
      "\t12 : f116 [NUMERICAL]\n",
      "\t12 : f115 [NUMERICAL]\n",
      "\t10 : f85 [NUMERICAL]\n",
      "\t10 : f66 [NUMERICAL]\n",
      "\t10 : f63 [NUMERICAL]\n",
      "\t10 : f51 [NUMERICAL]\n",
      "\t9 : f72 [NUMERICAL]\n",
      "\t9 : f26 [NUMERICAL]\n",
      "\t8 : f58 [NUMERICAL]\n",
      "\t7 : var [NUMERICAL]\n",
      "\t7 : f76 [NUMERICAL]\n",
      "\t5 : std [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t225 : f40 [NUMERICAL]\n",
      "\t158 : f70 [NUMERICAL]\n",
      "\t152 : nan [NUMERICAL]\n",
      "\t150 : f35 [NUMERICAL]\n",
      "\t147 : f47 [NUMERICAL]\n",
      "\t137 : f45 [NUMERICAL]\n",
      "\t134 : f1 [NUMERICAL]\n",
      "\t122 : f5 [NUMERICAL]\n",
      "\t121 : f21 [NUMERICAL]\n",
      "\t120 : f34 [NUMERICAL]\n",
      "\t117 : f8 [NUMERICAL]\n",
      "\t116 : f3 [NUMERICAL]\n",
      "\t115 : f95 [NUMERICAL]\n",
      "\t115 : f57 [NUMERICAL]\n",
      "\t113 : f92 [NUMERICAL]\n",
      "\t111 : f71 [NUMERICAL]\n",
      "\t110 : f106 [NUMERICAL]\n",
      "\t104 : f96 [NUMERICAL]\n",
      "\t103 : f77 [NUMERICAL]\n",
      "\t102 : f107 [NUMERICAL]\n",
      "\t99 : f65 [NUMERICAL]\n",
      "\t96 : f7 [NUMERICAL]\n",
      "\t96 : f48 [NUMERICAL]\n",
      "\t95 : f32 [NUMERICAL]\n",
      "\t95 : f31 [NUMERICAL]\n",
      "\t94 : f52 [NUMERICAL]\n",
      "\t93 : f86 [NUMERICAL]\n",
      "\t93 : f62 [NUMERICAL]\n",
      "\t92 : f79 [NUMERICAL]\n",
      "\t91 : f50 [NUMERICAL]\n",
      "\t91 : f2 [NUMERICAL]\n",
      "\t88 : f25 [NUMERICAL]\n",
      "\t88 : f16 [NUMERICAL]\n",
      "\t86 : f28 [NUMERICAL]\n",
      "\t86 : f102 [NUMERICAL]\n",
      "\t85 : f61 [NUMERICAL]\n",
      "\t83 : f99 [NUMERICAL]\n",
      "\t83 : f27 [NUMERICAL]\n",
      "\t82 : f9 [NUMERICAL]\n",
      "\t81 : f53 [NUMERICAL]\n",
      "\t80 : f24 [NUMERICAL]\n",
      "\t79 : f38 [NUMERICAL]\n",
      "\t78 : f75 [NUMERICAL]\n",
      "\t78 : f6 [NUMERICAL]\n",
      "\t77 : f36 [NUMERICAL]\n",
      "\t76 : f15 [NUMERICAL]\n",
      "\t75 : f78 [NUMERICAL]\n",
      "\t75 : f12 [NUMERICAL]\n",
      "\t74 : f13 [NUMERICAL]\n",
      "\t74 : f114 [NUMERICAL]\n",
      "\t73 : f42 [NUMERICAL]\n",
      "\t73 : f37 [NUMERICAL]\n",
      "\t73 : f33 [NUMERICAL]\n",
      "\t71 : f29 [NUMERICAL]\n",
      "\t70 : f17 [NUMERICAL]\n",
      "\t70 : f113 [NUMERICAL]\n",
      "\t69 : f81 [NUMERICAL]\n",
      "\t67 : f108 [NUMERICAL]\n",
      "\t66 : f74 [NUMERICAL]\n",
      "\t66 : f46 [NUMERICAL]\n",
      "\t66 : f11 [NUMERICAL]\n",
      "\t65 : f87 [NUMERICAL]\n",
      "\t65 : f18 [NUMERICAL]\n",
      "\t65 : f111 [NUMERICAL]\n",
      "\t65 : f110 [NUMERICAL]\n",
      "\t65 : f10 [NUMERICAL]\n",
      "\t64 : f98 [NUMERICAL]\n",
      "\t64 : f80 [NUMERICAL]\n",
      "\t63 : f97 [NUMERICAL]\n",
      "\t63 : f56 [NUMERICAL]\n",
      "\t63 : f54 [NUMERICAL]\n",
      "\t63 : f39 [NUMERICAL]\n",
      "\t63 : f30 [NUMERICAL]\n",
      "\t63 : f23 [NUMERICAL]\n",
      "\t62 : f109 [NUMERICAL]\n",
      "\t61 : f90 [NUMERICAL]\n",
      "\t61 : f55 [NUMERICAL]\n",
      "\t61 : f112 [NUMERICAL]\n",
      "\t60 : f49 [NUMERICAL]\n",
      "\t60 : f44 [NUMERICAL]\n",
      "\t59 : f101 [NUMERICAL]\n",
      "\t58 : f117 [NUMERICAL]\n",
      "\t58 : f103 [NUMERICAL]\n",
      "\t57 : f82 [NUMERICAL]\n",
      "\t57 : f104 [NUMERICAL]\n",
      "\t56 : f100 [NUMERICAL]\n",
      "\t55 : f93 [NUMERICAL]\n",
      "\t55 : f73 [NUMERICAL]\n",
      "\t55 : f68 [NUMERICAL]\n",
      "\t55 : f22 [NUMERICAL]\n",
      "\t54 : f69 [NUMERICAL]\n",
      "\t54 : f60 [NUMERICAL]\n",
      "\t54 : f4 [NUMERICAL]\n",
      "\t53 : f84 [NUMERICAL]\n",
      "\t53 : f14 [NUMERICAL]\n",
      "\t51 : f43 [NUMERICAL]\n",
      "\t50 : f63 [NUMERICAL]\n",
      "\t50 : f19 [NUMERICAL]\n",
      "\t49 : f91 [NUMERICAL]\n",
      "\t48 : f89 [NUMERICAL]\n",
      "\t48 : f72 [NUMERICAL]\n",
      "\t47 : f67 [NUMERICAL]\n",
      "\t47 : f59 [NUMERICAL]\n",
      "\t47 : f51 [NUMERICAL]\n",
      "\t47 : f118 [NUMERICAL]\n",
      "\t46 : f64 [NUMERICAL]\n",
      "\t43 : f105 [NUMERICAL]\n",
      "\t42 : f83 [NUMERICAL]\n",
      "\t42 : f66 [NUMERICAL]\n",
      "\t41 : f94 [NUMERICAL]\n",
      "\t41 : f20 [NUMERICAL]\n",
      "\t41 : f116 [NUMERICAL]\n",
      "\t41 : f115 [NUMERICAL]\n",
      "\t40 : f41 [NUMERICAL]\n",
      "\t39 : f26 [NUMERICAL]\n",
      "\t36 : f88 [NUMERICAL]\n",
      "\t35 : f76 [NUMERICAL]\n",
      "\t30 : f58 [NUMERICAL]\n",
      "\t24 : f85 [NUMERICAL]\n",
      "\t21 : std [NUMERICAL]\n",
      "\t15 : var [NUMERICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t8989 : HigherCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t300 : HigherCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t893 : HigherCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t1931 : HigherCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t3574 : HigherCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t8989 : HigherCondition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOXx77stfmtD"
   },
   "source": [
    "We can access all this information using the model inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "z24tp8mpm4fI"
   },
   "outputs": [],
   "source": [
    "inspector = model_1.make_inspector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9vB4DW6m4fJ",
    "outputId": "4a3f214e-5b99-4a7c-849f-06d4f7e700b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 300 trees\n"
     ]
    }
   ],
   "source": [
    "print(\"Model contains {} trees\".format(inspector.num_trees()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "1haUC-6Jf1CG",
    "outputId": "9d767717-88ed-4f35-d19f-234425da1296"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"f1\" (1; #0),\n",
       " \"f10\" (1; #1),\n",
       " \"f100\" (1; #2),\n",
       " \"f101\" (1; #3),\n",
       " \"f102\" (1; #4),\n",
       " \"f103\" (1; #5),\n",
       " \"f104\" (1; #6),\n",
       " \"f105\" (1; #7),\n",
       " \"f106\" (1; #8),\n",
       " \"f107\" (1; #9),\n",
       " \"f108\" (1; #10),\n",
       " \"f109\" (1; #11),\n",
       " \"f11\" (1; #12),\n",
       " \"f110\" (1; #13),\n",
       " \"f111\" (1; #14),\n",
       " \"f112\" (1; #15),\n",
       " \"f113\" (1; #16),\n",
       " \"f114\" (1; #17),\n",
       " \"f115\" (1; #18),\n",
       " \"f116\" (1; #19),\n",
       " \"f117\" (1; #20),\n",
       " \"f118\" (1; #21),\n",
       " \"f12\" (1; #22),\n",
       " \"f13\" (1; #23),\n",
       " \"f14\" (1; #24),\n",
       " \"f15\" (1; #25),\n",
       " \"f16\" (1; #26),\n",
       " \"f17\" (1; #27),\n",
       " \"f18\" (1; #28),\n",
       " \"f19\" (1; #29),\n",
       " \"f2\" (1; #30),\n",
       " \"f20\" (1; #31),\n",
       " \"f21\" (1; #32),\n",
       " \"f22\" (1; #33),\n",
       " \"f23\" (1; #34),\n",
       " \"f24\" (1; #35),\n",
       " \"f25\" (1; #36),\n",
       " \"f26\" (1; #37),\n",
       " \"f27\" (1; #38),\n",
       " \"f28\" (1; #39),\n",
       " \"f29\" (1; #40),\n",
       " \"f3\" (1; #41),\n",
       " \"f30\" (1; #42),\n",
       " \"f31\" (1; #43),\n",
       " \"f32\" (1; #44),\n",
       " \"f33\" (1; #45),\n",
       " \"f34\" (1; #46),\n",
       " \"f35\" (1; #47),\n",
       " \"f36\" (1; #48),\n",
       " \"f37\" (1; #49),\n",
       " \"f38\" (1; #50),\n",
       " \"f39\" (1; #51),\n",
       " \"f4\" (1; #52),\n",
       " \"f40\" (1; #53),\n",
       " \"f41\" (1; #54),\n",
       " \"f42\" (1; #55),\n",
       " \"f43\" (1; #56),\n",
       " \"f44\" (1; #57),\n",
       " \"f45\" (1; #58),\n",
       " \"f46\" (1; #59),\n",
       " \"f47\" (1; #60),\n",
       " \"f48\" (1; #61),\n",
       " \"f49\" (1; #62),\n",
       " \"f5\" (1; #63),\n",
       " \"f50\" (1; #64),\n",
       " \"f51\" (1; #65),\n",
       " \"f52\" (1; #66),\n",
       " \"f53\" (1; #67),\n",
       " \"f54\" (1; #68),\n",
       " \"f55\" (1; #69),\n",
       " \"f56\" (1; #70),\n",
       " \"f57\" (1; #71),\n",
       " \"f58\" (1; #72),\n",
       " \"f59\" (1; #73),\n",
       " \"f6\" (1; #74),\n",
       " \"f60\" (1; #75),\n",
       " \"f61\" (1; #76),\n",
       " \"f62\" (1; #77),\n",
       " \"f63\" (1; #78),\n",
       " \"f64\" (1; #79),\n",
       " \"f65\" (1; #80),\n",
       " \"f66\" (1; #81),\n",
       " \"f67\" (1; #82),\n",
       " \"f68\" (1; #83),\n",
       " \"f69\" (1; #84),\n",
       " \"f7\" (1; #85),\n",
       " \"f70\" (1; #86),\n",
       " \"f71\" (1; #87),\n",
       " \"f72\" (1; #88),\n",
       " \"f73\" (1; #89),\n",
       " \"f74\" (1; #90),\n",
       " \"f75\" (1; #91),\n",
       " \"f76\" (1; #92),\n",
       " \"f77\" (1; #93),\n",
       " \"f78\" (1; #94),\n",
       " \"f79\" (1; #95),\n",
       " \"f8\" (1; #96),\n",
       " \"f80\" (1; #97),\n",
       " \"f81\" (1; #98),\n",
       " \"f82\" (1; #99),\n",
       " \"f83\" (1; #100),\n",
       " \"f84\" (1; #101),\n",
       " \"f85\" (1; #102),\n",
       " \"f86\" (1; #103),\n",
       " \"f87\" (1; #104),\n",
       " \"f88\" (1; #105),\n",
       " \"f89\" (1; #106),\n",
       " \"f9\" (1; #107),\n",
       " \"f90\" (1; #108),\n",
       " \"f91\" (1; #109),\n",
       " \"f92\" (1; #110),\n",
       " \"f93\" (1; #111),\n",
       " \"f94\" (1; #112),\n",
       " \"f95\" (1; #113),\n",
       " \"f96\" (1; #114),\n",
       " \"f97\" (1; #115),\n",
       " \"f98\" (1; #116),\n",
       " \"f99\" (1; #117),\n",
       " \"nan\" (1; #118),\n",
       " \"std\" (1; #119),\n",
       " \"var\" (1; #120)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "inspector.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "-xPKUWDPgBgR",
    "outputId": "84ced980-5a7e-4a1c-cc5d-b2da3d46a4ff"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'NUM_AS_ROOT': [(\"nan\" (1; #118), 53.0),\n",
       "  (\"f40\" (1; #53), 9.0),\n",
       "  (\"f70\" (1; #86), 9.0),\n",
       "  (\"f1\" (1; #0), 8.0),\n",
       "  (\"f25\" (1; #36), 8.0),\n",
       "  (\"f28\" (1; #39), 8.0),\n",
       "  (\"f11\" (1; #12), 7.0),\n",
       "  (\"f24\" (1; #35), 7.0),\n",
       "  (\"f47\" (1; #60), 7.0),\n",
       "  (\"f65\" (1; #80), 7.0),\n",
       "  (\"f102\" (1; #4), 6.0),\n",
       "  (\"f2\" (1; #30), 6.0),\n",
       "  (\"f21\" (1; #32), 6.0),\n",
       "  (\"f8\" (1; #96), 6.0),\n",
       "  (\"f32\" (1; #44), 5.0),\n",
       "  (\"f37\" (1; #49), 5.0),\n",
       "  (\"f45\" (1; #58), 5.0),\n",
       "  (\"f106\" (1; #8), 4.0),\n",
       "  (\"f107\" (1; #9), 4.0),\n",
       "  (\"f113\" (1; #16), 4.0),\n",
       "  (\"f16\" (1; #26), 4.0),\n",
       "  (\"f35\" (1; #47), 4.0),\n",
       "  (\"f57\" (1; #71), 4.0),\n",
       "  (\"f67\" (1; #82), 4.0),\n",
       "  (\"f92\" (1; #110), 4.0),\n",
       "  (\"f95\" (1; #113), 4.0),\n",
       "  (\"f99\" (1; #117), 4.0),\n",
       "  (\"f111\" (1; #14), 3.0),\n",
       "  (\"f114\" (1; #17), 3.0),\n",
       "  (\"f23\" (1; #34), 3.0),\n",
       "  (\"f29\" (1; #40), 3.0),\n",
       "  (\"f34\" (1; #46), 3.0),\n",
       "  (\"f36\" (1; #48), 3.0),\n",
       "  (\"f39\" (1; #51), 3.0),\n",
       "  (\"f5\" (1; #63), 3.0),\n",
       "  (\"f50\" (1; #64), 3.0),\n",
       "  (\"f6\" (1; #74), 3.0),\n",
       "  (\"f61\" (1; #76), 3.0),\n",
       "  (\"f71\" (1; #87), 3.0),\n",
       "  (\"f75\" (1; #91), 3.0),\n",
       "  (\"f77\" (1; #93), 3.0),\n",
       "  (\"f108\" (1; #10), 2.0),\n",
       "  (\"f117\" (1; #20), 2.0),\n",
       "  (\"f18\" (1; #28), 2.0),\n",
       "  (\"f27\" (1; #38), 2.0),\n",
       "  (\"f3\" (1; #41), 2.0),\n",
       "  (\"f38\" (1; #50), 2.0),\n",
       "  (\"f4\" (1; #52), 2.0),\n",
       "  (\"f42\" (1; #55), 2.0),\n",
       "  (\"f52\" (1; #66), 2.0),\n",
       "  (\"f54\" (1; #68), 2.0),\n",
       "  (\"f7\" (1; #85), 2.0),\n",
       "  (\"f78\" (1; #94), 2.0),\n",
       "  (\"f9\" (1; #107), 2.0),\n",
       "  (\"f94\" (1; #112), 2.0),\n",
       "  (\"f96\" (1; #114), 2.0),\n",
       "  (\"f98\" (1; #116), 2.0),\n",
       "  (\"f110\" (1; #13), 1.0),\n",
       "  (\"f112\" (1; #15), 1.0),\n",
       "  (\"f115\" (1; #18), 1.0),\n",
       "  (\"f116\" (1; #19), 1.0),\n",
       "  (\"f118\" (1; #21), 1.0),\n",
       "  (\"f12\" (1; #22), 1.0),\n",
       "  (\"f13\" (1; #23), 1.0),\n",
       "  (\"f17\" (1; #27), 1.0),\n",
       "  (\"f22\" (1; #33), 1.0),\n",
       "  (\"f30\" (1; #42), 1.0),\n",
       "  (\"f31\" (1; #43), 1.0),\n",
       "  (\"f43\" (1; #56), 1.0),\n",
       "  (\"f46\" (1; #59), 1.0),\n",
       "  (\"f48\" (1; #61), 1.0),\n",
       "  (\"f49\" (1; #62), 1.0),\n",
       "  (\"f59\" (1; #73), 1.0),\n",
       "  (\"f60\" (1; #75), 1.0),\n",
       "  (\"f62\" (1; #77), 1.0),\n",
       "  (\"f81\" (1; #98), 1.0),\n",
       "  (\"f82\" (1; #99), 1.0),\n",
       "  (\"f86\" (1; #103), 1.0),\n",
       "  (\"f87\" (1; #104), 1.0),\n",
       "  (\"f88\" (1; #105), 1.0),\n",
       "  (\"f89\" (1; #106), 1.0)],\n",
       " 'MEAN_MIN_DEPTH': [(\"__LABEL\" (4; #121), 5.419134133163832),\n",
       "  (\"std\" (1; #119), 5.405800799830499),\n",
       "  (\"f85\" (1; #102), 5.397147036389637),\n",
       "  (\"var\" (1; #120), 5.3968760686477015),\n",
       "  (\"f58\" (1; #72), 5.391392197679962),\n",
       "  (\"f64\" (1; #79), 5.391117818740398),\n",
       "  (\"f26\" (1; #37), 5.390854563271353),\n",
       "  (\"f76\" (1; #92), 5.38838144499179),\n",
       "  (\"f91\" (1; #109), 5.385800799830499),\n",
       "  (\"f51\" (1; #65), 5.385585746067057),\n",
       "  (\"f93\" (1; #111), 5.3821448858520045),\n",
       "  (\"f41\" (1; #54), 5.381233057895014),\n",
       "  (\"f66\" (1; #81), 5.378381444991793),\n",
       "  (\"f63\" (1; #78), 5.378322119815668),\n",
       "  (\"f20\" (1; #31), 5.377413703056304),\n",
       "  (\"f105\" (1; #7), 5.372074437205362),\n",
       "  (\"f14\" (1; #24), 5.372037358970281),\n",
       "  (\"f100\" (1; #2), 5.3712846707982385),\n",
       "  (\"f72\" (1; #88), 5.36985056941575),\n",
       "  (\"f89\" (1; #106), 5.3667685417659845),\n",
       "  (\"f115\" (1; #18), 5.36676854176598),\n",
       "  (\"f59\" (1; #73), 5.365964240690713),\n",
       "  (\"f22\" (1; #33), 5.364295423486416),\n",
       "  (\"f104\" (1; #6), 5.361607251443401),\n",
       "  (\"f19\" (1; #29), 5.359994348217594),\n",
       "  (\"f116\" (1; #19), 5.359241660045552),\n",
       "  (\"f56\" (1; #70), 5.3585964987552295),\n",
       "  (\"f55\" (1; #69), 5.358058864346627),\n",
       "  (\"f83\" (1; #100), 5.356186413475287),\n",
       "  (\"f112\" (1; #15), 5.355374993378884),\n",
       "  (\"f94\" (1; #112), 5.354833057895013),\n",
       "  (\"f84\" (1; #101), 5.353742957783781),\n",
       "  (\"f15\" (1; #25), 5.35139219767996),\n",
       "  (\"f68\" (1; #83), 5.350624678213888),\n",
       "  (\"f82\" (1; #99), 5.348166391228348),\n",
       "  (\"f74\" (1; #90), 5.3475212299380255),\n",
       "  (\"f90\" (1; #108), 5.347521229938025),\n",
       "  (\"f10\" (1; #1), 5.346553488002543),\n",
       "  (\"f69\" (1; #84), 5.346123380475658),\n",
       "  (\"f33\" (1; #45), 5.343538232957251),\n",
       "  (\"f73\" (1; #89), 5.342723602945072),\n",
       "  (\"f118\" (1; #21), 5.342682520260604),\n",
       "  (\"f103\" (1; #5), 5.341284670798239),\n",
       "  (\"f44\" (1; #57), 5.341233057895012),\n",
       "  (\"f4\" (1; #52), 5.338627220721434),\n",
       "  (\"f88\" (1; #105), 5.33802814238042),\n",
       "  (\"f97\" (1; #115), 5.335370692303619),\n",
       "  (\"f80\" (1; #97), 5.331392197679964),\n",
       "  (\"f18\" (1; #28), 5.331177143916517),\n",
       "  (\"f101\" (1; #3), 5.330831521796702),\n",
       "  (\"f46\" (1; #59), 5.330639509507918),\n",
       "  (\"f30\" (1; #42), 5.328785746067057),\n",
       "  (\"f60\" (1; #75), 5.327091122411147),\n",
       "  (\"f43\" (1; #56), 5.3270911224111455),\n",
       "  (\"f108\" (1; #10), 5.327039604851951),\n",
       "  (\"f13\" (1; #23), 5.325893495418186),\n",
       "  (\"f110\" (1; #13), 5.322897574024045),\n",
       "  (\"f17\" (1; #27), 5.321269839504208),\n",
       "  (\"f54\" (1; #68), 5.319026606282114),\n",
       "  (\"f49\" (1; #62), 5.312574993378884),\n",
       "  (\"f109\" (1; #11), 5.311607251443402),\n",
       "  (\"f87\" (1; #104), 5.310427220721435),\n",
       "  (\"f111\" (1; #14), 5.310316928862757),\n",
       "  (\"f98\" (1; #116), 5.309990507971818),\n",
       "  (\"f67\" (1; #82), 5.308596498755231),\n",
       "  (\"f39\" (1; #51), 5.308020154669208),\n",
       "  (\"f81\" (1; #98), 5.305048111658454),\n",
       "  (\"f9\" (1; #107), 5.304402950368131),\n",
       "  (\"f53\" (1; #67), 5.303542735314368),\n",
       "  (\"f23\" (1; #34), 5.302560162084854),\n",
       "  (\"f117\" (1; #20), 5.300747036389639),\n",
       "  (\"f75\" (1; #91), 5.295048111658453),\n",
       "  (\"f27\" (1; #38), 5.290624678213884),\n",
       "  (\"f42\" (1; #55), 5.290424455744476),\n",
       "  (\"f38\" (1; #50), 5.289441882514963),\n",
       "  (\"f86\" (1; #103), 5.284554324911274),\n",
       "  (\"f113\" (1; #16), 5.282144885852006),\n",
       "  (\"f12\" (1; #22), 5.28050894115154),\n",
       "  (\"f79\" (1; #95), 5.279950384024576),\n",
       "  (\"f114\" (1; #17), 5.27612338047566),\n",
       "  (\"f62\" (1; #77), 5.2755857460670565),\n",
       "  (\"f36\" (1; #48), 5.270424455744477),\n",
       "  (\"f52\" (1; #66), 5.2690203771386175),\n",
       "  (\"f99\" (1; #117), 5.263981444991787),\n",
       "  (\"f6\" (1; #74), 5.2626997245616804),\n",
       "  (\"f31\" (1; #43), 5.2626825202606105),\n",
       "  (\"f78\" (1; #94), 5.26265947878595),\n",
       "  (\"f29\" (1; #40), 5.261929832088564),\n",
       "  (\"f61\" (1; #76), 5.2556932729487755),\n",
       "  (\"f48\" (1; #61), 5.2550250701838),\n",
       "  (\"f3\" (1; #41), 5.252682520260606),\n",
       "  (\"f37\" (1; #49), 5.241822305206841),\n",
       "  (\"f96\" (1; #114), 5.234402950368133),\n",
       "  (\"f50\" (1; #64), 5.233880676942632),\n",
       "  (\"f7\" (1; #85), 5.2326825202606075),\n",
       "  (\"f71\" (1; #87), 5.2270024312728385),\n",
       "  (\"f106\" (1; #8), 5.225609709200694),\n",
       "  (\"f107\" (1; #9), 5.21461800413157),\n",
       "  (\"f2\" (1; #30), 5.212790047142323),\n",
       "  (\"f102\" (1; #4), 5.21182230520684),\n",
       "  (\"f5\" (1; #63), 5.211069617034798),\n",
       "  (\"f11\" (1; #12), 5.207096774193544),\n",
       "  (\"f92\" (1; #110), 5.200472657450073),\n",
       "  (\"f32\" (1; #44), 5.197665040521214),\n",
       "  (\"f95\" (1; #113), 5.196483039355896),\n",
       "  (\"f34\" (1; #46), 5.196353795222203),\n",
       "  (\"f77\" (1; #93), 5.195969770644628),\n",
       "  (\"f57\" (1; #71), 5.195762927061813),\n",
       "  (\"f24\" (1; #35), 5.194080369722971),\n",
       "  (\"f16\" (1; #26), 5.1936502621960905),\n",
       "  (\"f28\" (1; #39), 5.189134133163828),\n",
       "  (\"f25\" (1; #36), 5.16267510461359),\n",
       "  (\"f8\" (1; #96), 5.155116928862757),\n",
       "  (\"f21\" (1; #32), 5.150120710842733),\n",
       "  (\"f1\" (1; #0), 5.143343042534027),\n",
       "  (\"f65\" (1; #80), 5.128396805974888),\n",
       "  (\"f47\" (1; #60), 5.117736283701466),\n",
       "  (\"f35\" (1; #47), 5.114402950368131),\n",
       "  (\"f45\" (1; #58), 5.1016158535939375),\n",
       "  (\"f70\" (1; #86), 5.043220154669209),\n",
       "  (\"f40\" (1; #53), 5.019671767572432),\n",
       "  (\"nan\" (1; #118), 4.3915405106202705)],\n",
       " 'NUM_NODES': [(\"f40\" (1; #53), 225.0),\n",
       "  (\"f70\" (1; #86), 158.0),\n",
       "  (\"nan\" (1; #118), 152.0),\n",
       "  (\"f35\" (1; #47), 150.0),\n",
       "  (\"f47\" (1; #60), 147.0),\n",
       "  (\"f45\" (1; #58), 137.0),\n",
       "  (\"f1\" (1; #0), 134.0),\n",
       "  (\"f5\" (1; #63), 122.0),\n",
       "  (\"f21\" (1; #32), 121.0),\n",
       "  (\"f34\" (1; #46), 120.0),\n",
       "  (\"f8\" (1; #96), 117.0),\n",
       "  (\"f3\" (1; #41), 116.0),\n",
       "  (\"f57\" (1; #71), 115.0),\n",
       "  (\"f95\" (1; #113), 115.0),\n",
       "  (\"f92\" (1; #110), 113.0),\n",
       "  (\"f71\" (1; #87), 111.0),\n",
       "  (\"f106\" (1; #8), 110.0),\n",
       "  (\"f96\" (1; #114), 104.0),\n",
       "  (\"f77\" (1; #93), 103.0),\n",
       "  (\"f107\" (1; #9), 102.0),\n",
       "  (\"f65\" (1; #80), 99.0),\n",
       "  (\"f48\" (1; #61), 96.0),\n",
       "  (\"f7\" (1; #85), 96.0),\n",
       "  (\"f31\" (1; #43), 95.0),\n",
       "  (\"f32\" (1; #44), 95.0),\n",
       "  (\"f52\" (1; #66), 94.0),\n",
       "  (\"f62\" (1; #77), 93.0),\n",
       "  (\"f86\" (1; #103), 93.0),\n",
       "  (\"f79\" (1; #95), 92.0),\n",
       "  (\"f2\" (1; #30), 91.0),\n",
       "  (\"f50\" (1; #64), 91.0),\n",
       "  (\"f16\" (1; #26), 88.0),\n",
       "  (\"f25\" (1; #36), 88.0),\n",
       "  (\"f102\" (1; #4), 86.0),\n",
       "  (\"f28\" (1; #39), 86.0),\n",
       "  (\"f61\" (1; #76), 85.0),\n",
       "  (\"f27\" (1; #38), 83.0),\n",
       "  (\"f99\" (1; #117), 83.0),\n",
       "  (\"f9\" (1; #107), 82.0),\n",
       "  (\"f53\" (1; #67), 81.0),\n",
       "  (\"f24\" (1; #35), 80.0),\n",
       "  (\"f38\" (1; #50), 79.0),\n",
       "  (\"f6\" (1; #74), 78.0),\n",
       "  (\"f75\" (1; #91), 78.0),\n",
       "  (\"f36\" (1; #48), 77.0),\n",
       "  (\"f15\" (1; #25), 76.0),\n",
       "  (\"f12\" (1; #22), 75.0),\n",
       "  (\"f78\" (1; #94), 75.0),\n",
       "  (\"f114\" (1; #17), 74.0),\n",
       "  (\"f13\" (1; #23), 74.0),\n",
       "  (\"f33\" (1; #45), 73.0),\n",
       "  (\"f37\" (1; #49), 73.0),\n",
       "  (\"f42\" (1; #55), 73.0),\n",
       "  (\"f29\" (1; #40), 71.0),\n",
       "  (\"f113\" (1; #16), 70.0),\n",
       "  (\"f17\" (1; #27), 70.0),\n",
       "  (\"f81\" (1; #98), 69.0),\n",
       "  (\"f108\" (1; #10), 67.0),\n",
       "  (\"f11\" (1; #12), 66.0),\n",
       "  (\"f46\" (1; #59), 66.0),\n",
       "  (\"f74\" (1; #90), 66.0),\n",
       "  (\"f10\" (1; #1), 65.0),\n",
       "  (\"f110\" (1; #13), 65.0),\n",
       "  (\"f111\" (1; #14), 65.0),\n",
       "  (\"f18\" (1; #28), 65.0),\n",
       "  (\"f87\" (1; #104), 65.0),\n",
       "  (\"f80\" (1; #97), 64.0),\n",
       "  (\"f98\" (1; #116), 64.0),\n",
       "  (\"f23\" (1; #34), 63.0),\n",
       "  (\"f30\" (1; #42), 63.0),\n",
       "  (\"f39\" (1; #51), 63.0),\n",
       "  (\"f54\" (1; #68), 63.0),\n",
       "  (\"f56\" (1; #70), 63.0),\n",
       "  (\"f97\" (1; #115), 63.0),\n",
       "  (\"f109\" (1; #11), 62.0),\n",
       "  (\"f112\" (1; #15), 61.0),\n",
       "  (\"f55\" (1; #69), 61.0),\n",
       "  (\"f90\" (1; #108), 61.0),\n",
       "  (\"f44\" (1; #57), 60.0),\n",
       "  (\"f49\" (1; #62), 60.0),\n",
       "  (\"f101\" (1; #3), 59.0),\n",
       "  (\"f103\" (1; #5), 58.0),\n",
       "  (\"f117\" (1; #20), 58.0),\n",
       "  (\"f104\" (1; #6), 57.0),\n",
       "  (\"f82\" (1; #99), 57.0),\n",
       "  (\"f100\" (1; #2), 56.0),\n",
       "  (\"f22\" (1; #33), 55.0),\n",
       "  (\"f68\" (1; #83), 55.0),\n",
       "  (\"f73\" (1; #89), 55.0),\n",
       "  (\"f93\" (1; #111), 55.0),\n",
       "  (\"f4\" (1; #52), 54.0),\n",
       "  (\"f60\" (1; #75), 54.0),\n",
       "  (\"f69\" (1; #84), 54.0),\n",
       "  (\"f14\" (1; #24), 53.0),\n",
       "  (\"f84\" (1; #101), 53.0),\n",
       "  (\"f43\" (1; #56), 51.0),\n",
       "  (\"f19\" (1; #29), 50.0),\n",
       "  (\"f63\" (1; #78), 50.0),\n",
       "  (\"f91\" (1; #109), 49.0),\n",
       "  (\"f72\" (1; #88), 48.0),\n",
       "  (\"f89\" (1; #106), 48.0),\n",
       "  (\"f118\" (1; #21), 47.0),\n",
       "  (\"f51\" (1; #65), 47.0),\n",
       "  (\"f59\" (1; #73), 47.0),\n",
       "  (\"f67\" (1; #82), 47.0),\n",
       "  (\"f64\" (1; #79), 46.0),\n",
       "  (\"f105\" (1; #7), 43.0),\n",
       "  (\"f66\" (1; #81), 42.0),\n",
       "  (\"f83\" (1; #100), 42.0),\n",
       "  (\"f115\" (1; #18), 41.0),\n",
       "  (\"f116\" (1; #19), 41.0),\n",
       "  (\"f20\" (1; #31), 41.0),\n",
       "  (\"f94\" (1; #112), 41.0),\n",
       "  (\"f41\" (1; #54), 40.0),\n",
       "  (\"f26\" (1; #37), 39.0),\n",
       "  (\"f88\" (1; #105), 36.0),\n",
       "  (\"f76\" (1; #92), 35.0),\n",
       "  (\"f58\" (1; #72), 30.0),\n",
       "  (\"f85\" (1; #102), 24.0),\n",
       "  (\"std\" (1; #119), 21.0),\n",
       "  (\"var\" (1; #120), 15.0)],\n",
       " 'SUM_SCORE': [(\"nan\" (1; #118), 339729.13145452464),\n",
       "  (\"f40\" (1; #53), 1275.032112349126),\n",
       "  (\"f70\" (1; #86), 828.3261780415726),\n",
       "  (\"f35\" (1; #47), 668.5154739441278),\n",
       "  (\"f47\" (1; #60), 656.6452808393695),\n",
       "  (\"f45\" (1; #58), 587.4778100494757),\n",
       "  (\"f21\" (1; #32), 534.8928542178398),\n",
       "  (\"f34\" (1; #46), 497.1268267241553),\n",
       "  (\"f1\" (1; #0), 488.2473218812029),\n",
       "  (\"f3\" (1; #41), 481.6246690839864),\n",
       "  (\"f95\" (1; #113), 450.7995282470639),\n",
       "  (\"f8\" (1; #96), 435.3754499806803),\n",
       "  (\"f96\" (1; #114), 434.9427626234384),\n",
       "  (\"f65\" (1; #80), 422.5348065606072),\n",
       "  (\"f77\" (1; #93), 412.08268765095136),\n",
       "  (\"f57\" (1; #71), 406.46323369863796),\n",
       "  (\"f5\" (1; #63), 399.81108703863526),\n",
       "  (\"f107\" (1; #9), 382.66309157336843),\n",
       "  (\"f106\" (1; #8), 373.6048126809402),\n",
       "  (\"f31\" (1; #43), 356.0165559034035),\n",
       "  (\"f16\" (1; #26), 354.5065889500713),\n",
       "  (\"f71\" (1; #87), 348.99483271025315),\n",
       "  (\"f92\" (1; #110), 348.23984343418056),\n",
       "  (\"f32\" (1; #44), 347.8604857996636),\n",
       "  (\"f7\" (1; #85), 323.4048099725235),\n",
       "  (\"f62\" (1; #77), 309.8142414428962),\n",
       "  (\"f52\" (1; #66), 305.30195790688083),\n",
       "  (\"f28\" (1; #39), 287.73779523160874),\n",
       "  (\"f48\" (1; #61), 286.5286224788018),\n",
       "  (\"f86\" (1; #103), 280.1016893685105),\n",
       "  (\"f79\" (1; #95), 279.2677523734733),\n",
       "  (\"f9\" (1; #107), 275.82561637207573),\n",
       "  (\"f2\" (1; #30), 275.0954577719076),\n",
       "  (\"f38\" (1; #50), 274.3399694145488),\n",
       "  (\"f50\" (1; #64), 273.0465527207675),\n",
       "  (\"f36\" (1; #48), 272.3374901518432),\n",
       "  (\"f75\" (1; #91), 270.03094686908526),\n",
       "  (\"f25\" (1; #36), 269.6949304835747),\n",
       "  (\"f78\" (1; #94), 260.5628016191472),\n",
       "  (\"f102\" (1; #4), 259.6961000483401),\n",
       "  (\"f61\" (1; #76), 257.0020016312984),\n",
       "  (\"f99\" (1; #117), 248.1103964021055),\n",
       "  (\"f27\" (1; #38), 244.79495816658732),\n",
       "  (\"f53\" (1; #67), 244.4306445095508),\n",
       "  (\"f6\" (1; #74), 244.1952870412473),\n",
       "  (\"f24\" (1; #35), 234.05009469176787),\n",
       "  (\"f12\" (1; #22), 225.1635241551411),\n",
       "  (\"f15\" (1; #25), 220.93108239001572),\n",
       "  (\"f114\" (1; #17), 217.6326708765281),\n",
       "  (\"f42\" (1; #55), 216.26194401922987),\n",
       "  (\"f37\" (1; #49), 215.66529926571718),\n",
       "  (\"f13\" (1; #23), 204.20027612382626),\n",
       "  (\"f97\" (1; #115), 195.25214340459024),\n",
       "  (\"f113\" (1; #16), 193.39986612851908),\n",
       "  (\"f103\" (1; #5), 191.40615497773524),\n",
       "  (\"f46\" (1; #59), 190.10502061267607),\n",
       "  (\"f109\" (1; #11), 189.64937291959018),\n",
       "  (\"f108\" (1; #10), 189.45812295017367),\n",
       "  (\"f81\" (1; #98), 188.8779424349375),\n",
       "  (\"f111\" (1; #14), 187.87164133147735),\n",
       "  (\"f10\" (1; #1), 186.42621867465687),\n",
       "  (\"f11\" (1; #12), 184.9357397626951),\n",
       "  (\"f18\" (1; #28), 183.9832963232925),\n",
       "  (\"f110\" (1; #13), 182.48344515581653),\n",
       "  (\"f17\" (1; #27), 181.83248836153075),\n",
       "  (\"f39\" (1; #51), 181.66620876036131),\n",
       "  (\"f44\" (1; #57), 180.86614554804828),\n",
       "  (\"f30\" (1; #42), 180.04164363253858),\n",
       "  (\"f98\" (1; #116), 178.68309162826824),\n",
       "  (\"f33\" (1; #45), 177.34742099104278),\n",
       "  (\"f74\" (1; #90), 176.79415306027795),\n",
       "  (\"f80\" (1; #97), 176.70346285851247),\n",
       "  (\"f23\" (1; #34), 174.03624417740275),\n",
       "  (\"f104\" (1; #6), 173.8689131547644),\n",
       "  (\"f56\" (1; #70), 171.80777325536747),\n",
       "  (\"f90\" (1; #108), 170.85474982174492),\n",
       "  (\"f29\" (1; #40), 168.3681933433661),\n",
       "  (\"f112\" (1; #15), 167.59187736951208),\n",
       "  (\"f54\" (1; #68), 167.3692577683796),\n",
       "  (\"f87\" (1; #104), 165.33954042150924),\n",
       "  (\"f60\" (1; #75), 163.3020770818033),\n",
       "  (\"f101\" (1; #3), 162.2422636274555),\n",
       "  (\"f49\" (1; #62), 160.24844612360107),\n",
       "  (\"f93\" (1; #111), 155.83133538343463),\n",
       "  (\"f55\" (1; #69), 155.72303388324372),\n",
       "  (\"f4\" (1; #52), 154.00458423015198),\n",
       "  (\"f117\" (1; #20), 154.0017582233886),\n",
       "  (\"f14\" (1; #24), 152.27386801442526),\n",
       "  (\"f100\" (1; #2), 148.4729611669227),\n",
       "  (\"f82\" (1; #99), 146.46490261682288),\n",
       "  (\"f22\" (1; #33), 146.06631079645274),\n",
       "  (\"f68\" (1; #83), 145.75228382166733),\n",
       "  (\"f69\" (1; #84), 144.84995966887527),\n",
       "  (\"f73\" (1; #89), 143.47809240406536),\n",
       "  (\"f84\" (1; #101), 138.06427277345233),\n",
       "  (\"f91\" (1; #109), 134.56995525921957),\n",
       "  (\"f89\" (1; #106), 132.70358424643746),\n",
       "  (\"f118\" (1; #21), 130.61102596884552),\n",
       "  (\"f63\" (1; #78), 128.39527931126668),\n",
       "  (\"f19\" (1; #29), 126.77013910326832),\n",
       "  (\"f72\" (1; #88), 124.02060987357527),\n",
       "  (\"f43\" (1; #56), 123.53575558297962),\n",
       "  (\"f67\" (1; #82), 118.6571655800891),\n",
       "  (\"f51\" (1; #65), 118.57149555041906),\n",
       "  (\"f59\" (1; #73), 114.94556974677073),\n",
       "  (\"f64\" (1; #79), 114.37070247145448),\n",
       "  (\"f116\" (1; #19), 111.72494113933783),\n",
       "  (\"f105\" (1; #7), 110.7373426907252),\n",
       "  (\"f83\" (1; #100), 106.83645837865583),\n",
       "  (\"f66\" (1; #81), 106.77074638412887),\n",
       "  (\"f20\" (1; #31), 105.00292465054008),\n",
       "  (\"f115\" (1; #18), 102.43613924730118),\n",
       "  (\"f41\" (1; #54), 100.24398357166979),\n",
       "  (\"f94\" (1; #112), 96.1741356185048),\n",
       "  (\"f76\" (1; #92), 93.66348247224596),\n",
       "  (\"f26\" (1; #37), 92.51645893607565),\n",
       "  (\"f88\" (1; #105), 91.34228968794082),\n",
       "  (\"f58\" (1; #72), 75.74255268938373),\n",
       "  (\"f85\" (1; #102), 59.416610032138124),\n",
       "  (\"std\" (1; #119), 48.47747122794408),\n",
       "  (\"var\" (1; #120), 37.71112454083004)]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "inspector.variable_importances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeSzUuF2gS5H"
   },
   "source": [
    "Let's plot a tiny part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "anv5ZXNum4fJ",
    "outputId": "ae41a050-2c1b-46f9-cb5c-f97b7b122765"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
       "<div id=\"tree_plot_bc9005c647dd4dfeb4702a6097716623\"></div>\n",
       "<script>\n",
       "/*\n",
       " * Copyright 2021 Google LLC.\n",
       " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       " * you may not use this file except in compliance with the License.\n",
       " * You may obtain a copy of the License at\n",
       " *\n",
       " *     https://www.apache.org/licenses/LICENSE-2.0\n",
       " *\n",
       " * Unless required by applicable law or agreed to in writing, software\n",
       " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       " * See the License for the specific language governing permissions and\n",
       " * limitations under the License.\n",
       " */\n",
       "\n",
       "/**\n",
       " *  Plotting of decision trees generated by TF-DF.\n",
       " *\n",
       " *  A tree is a recursive structure of node objects.\n",
       " *  A node contains one or more of the following components:\n",
       " *\n",
       " *    - A value: Representing the output of the node. If the node is not a leaf,\n",
       " *      the value is only present for analysis i.e. it is not used for\n",
       " *      predictions.\n",
       " *\n",
       " *    - A condition : For non-leaf nodes, the condition (also known as split)\n",
       " *      defines a binary test to branch to the positive or negative child.\n",
       " *\n",
       " *    - An explanation: Generally a plot showing the relation between the label\n",
       " *      and the condition to give insights about the effect of the condition.\n",
       " *\n",
       " *    - Two children : For non-leaf nodes, the children nodes. The first\n",
       " *      children (i.e. \"node.children[0]\") is the negative children (drawn in\n",
       " *      red). The second children is the positive one (drawn in green).\n",
       " *\n",
       " */\n",
       "\n",
       "/**\n",
       " * Plots a single decision tree into a DOM element.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!tree} raw_tree Recursive tree structure.\n",
       " * @param {string} canvas_id Id of the output dom element.\n",
       " */\n",
       "function display_tree(options, raw_tree, canvas_id) {\n",
       "  console.log(options);\n",
       "\n",
       "  // Determine the node placement.\n",
       "  const tree_struct = d3.tree().nodeSize(\n",
       "      [options.node_y_offset, options.node_x_offset])(d3.hierarchy(raw_tree));\n",
       "\n",
       "  // Boundaries of the node placement.\n",
       "  let x_min = Infinity;\n",
       "  let x_max = -x_min;\n",
       "  let y_min = Infinity;\n",
       "  let y_max = -x_min;\n",
       "\n",
       "  tree_struct.each(d => {\n",
       "    if (d.x > x_max) x_max = d.x;\n",
       "    if (d.x < x_min) x_min = d.x;\n",
       "    if (d.y > y_max) y_max = d.y;\n",
       "    if (d.y < y_min) y_min = d.y;\n",
       "  });\n",
       "\n",
       "  // Size of the plot.\n",
       "  const width = y_max - y_min + options.node_x_size + options.margin * 2;\n",
       "  const height = x_max - x_min + options.node_y_size + options.margin * 2 +\n",
       "      options.node_y_offset - options.node_y_size;\n",
       "\n",
       "  const plot = d3.select(canvas_id);\n",
       "\n",
       "  // Tool tip\n",
       "  options.tooltip = plot.append('div')\n",
       "                        .attr('width', 100)\n",
       "                        .attr('height', 100)\n",
       "                        .style('padding', '4px')\n",
       "                        .style('background', '#fff')\n",
       "                        .style('box-shadow', '4px 4px 0px rgba(0,0,0,0.1)')\n",
       "                        .style('border', '1px solid black')\n",
       "                        .style('font-family', 'sans-serif')\n",
       "                        .style('font-size', options.font_size)\n",
       "                        .style('position', 'absolute')\n",
       "                        .style('z-index', '10')\n",
       "                        .attr('pointer-events', 'none')\n",
       "                        .style('display', 'none');\n",
       "\n",
       "  // Create canvas\n",
       "  const svg = plot.append('svg').attr('width', width).attr('height', height);\n",
       "  const graph =\n",
       "      svg.style('overflow', 'visible')\n",
       "          .append('g')\n",
       "          .attr('font-family', 'sans-serif')\n",
       "          .attr('font-size', options.font_size)\n",
       "          .attr(\n",
       "              'transform',\n",
       "              () => `translate(${options.margin},${\n",
       "                  - x_min + options.node_y_offset / 2 + options.margin})`);\n",
       "\n",
       "  // Plot bounding box.\n",
       "  if (options.show_plot_bounding_box) {\n",
       "    svg.append('rect')\n",
       "        .attr('width', width)\n",
       "        .attr('height', height)\n",
       "        .attr('fill', 'none')\n",
       "        .attr('stroke-width', 1.0)\n",
       "        .attr('stroke', 'black');\n",
       "  }\n",
       "\n",
       "  // Draw the edges.\n",
       "  display_edges(options, graph, tree_struct);\n",
       "\n",
       "  // Draw the nodes.\n",
       "  display_nodes(options, graph, tree_struct);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Draw the nodes of the tree.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!graph} graph D3 search handle containing the graph.\n",
       " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
       " *     data, etc.).\n",
       " */\n",
       "function display_nodes(options, graph, tree_struct) {\n",
       "  const nodes = graph.append('g')\n",
       "                    .selectAll('g')\n",
       "                    .data(tree_struct.descendants())\n",
       "                    .join('g')\n",
       "                    .attr('transform', d => `translate(${d.y},${d.x})`);\n",
       "\n",
       "  nodes.append('rect')\n",
       "      .attr('x', 0.5)\n",
       "      .attr('y', 0.5)\n",
       "      .attr('width', options.node_x_size)\n",
       "      .attr('height', options.node_y_size)\n",
       "      .attr('stroke', 'lightgrey')\n",
       "      .attr('stroke-width', 1)\n",
       "      .attr('fill', 'white')\n",
       "      .attr('y', -options.node_y_size / 2);\n",
       "\n",
       "  // Brackets on the right of condition nodes without children.\n",
       "  non_leaf_node_without_children =\n",
       "      nodes.filter(node => node.data.condition != null && node.children == null)\n",
       "          .append('g')\n",
       "          .attr('transform', `translate(${options.node_x_size},0)`);\n",
       "\n",
       "  non_leaf_node_without_children.append('path')\n",
       "      .attr('d', 'M0,0 C 10,0 0,10 10,10')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.0)\n",
       "      .attr('stroke', '#F00');\n",
       "\n",
       "  non_leaf_node_without_children.append('path')\n",
       "      .attr('d', 'M0,0 C 10,0 0,-10 10,-10')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.0)\n",
       "      .attr('stroke', '#0F0');\n",
       "\n",
       "  const node_content = nodes.append('g').attr(\n",
       "      'transform',\n",
       "      `translate(0,${options.node_padding - options.node_y_size / 2})`);\n",
       "\n",
       "  node_content.append(node => create_node_element(options, node));\n",
       "}\n",
       "\n",
       "/**\n",
       " * Creates the D3 content for a single node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!node} node Node to draw.\n",
       " * @return {!d3} D3 content.\n",
       " */\n",
       "function create_node_element(options, node) {\n",
       "  // Output accumulator.\n",
       "  let output = {\n",
       "    // Content to draw.\n",
       "    content: d3.create('svg:g'),\n",
       "    // Vertical offset to the next element to draw.\n",
       "    vertical_offset: 0\n",
       "  };\n",
       "\n",
       "  // Conditions.\n",
       "  if (node.data.condition != null) {\n",
       "    display_condition(options, node.data.condition, output);\n",
       "  }\n",
       "\n",
       "  // Values.\n",
       "  if (node.data.value != null) {\n",
       "    display_value(options, node.data.value, output);\n",
       "  }\n",
       "\n",
       "  // Explanations.\n",
       "  if (node.data.explanation != null) {\n",
       "    display_explanation(options, node.data.explanation, output);\n",
       "  }\n",
       "\n",
       "  return output.content.node();\n",
       "}\n",
       "\n",
       "\n",
       "/**\n",
       " * Adds a single line of text inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {string} text Text to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_node_text(options, text, output) {\n",
       "  output.content.append('text')\n",
       "      .attr('x', options.node_padding)\n",
       "      .attr('y', output.vertical_offset)\n",
       "      .attr('alignment-baseline', 'hanging')\n",
       "      .text(text);\n",
       "  output.vertical_offset += 10;\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a single line of text inside of a node with a tooltip.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {string} text Text to display.\n",
       " * @param {string} tooltip Text in the Tooltip.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_node_text_with_tooltip(options, text, tooltip, output) {\n",
       "  const item = output.content.append('text')\n",
       "                   .attr('x', options.node_padding)\n",
       "                   .attr('alignment-baseline', 'hanging')\n",
       "                   .text(text);\n",
       "\n",
       "  add_tooltip(options, item, () => tooltip);\n",
       "  output.vertical_offset += 10;\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a tooltip to a dom element.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!dom} target Dom element to equip with a tooltip.\n",
       " * @param {!func} get_content Generates the html content of the tooltip.\n",
       " */\n",
       "function add_tooltip(options, target, get_content) {\n",
       "  function show(d) {\n",
       "    options.tooltip.style('display', 'block');\n",
       "    options.tooltip.html(get_content());\n",
       "  }\n",
       "\n",
       "  function hide(d) {\n",
       "    options.tooltip.style('display', 'none');\n",
       "  }\n",
       "\n",
       "  function move(d) {\n",
       "    options.tooltip.style('display', 'block');\n",
       "    options.tooltip.style('left', (d.pageX + 5) + 'px');\n",
       "    options.tooltip.style('top', d.pageY + 'px');\n",
       "  }\n",
       "\n",
       "  target.on('mouseover', show);\n",
       "  target.on('mouseout', hide);\n",
       "  target.on('mousemove', move);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a condition inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!condition} condition Condition to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_condition(options, condition, output) {\n",
       "  threshold_format = d3.format('r');\n",
       "\n",
       "  if (condition.type === 'IS_MISSING') {\n",
       "    display_node_text(options, `${condition.attribute} is missing`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'IS_TRUE') {\n",
       "    display_node_text(options, `${condition.attribute} is true`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'NUMERICAL_IS_HIGHER_THAN') {\n",
       "    format = d3.format('r');\n",
       "    display_node_text(\n",
       "        options,\n",
       "        `${condition.attribute} >= ${threshold_format(condition.threshold)}`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'CATEGORICAL_IS_IN') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `${condition.attribute} in [...]`,\n",
       "        `${condition.attribute} in [${condition.mask}]`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'CATEGORICAL_SET_CONTAINS') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `${condition.attribute} intersect [...]`,\n",
       "        `${condition.attribute} intersect [${condition.mask}]`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'NUMERICAL_SPARSE_OBLIQUE') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `Sparse oblique split...`,\n",
       "        `[${condition.attributes}]*[${condition.weights}]>=${\n",
       "            threshold_format(condition.threshold)}`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  display_node_text(\n",
       "      options, `Non supported condition ${condition.type}`, output);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a value inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!value} value Value to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_value(options, value, output) {\n",
       "  if (value.type === 'PROBABILITY') {\n",
       "    const left_margin = 0;\n",
       "    const right_margin = 50;\n",
       "    const plot_width = options.node_x_size - options.node_padding * 2 -\n",
       "        left_margin - right_margin;\n",
       "\n",
       "    let cusum = Array.from(d3.cumsum(value.distribution));\n",
       "    cusum.unshift(0);\n",
       "    const distribution_plot = output.content.append('g').attr(\n",
       "        'transform', `translate(0,${output.vertical_offset + 0.5})`);\n",
       "\n",
       "    distribution_plot.selectAll('rect')\n",
       "        .data(value.distribution)\n",
       "        .join('rect')\n",
       "        .attr('height', 10)\n",
       "        .attr(\n",
       "            'x',\n",
       "            (d, i) =>\n",
       "                (cusum[i] * plot_width + left_margin + options.node_padding))\n",
       "        .attr('width', (d, i) => d * plot_width)\n",
       "        .style('fill', (d, i) => d3.schemeSet1[i]);\n",
       "\n",
       "    const num_examples =\n",
       "        output.content.append('g')\n",
       "            .attr('transform', `translate(0,${output.vertical_offset})`)\n",
       "            .append('text')\n",
       "            .attr('x', options.node_x_size - options.node_padding)\n",
       "            .attr('alignment-baseline', 'hanging')\n",
       "            .attr('text-anchor', 'end')\n",
       "            .text(`(${value.num_examples})`);\n",
       "\n",
       "    const distribution_details = d3.create('ul');\n",
       "    distribution_details.selectAll('li')\n",
       "        .data(value.distribution)\n",
       "        .join('li')\n",
       "        .append('span')\n",
       "        .text(\n",
       "            (d, i) =>\n",
       "                'class ' + i + ': ' + d3.format('.3%')(value.distribution[i]));\n",
       "\n",
       "    add_tooltip(options, distribution_plot, () => distribution_details.html());\n",
       "    add_tooltip(options, num_examples, () => 'Number of examples');\n",
       "\n",
       "    output.vertical_offset += 10;\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (value.type === 'REGRESSION') {\n",
       "    display_node_text(\n",
       "        options,\n",
       "        'value: ' + d3.format('r')(value.value) + ` (` +\n",
       "            d3.format('.6')(value.num_examples) + `)`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  display_node_text(options, `Non supported value ${value.type}`, output);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds an explanation inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!explanation} explanation Explanation to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_explanation(options, explanation, output) {\n",
       "  // Margin before the explanation.\n",
       "  output.vertical_offset += 10;\n",
       "\n",
       "  display_node_text(\n",
       "      options, `Non supported explanation ${explanation.type}`, output);\n",
       "}\n",
       "\n",
       "\n",
       "/**\n",
       " * Draw the edges of the tree.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!graph} graph D3 search handle containing the graph.\n",
       " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
       " *     data, etc.).\n",
       " */\n",
       "function display_edges(options, graph, tree_struct) {\n",
       "  // Draw an edge between a parent and a child node with a bezier.\n",
       "  function draw_single_edge(d) {\n",
       "    return 'M' + (d.source.y + options.node_x_size) + ',' + d.source.x + ' C' +\n",
       "        (d.source.y + options.node_x_size + options.edge_rounding) + ',' +\n",
       "        d.source.x + ' ' + (d.target.y - options.edge_rounding) + ',' +\n",
       "        d.target.x + ' ' + d.target.y + ',' + d.target.x;\n",
       "  }\n",
       "\n",
       "  graph.append('g')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.2)\n",
       "      .selectAll('path')\n",
       "      .data(tree_struct.links())\n",
       "      .join('path')\n",
       "      .attr('d', draw_single_edge)\n",
       "      .attr(\n",
       "          'stroke', d => (d.target === d.source.children[0]) ? '#0F0' : '#F00');\n",
       "}\n",
       "\n",
       "display_tree({\"margin\": 10, \"node_x_size\": 160, \"node_y_size\": 28, \"node_x_offset\": 180, \"node_y_offset\": 33, \"font_size\": 10, \"edge_rounding\": 20, \"node_padding\": 2, \"show_plot_bounding_box\": false}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.0, \"num_examples\": 775945.0, \"standard_deviation\": 0.4999977932125557}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"nan\", \"threshold\": 0.5}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.08720653504133224, \"num_examples\": 484938.0, \"standard_deviation\": 0.4506821758538915}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"nan\", \"threshold\": 1.5}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.10288945585489273, \"num_examples\": 375335.0, \"standard_deviation\": 0.4296486572402119}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f35\", \"threshold\": -52571498610688.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.10384068638086319, \"num_examples\": 346981.0, \"standard_deviation\": 0.4282241275178949}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f96\", \"threshold\": -30.300498962402344}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.09123750776052475, \"num_examples\": 28354.0, \"standard_deviation\": 0.44568679202299544}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f70\", \"threshold\": -0.0011663000332191586}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.03349752351641655, \"num_examples\": 109603.0, \"standard_deviation\": 0.493186024992539}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f45\", \"threshold\": 0.0015141000039875507}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.030990252271294594, \"num_examples\": 85080.0, \"standard_deviation\": 0.4941903906536537}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f3\", \"threshold\": 197.63999938964844}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.042183201760053635, \"num_examples\": 24523.0, \"standard_deviation\": 0.4890634756691486}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f21\", \"threshold\": -1594.6500244140625}}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14532214403152466, \"num_examples\": 291007.0, \"standard_deviation\": 0.341947530500985}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f34\", \"threshold\": 0.0005851149908266962}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14844898879528046, \"num_examples\": 109288.0, \"standard_deviation\": 0.333407310763581}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f3\", \"threshold\": 702.0849609375}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14642682671546936, \"num_examples\": 68371.0, \"standard_deviation\": 0.338967626409793}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f52\", \"threshold\": 56.784000396728516}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.15182016789913177, \"num_examples\": 40917.0, \"standard_deviation\": 0.3237274468273009}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.1434398740530014, \"num_examples\": 181719.0, \"standard_deviation\": 0.3468975993252609}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f45\", \"threshold\": 6.890049553476274e-05}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14443330466747284, \"num_examples\": 157397.0, \"standard_deviation\": 0.34430026609269176}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"f40\", \"threshold\": 0.00044745998457074165}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.1369977444410324, \"num_examples\": 24322.0, \"standard_deviation\": 0.3628462988398754}}]}]}]}, \"#tree_plot_bc9005c647dd4dfeb4702a6097716623\")\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(model_1, tree_idx=0, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjmqazjkgJUy"
   },
   "source": [
    "### Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "OWFubX2fgNDL",
    "outputId": "77c50056-9bfc-47ff-d0f7-5ade3a4024ff"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs = inspector.training_logs()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Accuracy (out-of-bag)')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Logloss (out-of-bag)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm47pZN8hIQT"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's evaluate the model using the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnZov8y9hF_E",
    "outputId": "a3cab866-00b2-4322-de4c-c100a62ee65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 12s 109ms/step - loss: 0.0000e+00 - auc: 0.8129\n",
      "loss: 0.0\n",
      "auc: 0.8128870725631714\n"
     ]
    }
   ],
   "source": [
    "evaluation = model_1.evaluate(valid_ds, return_dict = True)\n",
    "for name, value in evaluation.items():\n",
    "    print(\"{}: {}\".format(name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WhDXBs4hXih"
   },
   "source": [
    "Let's now look at our second approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od4HmBXCh_UD"
   },
   "source": [
    "## Second Approach: TensorFlow based preprocessing\n",
    "\n",
    "In this second approach, we will not add the 3 columns that we added using pandas but rather through TensorFlow / Keras preprocessing that we will pass to the model via the preprocessing parameter.\n",
    "\n",
    "You will notice that this approach is more difficult, more complicated, however it has the advantage of having the preprocessing within the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ShxOoqJLikgF"
   },
   "outputs": [],
   "source": [
    "train_full_data = pd.read_csv(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "HD4VBMzwiwPw"
   },
   "outputs": [],
   "source": [
    "train_full_data = train_full_data.drop('id', axis=1)\n",
    "features = [f'f{i}' for i in range(1, 119)]\n",
    "label = 'claim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0hZNBOfh_UK"
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtbbRjx9h_UK",
    "outputId": "6ecc262a-375e-40bd-b5ce-c5a87113ecc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862073 samples in training and 95846 in validation\n"
     ]
    }
   ],
   "source": [
    "train_ds_pd, valid_ds_pd = split_dataset(train_full_data, test_ratio=VALID_RATIO)\n",
    "print(\"{} samples in training and {} in validation\".format(train_ds_pd.shape[0], valid_ds_pd.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "URosoQkah_UL"
   },
   "outputs": [],
   "source": [
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
    "valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV3-CWsTi5GV"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "D79PxwlSi9J2"
   },
   "outputs": [],
   "source": [
    "def reduce_mean_without_nan(input_tensor):\n",
    "    return tf.experimental.numpy.nanmean(input_tensor, axis=1, keepdims=True)\n",
    "\n",
    "def get_nan_std_var(input_tensor):\n",
    "    # nan tensor\n",
    "    is_nan = tf.math.is_nan(input_tensor)\n",
    "    \n",
    "    # Get the number of nans available in each sample\n",
    "    nan_number_per_sample = tf.cast(\n",
    "        tf.math.reduce_sum(\n",
    "            tf.where(is_nan, [1], [0]),\n",
    "            axis=1,\n",
    "            keepdims=True\n",
    "        ),\n",
    "        tf.float32\n",
    "    )\n",
    "    \n",
    "    # Calculate mean excluding nan\n",
    "    mean_excluding_nan = keras.layers.Lambda(reduce_mean_without_nan)(input_tensor)\n",
    "    \n",
    "    # input tensor replacing nan with the mean for each row (i.e. for each sample)\n",
    "    input_tensor_with_nan_replaced_by_mean = tf.where(\n",
    "        is_nan,\n",
    "        mean_excluding_nan,\n",
    "        input_tensor\n",
    "    )\n",
    "    \n",
    "    squared_distance_from_mean = tf.math.reduce_sum(\n",
    "        tf.math.square(\n",
    "            input_tensor_with_nan_replaced_by_mean - mean_excluding_nan,\n",
    "        ),\n",
    "        axis=1,\n",
    "        keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Calculate std\n",
    "    std = tf.math.sqrt(\n",
    "        tf.math.divide(\n",
    "            squared_distance_from_mean,\n",
    "            input_tensor.shape[1] - nan_number_per_sample,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate var\n",
    "    var = tf.math.divide(\n",
    "        squared_distance_from_mean,\n",
    "        input_tensor.shape[1] - nan_number_per_sample - 1,\n",
    "    )\n",
    "    \n",
    "    stack =tf.stack([nan_number_per_sample, std, var], axis=1)\n",
    "    \n",
    "    return tf.squeeze(stack, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "aFIlVzw1i_vm",
    "outputId": "b9633f5f-d205-4886-d241-3058d13d658b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " f1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f29 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f30 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f31 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f32 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f33 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f34 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f35 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f36 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f37 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f38 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f39 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f40 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f41 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f42 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f43 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f44 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f45 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f46 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f47 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f48 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f49 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f50 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f51 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f52 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f53 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f54 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f55 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f56 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f57 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f58 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f59 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f60 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f61 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f62 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f63 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f64 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f65 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f66 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f67 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f68 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f69 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f70 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f71 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f72 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f73 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f74 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f75 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f76 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f77 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f78 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f79 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f80 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f81 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f82 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f83 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f84 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f85 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f86 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f87 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f88 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f89 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f90 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f91 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f92 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f93 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f94 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f95 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f96 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f97 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f98 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f99 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f100 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f101 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f102 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f103 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f104 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f105 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f106 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f107 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f108 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f109 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f110 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f111 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f112 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f113 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f114 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f115 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f116 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f117 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " f118 (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " inputs (Concatenate)           (None, 118)          0           ['f1[0][0]',                     \n",
      "                                                                  'f2[0][0]',                     \n",
      "                                                                  'f3[0][0]',                     \n",
      "                                                                  'f4[0][0]',                     \n",
      "                                                                  'f5[0][0]',                     \n",
      "                                                                  'f6[0][0]',                     \n",
      "                                                                  'f7[0][0]',                     \n",
      "                                                                  'f8[0][0]',                     \n",
      "                                                                  'f9[0][0]',                     \n",
      "                                                                  'f10[0][0]',                    \n",
      "                                                                  'f11[0][0]',                    \n",
      "                                                                  'f12[0][0]',                    \n",
      "                                                                  'f13[0][0]',                    \n",
      "                                                                  'f14[0][0]',                    \n",
      "                                                                  'f15[0][0]',                    \n",
      "                                                                  'f16[0][0]',                    \n",
      "                                                                  'f17[0][0]',                    \n",
      "                                                                  'f18[0][0]',                    \n",
      "                                                                  'f19[0][0]',                    \n",
      "                                                                  'f20[0][0]',                    \n",
      "                                                                  'f21[0][0]',                    \n",
      "                                                                  'f22[0][0]',                    \n",
      "                                                                  'f23[0][0]',                    \n",
      "                                                                  'f24[0][0]',                    \n",
      "                                                                  'f25[0][0]',                    \n",
      "                                                                  'f26[0][0]',                    \n",
      "                                                                  'f27[0][0]',                    \n",
      "                                                                  'f28[0][0]',                    \n",
      "                                                                  'f29[0][0]',                    \n",
      "                                                                  'f30[0][0]',                    \n",
      "                                                                  'f31[0][0]',                    \n",
      "                                                                  'f32[0][0]',                    \n",
      "                                                                  'f33[0][0]',                    \n",
      "                                                                  'f34[0][0]',                    \n",
      "                                                                  'f35[0][0]',                    \n",
      "                                                                  'f36[0][0]',                    \n",
      "                                                                  'f37[0][0]',                    \n",
      "                                                                  'f38[0][0]',                    \n",
      "                                                                  'f39[0][0]',                    \n",
      "                                                                  'f40[0][0]',                    \n",
      "                                                                  'f41[0][0]',                    \n",
      "                                                                  'f42[0][0]',                    \n",
      "                                                                  'f43[0][0]',                    \n",
      "                                                                  'f44[0][0]',                    \n",
      "                                                                  'f45[0][0]',                    \n",
      "                                                                  'f46[0][0]',                    \n",
      "                                                                  'f47[0][0]',                    \n",
      "                                                                  'f48[0][0]',                    \n",
      "                                                                  'f49[0][0]',                    \n",
      "                                                                  'f50[0][0]',                    \n",
      "                                                                  'f51[0][0]',                    \n",
      "                                                                  'f52[0][0]',                    \n",
      "                                                                  'f53[0][0]',                    \n",
      "                                                                  'f54[0][0]',                    \n",
      "                                                                  'f55[0][0]',                    \n",
      "                                                                  'f56[0][0]',                    \n",
      "                                                                  'f57[0][0]',                    \n",
      "                                                                  'f58[0][0]',                    \n",
      "                                                                  'f59[0][0]',                    \n",
      "                                                                  'f60[0][0]',                    \n",
      "                                                                  'f61[0][0]',                    \n",
      "                                                                  'f62[0][0]',                    \n",
      "                                                                  'f63[0][0]',                    \n",
      "                                                                  'f64[0][0]',                    \n",
      "                                                                  'f65[0][0]',                    \n",
      "                                                                  'f66[0][0]',                    \n",
      "                                                                  'f67[0][0]',                    \n",
      "                                                                  'f68[0][0]',                    \n",
      "                                                                  'f69[0][0]',                    \n",
      "                                                                  'f70[0][0]',                    \n",
      "                                                                  'f71[0][0]',                    \n",
      "                                                                  'f72[0][0]',                    \n",
      "                                                                  'f73[0][0]',                    \n",
      "                                                                  'f74[0][0]',                    \n",
      "                                                                  'f75[0][0]',                    \n",
      "                                                                  'f76[0][0]',                    \n",
      "                                                                  'f77[0][0]',                    \n",
      "                                                                  'f78[0][0]',                    \n",
      "                                                                  'f79[0][0]',                    \n",
      "                                                                  'f80[0][0]',                    \n",
      "                                                                  'f81[0][0]',                    \n",
      "                                                                  'f82[0][0]',                    \n",
      "                                                                  'f83[0][0]',                    \n",
      "                                                                  'f84[0][0]',                    \n",
      "                                                                  'f85[0][0]',                    \n",
      "                                                                  'f86[0][0]',                    \n",
      "                                                                  'f87[0][0]',                    \n",
      "                                                                  'f88[0][0]',                    \n",
      "                                                                  'f89[0][0]',                    \n",
      "                                                                  'f90[0][0]',                    \n",
      "                                                                  'f91[0][0]',                    \n",
      "                                                                  'f92[0][0]',                    \n",
      "                                                                  'f93[0][0]',                    \n",
      "                                                                  'f94[0][0]',                    \n",
      "                                                                  'f95[0][0]',                    \n",
      "                                                                  'f96[0][0]',                    \n",
      "                                                                  'f97[0][0]',                    \n",
      "                                                                  'f98[0][0]',                    \n",
      "                                                                  'f99[0][0]',                    \n",
      "                                                                  'f100[0][0]',                   \n",
      "                                                                  'f101[0][0]',                   \n",
      "                                                                  'f102[0][0]',                   \n",
      "                                                                  'f103[0][0]',                   \n",
      "                                                                  'f104[0][0]',                   \n",
      "                                                                  'f105[0][0]',                   \n",
      "                                                                  'f106[0][0]',                   \n",
      "                                                                  'f107[0][0]',                   \n",
      "                                                                  'f108[0][0]',                   \n",
      "                                                                  'f109[0][0]',                   \n",
      "                                                                  'f110[0][0]',                   \n",
      "                                                                  'f111[0][0]',                   \n",
      "                                                                  'f112[0][0]',                   \n",
      "                                                                  'f113[0][0]',                   \n",
      "                                                                  'f114[0][0]',                   \n",
      "                                                                  'f115[0][0]',                   \n",
      "                                                                  'f116[0][0]',                   \n",
      "                                                                  'f117[0][0]',                   \n",
      "                                                                  'f118[0][0]']                   \n",
      "                                                                                                  \n",
      " tf.math.is_nan_1 (TFOpLambda)  (None, 118)          0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1)            0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.where_2 (TFOpLambda)        (None, 118)          0           ['tf.math.is_nan_1[0][0]']       \n",
      "                                                                                                  \n",
      " tf.where_3 (TFOpLambda)        (None, 118)          0           ['tf.math.is_nan_1[0][0]',       \n",
      "                                                                  'lambda_1[0][0]',               \n",
      "                                                                  'inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_2 (TFOpLamb  (None, 1)           0           ['tf.where_2[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_4 (TFOpLambda  (None, 118)         0           ['tf.where_3[0][0]',             \n",
      " )                                                                'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)         (None, 1)            0           ['tf.math.reduce_sum_2[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.square_1 (TFOpLambda)  (None, 118)          0           ['tf.math.subtract_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_3 (TFOpLamb  (None, 1)           0           ['tf.math.square_1[0][0]']       \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_5 (TFOpLambda  (None, 1)           0           ['tf.cast_1[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract_6 (TFOpLambda  (None, 1)           0           ['tf.cast_1[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.truediv_2 (TFOpLambda)  (None, 1)           0           ['tf.math.reduce_sum_3[0][0]',   \n",
      "                                                                  'tf.math.subtract_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.subtract_7 (TFOpLambda  (None, 1)           0           ['tf.math.subtract_6[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sqrt_1 (TFOpLambda)    (None, 1)            0           ['tf.math.truediv_2[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.truediv_3 (TFOpLambda)  (None, 1)           0           ['tf.math.reduce_sum_3[0][0]',   \n",
      "                                                                  'tf.math.subtract_7[0][0]']     \n",
      "                                                                                                  \n",
      " tf.stack_1 (TFOpLambda)        (None, 3, 1)         0           ['tf.cast_1[0][0]',              \n",
      "                                                                  'tf.math.sqrt_1[0][0]',         \n",
      "                                                                  'tf.math.truediv_3[0][0]']      \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_1 (TFOpLa  (None, 3)           0           ['tf.stack_1[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 121)          0           ['inputs[0][0]',                 \n",
      "                                                                  'tf.compat.v1.squeeze_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "def build_preprocessing_model(features):\n",
    "    # Create inputs\n",
    "    input_layers = []\n",
    "\n",
    "    # Each feature will be one input\n",
    "    for feature in features:\n",
    "        input_layers.append(keras.layers.Input(shape=(1,), name=feature))\n",
    "    \n",
    "    # Concatenate all inputs\n",
    "    inputs = keras.layers.concatenate(input_layers, name=\"inputs\")\n",
    "        \n",
    "    \n",
    "    # Add 3 additional features:\n",
    "    # - How many nan are they in each sample\n",
    "    # - std accross the features of each sample\n",
    "    # - var accross the features of each sample\n",
    "    additional_features = get_nan_std_var(inputs)\n",
    "    \n",
    "    outputs = keras.layers.concatenate([inputs, additional_features])\n",
    "    \n",
    "    return keras.Model(input_layers, outputs)\n",
    "\n",
    "preprocessing_model = build_preprocessing_model(features)\n",
    "preprocessing_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEtdC3ESh_UL"
   },
   "source": [
    "### GradientBoostedTreesModel Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VjQUlhih_UL",
    "outputId": "97cb95a2-c76a-49b8-b2c2-4c2fbf89ce16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmp9b_jj2ix as temporary training directory\n"
     ]
    }
   ],
   "source": [
    "model_2 = tfdf.keras.GradientBoostedTreesModel(\n",
    "    growing_strategy = 'BEST_FIRST_GLOBAL',\n",
    "    l1_regularization = 0.6,\n",
    "    preprocessing = preprocessing_model\n",
    ")\n",
    "\n",
    "model_2.compile(metrics=[keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLPsJIjGJGBy"
   },
   "source": [
    "The next cell will take some time to get executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "VDUwD193h_UL",
    "outputId": "90795d28-9d88-4b73-b345-9c07f7b0c376"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset...\n",
      "Training tensor examples:\n",
      "Features: {'f1': <tf.Tensor 'data:0' shape=(None,) dtype=float64>, 'f2': <tf.Tensor 'data_30:0' shape=(None,) dtype=float64>, 'f3': <tf.Tensor 'data_41:0' shape=(None,) dtype=float64>, 'f4': <tf.Tensor 'data_52:0' shape=(None,) dtype=float64>, 'f5': <tf.Tensor 'data_63:0' shape=(None,) dtype=float64>, 'f6': <tf.Tensor 'data_74:0' shape=(None,) dtype=float64>, 'f7': <tf.Tensor 'data_85:0' shape=(None,) dtype=float64>, 'f8': <tf.Tensor 'data_96:0' shape=(None,) dtype=float64>, 'f9': <tf.Tensor 'data_107:0' shape=(None,) dtype=float64>, 'f10': <tf.Tensor 'data_1:0' shape=(None,) dtype=float64>, 'f11': <tf.Tensor 'data_12:0' shape=(None,) dtype=float64>, 'f12': <tf.Tensor 'data_22:0' shape=(None,) dtype=float64>, 'f13': <tf.Tensor 'data_23:0' shape=(None,) dtype=float64>, 'f14': <tf.Tensor 'data_24:0' shape=(None,) dtype=float64>, 'f15': <tf.Tensor 'data_25:0' shape=(None,) dtype=float64>, 'f16': <tf.Tensor 'data_26:0' shape=(None,) dtype=float64>, 'f17': <tf.Tensor 'data_27:0' shape=(None,) dtype=float64>, 'f18': <tf.Tensor 'data_28:0' shape=(None,) dtype=float64>, 'f19': <tf.Tensor 'data_29:0' shape=(None,) dtype=float64>, 'f20': <tf.Tensor 'data_31:0' shape=(None,) dtype=float64>, 'f21': <tf.Tensor 'data_32:0' shape=(None,) dtype=float64>, 'f22': <tf.Tensor 'data_33:0' shape=(None,) dtype=float64>, 'f23': <tf.Tensor 'data_34:0' shape=(None,) dtype=float64>, 'f24': <tf.Tensor 'data_35:0' shape=(None,) dtype=float64>, 'f25': <tf.Tensor 'data_36:0' shape=(None,) dtype=float64>, 'f26': <tf.Tensor 'data_37:0' shape=(None,) dtype=float64>, 'f27': <tf.Tensor 'data_38:0' shape=(None,) dtype=float64>, 'f28': <tf.Tensor 'data_39:0' shape=(None,) dtype=float64>, 'f29': <tf.Tensor 'data_40:0' shape=(None,) dtype=float64>, 'f30': <tf.Tensor 'data_42:0' shape=(None,) dtype=float64>, 'f31': <tf.Tensor 'data_43:0' shape=(None,) dtype=float64>, 'f32': <tf.Tensor 'data_44:0' shape=(None,) dtype=float64>, 'f33': <tf.Tensor 'data_45:0' shape=(None,) dtype=float64>, 'f34': <tf.Tensor 'data_46:0' shape=(None,) dtype=float64>, 'f35': <tf.Tensor 'data_47:0' shape=(None,) dtype=float64>, 'f36': <tf.Tensor 'data_48:0' shape=(None,) dtype=float64>, 'f37': <tf.Tensor 'data_49:0' shape=(None,) dtype=float64>, 'f38': <tf.Tensor 'data_50:0' shape=(None,) dtype=float64>, 'f39': <tf.Tensor 'data_51:0' shape=(None,) dtype=float64>, 'f40': <tf.Tensor 'data_53:0' shape=(None,) dtype=float64>, 'f41': <tf.Tensor 'data_54:0' shape=(None,) dtype=float64>, 'f42': <tf.Tensor 'data_55:0' shape=(None,) dtype=float64>, 'f43': <tf.Tensor 'data_56:0' shape=(None,) dtype=float64>, 'f44': <tf.Tensor 'data_57:0' shape=(None,) dtype=float64>, 'f45': <tf.Tensor 'data_58:0' shape=(None,) dtype=float64>, 'f46': <tf.Tensor 'data_59:0' shape=(None,) dtype=float64>, 'f47': <tf.Tensor 'data_60:0' shape=(None,) dtype=float64>, 'f48': <tf.Tensor 'data_61:0' shape=(None,) dtype=float64>, 'f49': <tf.Tensor 'data_62:0' shape=(None,) dtype=float64>, 'f50': <tf.Tensor 'data_64:0' shape=(None,) dtype=float64>, 'f51': <tf.Tensor 'data_65:0' shape=(None,) dtype=float64>, 'f52': <tf.Tensor 'data_66:0' shape=(None,) dtype=float64>, 'f53': <tf.Tensor 'data_67:0' shape=(None,) dtype=float64>, 'f54': <tf.Tensor 'data_68:0' shape=(None,) dtype=float64>, 'f55': <tf.Tensor 'data_69:0' shape=(None,) dtype=float64>, 'f56': <tf.Tensor 'data_70:0' shape=(None,) dtype=float64>, 'f57': <tf.Tensor 'data_71:0' shape=(None,) dtype=float64>, 'f58': <tf.Tensor 'data_72:0' shape=(None,) dtype=float64>, 'f59': <tf.Tensor 'data_73:0' shape=(None,) dtype=float64>, 'f60': <tf.Tensor 'data_75:0' shape=(None,) dtype=float64>, 'f61': <tf.Tensor 'data_76:0' shape=(None,) dtype=float64>, 'f62': <tf.Tensor 'data_77:0' shape=(None,) dtype=float64>, 'f63': <tf.Tensor 'data_78:0' shape=(None,) dtype=float64>, 'f64': <tf.Tensor 'data_79:0' shape=(None,) dtype=float64>, 'f65': <tf.Tensor 'data_80:0' shape=(None,) dtype=float64>, 'f66': <tf.Tensor 'data_81:0' shape=(None,) dtype=float64>, 'f67': <tf.Tensor 'data_82:0' shape=(None,) dtype=float64>, 'f68': <tf.Tensor 'data_83:0' shape=(None,) dtype=float64>, 'f69': <tf.Tensor 'data_84:0' shape=(None,) dtype=float64>, 'f70': <tf.Tensor 'data_86:0' shape=(None,) dtype=float64>, 'f71': <tf.Tensor 'data_87:0' shape=(None,) dtype=float64>, 'f72': <tf.Tensor 'data_88:0' shape=(None,) dtype=float64>, 'f73': <tf.Tensor 'data_89:0' shape=(None,) dtype=float64>, 'f74': <tf.Tensor 'data_90:0' shape=(None,) dtype=float64>, 'f75': <tf.Tensor 'data_91:0' shape=(None,) dtype=float64>, 'f76': <tf.Tensor 'data_92:0' shape=(None,) dtype=float64>, 'f77': <tf.Tensor 'data_93:0' shape=(None,) dtype=float64>, 'f78': <tf.Tensor 'data_94:0' shape=(None,) dtype=float64>, 'f79': <tf.Tensor 'data_95:0' shape=(None,) dtype=float64>, 'f80': <tf.Tensor 'data_97:0' shape=(None,) dtype=float64>, 'f81': <tf.Tensor 'data_98:0' shape=(None,) dtype=float64>, 'f82': <tf.Tensor 'data_99:0' shape=(None,) dtype=float64>, 'f83': <tf.Tensor 'data_100:0' shape=(None,) dtype=float64>, 'f84': <tf.Tensor 'data_101:0' shape=(None,) dtype=float64>, 'f85': <tf.Tensor 'data_102:0' shape=(None,) dtype=float64>, 'f86': <tf.Tensor 'data_103:0' shape=(None,) dtype=float64>, 'f87': <tf.Tensor 'data_104:0' shape=(None,) dtype=float64>, 'f88': <tf.Tensor 'data_105:0' shape=(None,) dtype=float64>, 'f89': <tf.Tensor 'data_106:0' shape=(None,) dtype=float64>, 'f90': <tf.Tensor 'data_108:0' shape=(None,) dtype=float64>, 'f91': <tf.Tensor 'data_109:0' shape=(None,) dtype=float64>, 'f92': <tf.Tensor 'data_110:0' shape=(None,) dtype=float64>, 'f93': <tf.Tensor 'data_111:0' shape=(None,) dtype=float64>, 'f94': <tf.Tensor 'data_112:0' shape=(None,) dtype=float64>, 'f95': <tf.Tensor 'data_113:0' shape=(None,) dtype=float64>, 'f96': <tf.Tensor 'data_114:0' shape=(None,) dtype=float64>, 'f97': <tf.Tensor 'data_115:0' shape=(None,) dtype=float64>, 'f98': <tf.Tensor 'data_116:0' shape=(None,) dtype=float64>, 'f99': <tf.Tensor 'data_117:0' shape=(None,) dtype=float64>, 'f100': <tf.Tensor 'data_2:0' shape=(None,) dtype=float64>, 'f101': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'f102': <tf.Tensor 'data_4:0' shape=(None,) dtype=float64>, 'f103': <tf.Tensor 'data_5:0' shape=(None,) dtype=float64>, 'f104': <tf.Tensor 'data_6:0' shape=(None,) dtype=float64>, 'f105': <tf.Tensor 'data_7:0' shape=(None,) dtype=float64>, 'f106': <tf.Tensor 'data_8:0' shape=(None,) dtype=float64>, 'f107': <tf.Tensor 'data_9:0' shape=(None,) dtype=float64>, 'f108': <tf.Tensor 'data_10:0' shape=(None,) dtype=float64>, 'f109': <tf.Tensor 'data_11:0' shape=(None,) dtype=float64>, 'f110': <tf.Tensor 'data_13:0' shape=(None,) dtype=float64>, 'f111': <tf.Tensor 'data_14:0' shape=(None,) dtype=float64>, 'f112': <tf.Tensor 'data_15:0' shape=(None,) dtype=float64>, 'f113': <tf.Tensor 'data_16:0' shape=(None,) dtype=float64>, 'f114': <tf.Tensor 'data_17:0' shape=(None,) dtype=float64>, 'f115': <tf.Tensor 'data_18:0' shape=(None,) dtype=float64>, 'f116': <tf.Tensor 'data_19:0' shape=(None,) dtype=float64>, 'f117': <tf.Tensor 'data_20:0' shape=(None,) dtype=float64>, 'f118': <tf.Tensor 'data_21:0' shape=(None,) dtype=float64>}\n",
      "Label: Tensor(\"data_118:0\", shape=(None,), dtype=int64)\n",
      "Weights: None\n",
      "Tensor example after pre-processing:\n",
      "Tensor(\"model_1/concatenate_1/concat:0\", shape=(None, 121), dtype=float32)\n",
      "Normalized tensor features:\n",
      " {'model_1/concatenate_1/concat:0.0': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.1': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_1:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.2': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_2:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.3': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_3:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.4': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_4:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.5': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_5:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.6': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_6:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.7': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_7:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.8': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_8:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.9': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_9:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.10': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_10:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.11': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_11:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.12': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_12:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.13': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_13:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.14': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_14:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.15': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_15:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.16': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_16:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.17': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_17:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.18': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_18:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.19': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_19:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.20': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_20:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.21': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_21:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.22': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_22:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.23': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_23:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.24': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_24:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.25': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_25:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.26': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_26:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.27': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_27:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.28': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_28:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.29': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_29:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.30': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_30:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.31': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_31:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.32': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_32:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.33': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_33:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.34': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_34:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.35': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_35:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.36': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_36:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.37': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_37:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.38': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_38:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.39': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_39:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.40': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_40:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.41': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_41:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.42': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_42:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.43': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_43:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.44': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_44:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.45': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_45:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.46': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_46:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.47': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_47:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.48': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_48:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.49': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_49:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.50': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_50:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.51': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_51:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.52': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_52:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.53': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_53:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.54': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_54:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.55': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_55:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.56': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_56:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.57': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_57:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.58': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_58:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.59': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_59:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.60': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_60:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.61': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_61:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.62': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_62:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.63': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_63:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.64': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_64:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.65': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_65:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.66': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_66:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.67': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_67:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.68': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_68:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.69': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_69:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.70': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_70:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.71': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_71:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.72': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_72:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.73': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_73:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.74': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_74:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.75': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_75:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.76': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_76:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.77': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_77:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.78': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_78:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.79': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_79:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.80': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_80:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.81': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_81:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.82': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_82:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.83': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_83:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.84': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_84:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.85': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_85:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.86': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_86:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.87': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_87:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.88': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_88:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.89': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_89:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.90': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_90:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.91': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_91:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.92': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_92:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.93': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_93:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.94': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_94:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.95': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_95:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.96': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_96:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.97': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_97:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.98': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_98:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.99': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_99:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.100': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_100:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.101': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_101:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.102': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_102:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.103': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_103:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.104': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_104:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.105': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_105:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.106': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_106:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.107': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_107:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.108': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_108:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.109': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_109:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.110': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_110:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.111': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_111:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.112': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_112:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.113': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_113:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.114': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_114:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.115': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_115:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.116': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_116:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.117': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_117:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.118': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_118:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.119': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_119:0' shape=(None,) dtype=float32>), 'model_1/concatenate_1/concat:0.120': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'strided_slice_120:0' shape=(None,) dtype=float32>)}\n",
      "Training dataset read in 0:01:28.657005. Found 862073 examples.\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO kernel.cc:813] Start Yggdrasil model training\n",
      "[INFO kernel.cc:814] Collect training examples\n",
      "[INFO kernel.cc:422] Number of batches: 863\n",
      "[INFO kernel.cc:423] Number of examples: 862073\n",
      "[INFO kernel.cc:836] Training dataset:\n",
      "Number of records: 862073\n",
      "Number of columns: 122\n",
      "\n",
      "Number of columns by type:\n",
      "\tNUMERICAL: 121 (99.1803%)\n",
      "\tCATEGORICAL: 1 (0.819672%)\n",
      "\n",
      "Columns:\n",
      "\n",
      "NUMERICAL: 121 (99.1803%)\n",
      "\t0: \"model_1/concatenate_1/concat:0.0\" NUMERICAL num-nas:13713 (1.5907%) mean:0.090203 min:-0.14991 max:0.41517 sd:0.0435705\n",
      "\t1: \"model_1/concatenate_1/concat:0.1\" NUMERICAL num-nas:13645 (1.58281%) mean:0.345976 min:-0.019044 max:0.51899 sd:0.146234\n",
      "\t2: \"model_1/concatenate_1/concat:0.10\" NUMERICAL num-nas:13869 (1.6088%) mean:0.729908 min:-8.0863 max:8.6505 sd:1.49508\n",
      "\t3: \"model_1/concatenate_1/concat:0.100\" NUMERICAL num-nas:13841 (1.60555%) mean:20.2062 min:-4.2949 max:105.62 sd:19.612\n",
      "\t4: \"model_1/concatenate_1/concat:0.101\" NUMERICAL num-nas:13661 (1.58467%) mean:321655 min:-227770 max:2.3379e+06 sd:327770\n",
      "\t5: \"model_1/concatenate_1/concat:0.102\" NUMERICAL num-nas:13985 (1.62225%) mean:548.517 min:-222.21 max:3260.9 sd:864.193\n",
      "\t6: \"model_1/concatenate_1/concat:0.103\" NUMERICAL num-nas:13676 (1.58641%) mean:3856.35 min:-11581 max:46876 sd:6671.3\n",
      "\t7: \"model_1/concatenate_1/concat:0.104\" NUMERICAL num-nas:13827 (1.60392%) mean:0.177995 min:-0.029027 max:0.49156 sd:0.123354\n",
      "\t8: \"model_1/concatenate_1/concat:0.105\" NUMERICAL num-nas:13938 (1.6168%) mean:0.160894 min:-0.066726 max:0.84855 sd:0.141677\n",
      "\t9: \"model_1/concatenate_1/concat:0.106\" NUMERICAL num-nas:13853 (1.60694%) mean:0.0141831 min:-0.0075354 max:0.089019 sd:0.0162541\n",
      "\t10: \"model_1/concatenate_1/concat:0.107\" NUMERICAL num-nas:13857 (1.6074%) mean:1.67153e+09 min:-5.877e+08 max:7.5565e+09 sd:1.8755e+09\n",
      "\t11: \"model_1/concatenate_1/concat:0.108\" NUMERICAL num-nas:14031 (1.62759%) mean:0.398651 min:-0.042355 max:1.1236 sd:0.298089\n",
      "\t12: \"model_1/concatenate_1/concat:0.109\" NUMERICAL num-nas:13833 (1.60462%) mean:-19.9155 min:-105.86 max:1.6134 sd:18.5747\n",
      "\t13: \"model_1/concatenate_1/concat:0.11\" NUMERICAL num-nas:14021 (1.62643%) mean:1.84467e+09 min:-4.081e+08 max:8.4736e+09 sd:2.12544e+09\n",
      "\t14: \"model_1/concatenate_1/concat:0.110\" NUMERICAL num-nas:13930 (1.61587%) mean:2.07441 min:0.27704 max:4.5659 sd:0.895699\n",
      "\t15: \"model_1/concatenate_1/concat:0.111\" NUMERICAL num-nas:13841 (1.60555%) mean:23.8917 min:-27.691 max:217.84 sd:45.5696\n",
      "\t16: \"model_1/concatenate_1/concat:0.112\" NUMERICAL num-nas:13708 (1.59012%) mean:1.74612 min:-26.589 max:47.757 sd:10.0904\n",
      "\t17: \"model_1/concatenate_1/concat:0.113\" NUMERICAL num-nas:13898 (1.61216%) mean:63148.9 min:-81977 max:526050 sd:92458\n",
      "\t18: \"model_1/concatenate_1/concat:0.114\" NUMERICAL num-nas:13952 (1.61842%) mean:1.20886 min:0.90527 max:1.8748 sd:0.115007\n",
      "\t19: \"model_1/concatenate_1/concat:0.115\" NUMERICAL num-nas:13959 (1.61924%) mean:4.27522e+16 min:-8.9444e+15 max:3.2499e+17 sd:6.73152e+16\n",
      "\t20: \"model_1/concatenate_1/concat:0.116\" NUMERICAL num-nas:13797 (1.60044%) mean:3959.46 min:-415.24 max:13151 sd:3156.38\n",
      "\t21: \"model_1/concatenate_1/concat:0.117\" NUMERICAL num-nas:13668 (1.58548%) mean:0.55923 min:-0.15124 max:2.7436 sd:0.408348\n",
      "\t22: \"model_1/concatenate_1/concat:0.118\" NUMERICAL mean:1.90007 min:0 max:14 sd:2.02676\n",
      "\t23: \"model_1/concatenate_1/concat:0.119\" NUMERICAL mean:5.72266e+15 min:1.76297e+11 max:3.22603e+16 sd:6.08965e+15\n",
      "\t24: \"model_1/concatenate_1/concat:0.12\" NUMERICAL num-nas:13940 (1.61703%) mean:0.247804 min:-0.1038 max:0.58977 sd:0.101142\n",
      "\t25: \"model_1/concatenate_1/concat:0.120\" NUMERICAL mean:7.04396e+31 min:3.13606e+22 max:1.0497e+33 sd:nan\n",
      "\t26: \"model_1/concatenate_1/concat:0.13\" NUMERICAL num-nas:13681 (1.58699%) mean:6.99698 min:-0.85376 max:36.951 sd:6.62122\n",
      "\t27: \"model_1/concatenate_1/concat:0.14\" NUMERICAL num-nas:13958 (1.61912%) mean:0.0193792 min:-0.33566 max:0.49976 sd:0.101894\n",
      "\t28: \"model_1/concatenate_1/concat:0.15\" NUMERICAL num-nas:13833 (1.60462%) mean:444.496 min:-116.88 max:2335.4 sd:631.09\n",
      "\t29: \"model_1/concatenate_1/concat:0.16\" NUMERICAL num-nas:13834 (1.60474%) mean:6.89279 min:-3.6645 max:19.189 sd:1.71567\n",
      "\t30: \"model_1/concatenate_1/concat:0.17\" NUMERICAL num-nas:13833 (1.60462%) mean:4.49038 min:-0.066527 max:25.458 sd:3.90031\n",
      "\t31: \"model_1/concatenate_1/concat:0.18\" NUMERICAL num-nas:13945 (1.61761%) mean:22.4513 min:-4.4225 max:79.54 sd:14.6128\n",
      "\t32: \"model_1/concatenate_1/concat:0.19\" NUMERICAL num-nas:13852 (1.60682%) mean:203.877 min:-58.834 max:1032.2 sd:281.039\n",
      "\t33: \"model_1/concatenate_1/concat:0.2\" NUMERICAL num-nas:13927 (1.61552%) mean:4066.37 min:-9421.7 max:39544 sd:6414.42\n",
      "\t34: \"model_1/concatenate_1/concat:0.20\" NUMERICAL num-nas:13855 (1.60717%) mean:61048.7 min:-84079 max:523590 sd:89874.3\n",
      "\t35: \"model_1/concatenate_1/concat:0.21\" NUMERICAL num-nas:13752 (1.59522%) mean:2.26876 min:-6.0094 max:11.306 sd:0.895077\n",
      "\t36: \"model_1/concatenate_1/concat:0.22\" NUMERICAL num-nas:13869 (1.6088%) mean:87.1482 min:-20.514 max:160.45 sd:37.3692\n",
      "\t37: \"model_1/concatenate_1/concat:0.23\" NUMERICAL num-nas:14059 (1.63084%) mean:0.340633 min:-5.7352 max:6.96 sd:1.64382\n",
      "\t38: \"model_1/concatenate_1/concat:0.24\" NUMERICAL num-nas:13954 (1.61866%) mean:414.9 min:-71.502 max:1220.8 sd:314.752\n",
      "\t39: \"model_1/concatenate_1/concat:0.25\" NUMERICAL num-nas:13798 (1.60056%) mean:3.38186e+12 min:-6.9567e+11 max:2.5805e+13 sd:5.65735e+12\n",
      "\t40: \"model_1/concatenate_1/concat:0.26\" NUMERICAL num-nas:13886 (1.61077%) mean:1.25332e+12 min:-9.3842e+11 max:5.4471e+12 sd:1.64247e+12\n",
      "\t41: \"model_1/concatenate_1/concat:0.27\" NUMERICAL num-nas:13745 (1.59441%) mean:2.25573e+06 min:-470600 max:8.9606e+06 sd:2.30353e+06\n",
      "\t42: \"model_1/concatenate_1/concat:0.28\" NUMERICAL num-nas:13865 (1.60833%) mean:0.329019 min:-0.0056592 max:1.0952 sd:0.433835\n",
      "\t43: \"model_1/concatenate_1/concat:0.29\" NUMERICAL num-nas:13907 (1.6132%) mean:7.88405 min:-0.52999 max:36.744 sd:5.93912\n",
      "\t44: \"model_1/concatenate_1/concat:0.3\" NUMERICAL num-nas:14007 (1.6248%) mean:0.201197 min:-0.082122 max:1.3199 sd:0.212507\n",
      "\t45: \"model_1/concatenate_1/concat:0.30\" NUMERICAL num-nas:14123 (1.63826%) mean:0.394536 min:-3.8135 max:3.7531 sd:0.781864\n",
      "\t46: \"model_1/concatenate_1/concat:0.31\" NUMERICAL num-nas:13890 (1.61123%) mean:134520 min:-349650 max:1.154e+06 sd:203682\n",
      "\t47: \"model_1/concatenate_1/concat:0.32\" NUMERICAL num-nas:13964 (1.61982%) mean:358067 min:-605590 max:2.8732e+06 sd:462798\n",
      "\t48: \"model_1/concatenate_1/concat:0.33\" NUMERICAL num-nas:13682 (1.5871%) mean:-4.8793e-06 min:-0.0038813 max:0.0039147 sd:0.00153399\n",
      "\t49: \"model_1/concatenate_1/concat:0.34\" NUMERICAL num-nas:13763 (1.5965%) mean:2.78175e+16 min:-2.0689e+16 max:1.5905e+17 sd:3.45231e+16\n",
      "\t50: \"model_1/concatenate_1/concat:0.35\" NUMERICAL num-nas:13752 (1.59522%) mean:185.666 min:-2414.3 max:3728.5 sd:701.46\n",
      "\t51: \"model_1/concatenate_1/concat:0.36\" NUMERICAL num-nas:13826 (1.60381%) mean:405.95 min:-40.881 max:1218 sd:314.734\n",
      "\t52: \"model_1/concatenate_1/concat:0.37\" NUMERICAL num-nas:13911 (1.61367%) mean:1.76888 min:0.5461 max:4.084 sd:0.589188\n",
      "\t53: \"model_1/concatenate_1/concat:0.38\" NUMERICAL num-nas:13948 (1.61796%) mean:1980.31 min:-433.7 max:11195 sd:1958.36\n",
      "\t54: \"model_1/concatenate_1/concat:0.39\" NUMERICAL num-nas:13830 (1.60427%) mean:0.359671 min:-0.007641 max:1.0435 sd:0.441837\n",
      "\t55: \"model_1/concatenate_1/concat:0.4\" NUMERICAL num-nas:13815 (1.60253%) mean:0.304834 min:-0.0069898 max:0.55475 sd:0.14539\n",
      "\t56: \"model_1/concatenate_1/concat:0.40\" NUMERICAL num-nas:13872 (1.60914%) mean:446.698 min:-107.38 max:2335.4 sd:620.491\n",
      "\t57: \"model_1/concatenate_1/concat:0.41\" NUMERICAL num-nas:13824 (1.60358%) mean:0.359633 min:-0.05771 max:1.0282 sd:0.407378\n",
      "\t58: \"model_1/concatenate_1/concat:0.42\" NUMERICAL num-nas:13860 (1.60775%) mean:6.94659 min:-4.0329 max:19.978 sd:1.83269\n",
      "\t59: \"model_1/concatenate_1/concat:0.43\" NUMERICAL num-nas:13996 (1.62353%) mean:29.7586 min:-8.1892 max:180.97 sd:28.7676\n",
      "\t60: \"model_1/concatenate_1/concat:0.44\" NUMERICAL num-nas:13942 (1.61726%) mean:0.0134448 min:-0.01026 max:0.066794 sd:0.0146553\n",
      "\t61: \"model_1/concatenate_1/concat:0.45\" NUMERICAL num-nas:14093 (1.63478%) mean:4.27781 min:-3.5615 max:10.066 sd:1.14031\n",
      "\t62: \"model_1/concatenate_1/concat:0.46\" NUMERICAL num-nas:13928 (1.61564%) mean:0.0290112 min:-2.6172 max:3.0153 sd:0.677008\n",
      "\t63: \"model_1/concatenate_1/concat:0.47\" NUMERICAL num-nas:13883 (1.61042%) mean:6.37812 min:1.0564 max:16.87 sd:2.10639\n",
      "\t64: \"model_1/concatenate_1/concat:0.48\" NUMERICAL num-nas:13778 (1.59824%) mean:-0.425229 min:-1.7306 max:1.799 sd:0.729372\n",
      "\t65: \"model_1/concatenate_1/concat:0.49\" NUMERICAL num-nas:13989 (1.62272%) mean:0.299876 min:-0.006924 max:0.54832 sd:0.146138\n",
      "\t66: \"model_1/concatenate_1/concat:0.5\" NUMERICAL num-nas:14044 (1.6291%) mean:-0.0721587 min:-12.791 max:11.202 sd:2.12346\n",
      "\t67: \"model_1/concatenate_1/concat:0.50\" NUMERICAL num-nas:13906 (1.61309%) mean:56.6446 min:-131.95 max:496.75 sd:88.2037\n",
      "\t68: \"model_1/concatenate_1/concat:0.51\" NUMERICAL num-nas:13749 (1.59488%) mean:2682.92 min:-721.61 max:14553 sd:2526.44\n",
      "\t69: \"model_1/concatenate_1/concat:0.52\" NUMERICAL num-nas:13945 (1.61761%) mean:12.194 min:-26.637 max:131.75 sd:21.6322\n",
      "\t70: \"model_1/concatenate_1/concat:0.53\" NUMERICAL num-nas:13872 (1.60914%) mean:137.373 min:98.986 max:175.16 sd:16.0422\n",
      "\t71: \"model_1/concatenate_1/concat:0.54\" NUMERICAL num-nas:13839 (1.60532%) mean:0.250684 min:-0.033956 max:0.49607 sd:0.110027\n",
      "\t72: \"model_1/concatenate_1/concat:0.55\" NUMERICAL num-nas:13892 (1.61146%) mean:0.411101 min:-0.052052 max:1.1866 sd:0.323798\n",
      "\t73: \"model_1/concatenate_1/concat:0.56\" NUMERICAL num-nas:14015 (1.62573%) mean:1.11977e-05 min:-0.003899 max:0.0039055 sd:0.00151958\n",
      "\t74: \"model_1/concatenate_1/concat:0.57\" NUMERICAL num-nas:13939 (1.61692%) mean:-0.329407 min:-1.179 max:0.071947 sd:0.281542\n",
      "\t75: \"model_1/concatenate_1/concat:0.58\" NUMERICAL num-nas:13872 (1.60914%) mean:3.05813 min:0.68364 max:7.7206 sd:1.73468\n",
      "\t76: \"model_1/concatenate_1/concat:0.59\" NUMERICAL num-nas:14020 (1.62631%) mean:0.548867 min:-0.15099 max:1.0141 sd:0.268411\n",
      "\t77: \"model_1/concatenate_1/concat:0.6\" NUMERICAL num-nas:13973 (1.62086%) mean:1621.27 min:-224.8 max:5426.6 sd:1276.02\n",
      "\t78: \"model_1/concatenate_1/concat:0.60\" NUMERICAL num-nas:13890 (1.61123%) mean:0.273623 min:-0.19692 max:1.0751 sd:0.25635\n",
      "\t79: \"model_1/concatenate_1/concat:0.61\" NUMERICAL num-nas:13963 (1.6197%) mean:2.47051e+09 min:-1.8256e+09 max:1.8289e+10 sd:2.9025e+09\n",
      "\t80: \"model_1/concatenate_1/concat:0.62\" NUMERICAL num-nas:13861 (1.60787%) mean:36.8465 min:-11.941 max:210.43 sd:34.6979\n",
      "\t81: \"model_1/concatenate_1/concat:0.63\" NUMERICAL num-nas:13964 (1.61982%) mean:0.212949 min:-0.13478 max:1.352 sd:0.225117\n",
      "\t82: \"model_1/concatenate_1/concat:0.64\" NUMERICAL num-nas:13835 (1.60485%) mean:47856.5 min:-3302.6 max:91871 sd:36005.9\n",
      "\t83: \"model_1/concatenate_1/concat:0.65\" NUMERICAL num-nas:13846 (1.60613%) mean:84.0987 min:-22.021 max:161.75 sd:36.0393\n",
      "\t84: \"model_1/concatenate_1/concat:0.66\" NUMERICAL num-nas:13970 (1.62051%) mean:608.264 min:-68.682 max:1996.7 sd:527.487\n",
      "\t85: \"model_1/concatenate_1/concat:0.67\" NUMERICAL num-nas:13985 (1.62225%) mean:29.0216 min:-2.1598 max:167.66 sd:27.3771\n",
      "\t86: \"model_1/concatenate_1/concat:0.68\" NUMERICAL num-nas:13987 (1.62248%) mean:1.21246 min:0.84922 max:1.8917 sd:0.129363\n",
      "\t87: \"model_1/concatenate_1/concat:0.69\" NUMERICAL num-nas:13765 (1.59673%) mean:0.418672 min:-0.0092006 max:1.0179 sd:0.493478\n",
      "\t88: \"model_1/concatenate_1/concat:0.7\" NUMERICAL num-nas:13883 (1.61042%) mean:376992 min:-29843 max:1.9137e+06 sd:345379\n",
      "\t89: \"model_1/concatenate_1/concat:0.70\" NUMERICAL num-nas:13941 (1.61715%) mean:1.54483 min:0.7742 max:3.7853 sd:0.441672\n",
      "\t90: \"model_1/concatenate_1/concat:0.71\" NUMERICAL num-nas:13706 (1.58989%) mean:482.399 min:-64.669 max:1453.9 sd:378.661\n",
      "\t91: \"model_1/concatenate_1/concat:0.72\" NUMERICAL num-nas:14042 (1.62886%) mean:7.96022e+14 min:-2.5788e+14 max:6.0879e+15 sd:1.19162e+15\n",
      "\t92: \"model_1/concatenate_1/concat:0.73\" NUMERICAL num-nas:14078 (1.63304%) mean:1.06401e+12 min:-6.1067e+11 max:6.6946e+12 sd:2.00399e+12\n",
      "\t93: \"model_1/concatenate_1/concat:0.74\" NUMERICAL num-nas:13939 (1.61692%) mean:0.376378 min:-0.013154 max:1.0304 sd:0.444928\n",
      "\t94: \"model_1/concatenate_1/concat:0.75\" NUMERICAL num-nas:14043 (1.62898%) mean:6.87743 min:-2.9862 max:18.366 sd:1.70844\n",
      "\t95: \"model_1/concatenate_1/concat:0.76\" NUMERICAL num-nas:13716 (1.59105%) mean:10725.2 min:-1546 max:56889 sd:15108.6\n",
      "\t96: \"model_1/concatenate_1/concat:0.77\" NUMERICAL num-nas:13881 (1.61019%) mean:10525.7 min:-1284.2 max:47503 sd:10419.5\n",
      "\t97: \"model_1/concatenate_1/concat:0.78\" NUMERICAL num-nas:13829 (1.60416%) mean:1.56072 min:-24.288 max:43.552 sd:9.08336\n",
      "\t98: \"model_1/concatenate_1/concat:0.79\" NUMERICAL num-nas:13768 (1.59708%) mean:0.194318 min:-0.017615 max:1.3572 sd:0.162354\n",
      "\t99: \"model_1/concatenate_1/concat:0.8\" NUMERICAL num-nas:13730 (1.59267%) mean:1.80623e+15 min:-1.1533e+15 max:1.0417e+16 sd:2.33564e+15\n",
      "\t100: \"model_1/concatenate_1/concat:0.80\" NUMERICAL num-nas:13829 (1.60416%) mean:3.24023 min:0.9642 max:7.2883 sd:1.99261\n",
      "\t101: \"model_1/concatenate_1/concat:0.81\" NUMERICAL num-nas:13963 (1.6197%) mean:1.05401e+11 min:-7.3457e+10 max:7.3897e+11 sd:9.897e+10\n",
      "\t102: \"model_1/concatenate_1/concat:0.82\" NUMERICAL num-nas:14076 (1.63281%) mean:152.876 min:-28.752 max:950.53 sd:227.9\n",
      "\t103: \"model_1/concatenate_1/concat:0.83\" NUMERICAL num-nas:13836 (1.60497%) mean:6.12432e+06 min:-2.992e+06 max:3.4511e+07 sd:8.76281e+06\n",
      "\t104: \"model_1/concatenate_1/concat:0.84\" NUMERICAL num-nas:13882 (1.6103%) mean:635.241 min:-74.545 max:2307.5 sd:583.487\n",
      "\t105: \"model_1/concatenate_1/concat:0.85\" NUMERICAL num-nas:13939 (1.61692%) mean:3.25143e+10 min:-5.9495e+09 max:1.3097e+11 sd:3.06933e+10\n",
      "\t106: \"model_1/concatenate_1/concat:0.86\" NUMERICAL num-nas:13762 (1.59638%) mean:26.6139 min:-7.6164 max:147.08 sd:25.4748\n",
      "\t107: \"model_1/concatenate_1/concat:0.87\" NUMERICAL num-nas:14045 (1.62921%) mean:207.353 min:-22.576 max:618.13 sd:158.236\n",
      "\t108: \"model_1/concatenate_1/concat:0.88\" NUMERICAL num-nas:13883 (1.61042%) mean:3805.98 min:-296.78 max:20675 sd:3533.06\n",
      "\t109: \"model_1/concatenate_1/concat:0.89\" NUMERICAL num-nas:13943 (1.61738%) mean:6.73496 min:-0.25757 max:21.893 sd:3.15943\n",
      "\t110: \"model_1/concatenate_1/concat:0.9\" NUMERICAL num-nas:13678 (1.58664%) mean:5324.28 min:-26404 max:85622 sd:10068.4\n",
      "\t111: \"model_1/concatenate_1/concat:0.90\" NUMERICAL num-nas:13941 (1.61715%) mean:0.366788 min:-0.012238 max:0.51629 sd:0.146407\n",
      "\t112: \"model_1/concatenate_1/concat:0.91\" NUMERICAL num-nas:13918 (1.61448%) mean:4870.1 min:-12829 max:55362 sd:8431\n",
      "\t113: \"model_1/concatenate_1/concat:0.92\" NUMERICAL num-nas:13931 (1.61599%) mean:132.216 min:-12.922 max:448.78 sd:110.05\n",
      "\t114: \"model_1/concatenate_1/concat:0.93\" NUMERICAL num-nas:13912 (1.61378%) mean:0.821044 min:-3.2933 max:3.9251 sd:0.712819\n",
      "\t115: \"model_1/concatenate_1/concat:0.94\" NUMERICAL num-nas:14057 (1.6306%) mean:13.1195 min:-1.3524 max:65.317 sd:12.7396\n",
      "\t116: \"model_1/concatenate_1/concat:0.95\" NUMERICAL num-nas:13682 (1.5871%) mean:3849.76 min:-7764.3 max:38704 sd:6438.17\n",
      "\t117: \"model_1/concatenate_1/concat:0.96\" NUMERICAL num-nas:13722 (1.59174%) mean:0.99997 min:0.9961 max:1.0039 sd:0.00153282\n",
      "\t118: \"model_1/concatenate_1/concat:0.97\" NUMERICAL num-nas:13737 (1.59348%) mean:1.41523e+13 min:-5.7146e+12 max:7.1701e+13 sd:1.64046e+13\n",
      "\t119: \"model_1/concatenate_1/concat:0.98\" NUMERICAL num-nas:13915 (1.61413%) mean:1.6835 min:0.6082 max:4.1691 sd:0.712225\n",
      "\t120: \"model_1/concatenate_1/concat:0.99\" NUMERICAL num-nas:13988 (1.6226%) mean:0.425759 min:-0.034559 max:1.0613 sd:0.283624\n",
      "\n",
      "CATEGORICAL: 1 (0.819672%)\n",
      "\t121: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n",
      "\n",
      "Terminology:\n",
      "\tnas: Number of non-available (i.e. missing) values.\n",
      "\tood: Out of dictionary.\n",
      "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
      "\ttokenized: The attribute value is obtained through tokenization.\n",
      "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
      "\tvocab-size: Number of unique values.\n",
      "\n",
      "[INFO kernel.cc:882] Configure learner\n",
      "[WARNING gradient_boosted_trees.cc:1671] Subsample hyperparameter given but sampling method does not match.\n",
      "[WARNING gradient_boosted_trees.cc:1684] GOSS alpha hyperparameter given but GOSS is disabled.\n",
      "[WARNING gradient_boosted_trees.cc:1693] GOSS beta hyperparameter given but GOSS is disabled.\n",
      "[WARNING gradient_boosted_trees.cc:1705] SelGB ratio hyperparameter given but SelGB is disabled.\n",
      "[INFO kernel.cc:912] Training config:\n",
      "learner: \"GRADIENT_BOOSTED_TREES\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.0\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.1\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.10\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.100\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.101\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.102\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.103\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.104\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.105\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.106\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.107\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.108\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.109\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.11\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.110\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.111\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.112\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.113\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.114\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.115\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.116\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.117\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.118\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.119\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.12\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.120\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.13\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.14\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.15\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.16\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.17\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.18\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.19\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.2\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.20\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.21\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.22\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.23\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.24\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.25\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.26\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.27\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.28\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.29\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.3\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.30\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.31\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.32\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.33\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.34\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.35\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.36\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.37\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.38\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.39\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.4\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.40\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.41\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.42\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.43\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.44\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.45\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.46\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.47\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.48\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.49\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.5\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.50\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.51\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.52\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.53\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.54\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.55\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.56\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.57\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.58\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.59\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.6\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.60\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.61\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.62\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.63\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.64\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.65\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.66\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.67\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.68\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.69\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.7\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.70\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.71\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.72\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.73\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.74\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.75\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.76\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.77\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.78\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.79\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.8\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.80\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.81\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.82\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.83\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.84\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.85\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.86\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.87\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.88\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.89\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.9\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.90\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.91\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.92\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.93\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.94\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.95\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.96\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.97\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.98\"\n",
      "features: \"model_1/concatenate_1/concat:0\\\\.99\"\n",
      "label: \"__LABEL\"\n",
      "task: CLASSIFICATION\n",
      "random_seed: 123456\n",
      "metadata {\n",
      "  framework: \"TF Keras\"\n",
      "}\n",
      "pure_serving_model: false\n",
      "[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n",
      "  num_trees: 300\n",
      "  decision_tree {\n",
      "    max_depth: 6\n",
      "    min_examples: 5\n",
      "    in_split_min_examples_check: true\n",
      "    keep_non_leaf_label_distribution: true\n",
      "    num_candidate_attributes: -1\n",
      "    missing_value_policy: GLOBAL_IMPUTATION\n",
      "    allow_na_conditions: false\n",
      "    categorical_set_greedy_forward {\n",
      "      sampling: 0.1\n",
      "      max_num_items: -1\n",
      "      min_item_frequency: 1\n",
      "    }\n",
      "    growing_strategy_best_first_global {\n",
      "    }\n",
      "    categorical {\n",
      "      cart {\n",
      "      }\n",
      "    }\n",
      "    axis_aligned_split {\n",
      "    }\n",
      "    internal {\n",
      "      sorting_strategy: PRESORTED\n",
      "    }\n",
      "    uplift {\n",
      "      min_examples_in_treatment: 5\n",
      "      split_score: KULLBACK_LEIBLER\n",
      "    }\n",
      "  }\n",
      "  shrinkage: 0.1\n",
      "  loss: DEFAULT\n",
      "  validation_set_ratio: 0.1\n",
      "  validation_interval_in_trees: 1\n",
      "  early_stopping: VALIDATION_LOSS_INCREASE\n",
      "  early_stopping_num_trees_look_ahead: 30\n",
      "  l2_regularization: 0\n",
      "  lambda_loss: 1\n",
      "  mart {\n",
      "  }\n",
      "  adapt_subsample_for_maximum_training_duration: false\n",
      "  l1_regularization: 0.6\n",
      "  use_hessian_gain: false\n",
      "  l2_regularization_categorical: 1\n",
      "  apply_link_function: true\n",
      "  compute_permutation_variable_importance: false\n",
      "  binary_focal_loss_options {\n",
      "    misprediction_exponent: 2\n",
      "    positive_sample_coefficient: 0.5\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO kernel.cc:915] Deployment config:\n",
      "cache_path: \"/tmp/tmp9b_jj2ix/working_cache\"\n",
      "num_threads: 2\n",
      "try_resume_training: true\n",
      "\n",
      "[INFO kernel.cc:944] Train model\n",
      "[INFO gradient_boosted_trees.cc:405] Default loss set to BINOMIAL_LOG_LIKELIHOOD\n",
      "[INFO gradient_boosted_trees.cc:1008] Training gradient boosted tree on 862073 example(s) and 121 feature(s).\n",
      "[INFO gradient_boosted_trees.cc:1051] 775804 examples used for training and 86269 examples used for validation\n",
      "[INFO gradient_boosted_trees.cc:1434] \tnum-trees:1 train-loss:1.323314 train-accuracy:0.772601 valid-loss:1.323352 valid-accuracy:0.771830\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:2 train-loss:1.272039 train-accuracy:0.772553 valid-loss:1.272138 valid-accuracy:0.771795\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:3 train-loss:1.229959 train-accuracy:0.772490 valid-loss:1.230146 valid-accuracy:0.771842\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:4 train-loss:1.195232 train-accuracy:0.772504 valid-loss:1.195441 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:5 train-loss:1.166460 train-accuracy:0.772475 valid-loss:1.166737 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:6 train-loss:1.142544 train-accuracy:0.772524 valid-loss:1.142919 valid-accuracy:0.771772\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:7 train-loss:1.122620 train-accuracy:0.772498 valid-loss:1.123064 valid-accuracy:0.771784\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:8 train-loss:1.105999 train-accuracy:0.772486 valid-loss:1.106539 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:9 train-loss:1.092120 train-accuracy:0.772459 valid-loss:1.092718 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:10 train-loss:1.080522 train-accuracy:0.772480 valid-loss:1.081199 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:11 train-loss:1.070821 train-accuracy:0.772470 valid-loss:1.071561 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:12 train-loss:1.062718 train-accuracy:0.772448 valid-loss:1.063514 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:13 train-loss:1.055939 train-accuracy:0.772444 valid-loss:1.056811 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:14 train-loss:1.050266 train-accuracy:0.772435 valid-loss:1.051216 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:15 train-loss:1.045525 train-accuracy:0.772435 valid-loss:1.046546 valid-accuracy:0.771830\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:16 train-loss:1.041564 train-accuracy:0.772444 valid-loss:1.042655 valid-accuracy:0.771830\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:17 train-loss:1.038246 train-accuracy:0.772453 valid-loss:1.039407 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:18 train-loss:1.035481 train-accuracy:0.772467 valid-loss:1.036740 valid-accuracy:0.771784\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:19 train-loss:1.033155 train-accuracy:0.772457 valid-loss:1.034500 valid-accuracy:0.771795\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:20 train-loss:1.031201 train-accuracy:0.772453 valid-loss:1.032663 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:21 train-loss:1.029557 train-accuracy:0.772446 valid-loss:1.031109 valid-accuracy:0.771784\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:22 train-loss:1.028163 train-accuracy:0.772463 valid-loss:1.029795 valid-accuracy:0.771784\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:23 train-loss:1.026988 train-accuracy:0.772490 valid-loss:1.028695 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:24 train-loss:1.025992 train-accuracy:0.772499 valid-loss:1.027804 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:25 train-loss:1.025156 train-accuracy:0.772504 valid-loss:1.027032 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:26 train-loss:1.024446 train-accuracy:0.772506 valid-loss:1.026426 valid-accuracy:0.771795\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:27 train-loss:1.023822 train-accuracy:0.772506 valid-loss:1.025925 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:28 train-loss:1.023273 train-accuracy:0.772501 valid-loss:1.025495 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:29 train-loss:1.022799 train-accuracy:0.772501 valid-loss:1.025133 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:30 train-loss:1.022346 train-accuracy:0.772502 valid-loss:1.024789 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:31 train-loss:1.021961 train-accuracy:0.772520 valid-loss:1.024528 valid-accuracy:0.771818\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:32 train-loss:1.021606 train-accuracy:0.772535 valid-loss:1.024231 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:33 train-loss:1.021306 train-accuracy:0.772539 valid-loss:1.024028 valid-accuracy:0.771807\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:34 train-loss:1.021001 train-accuracy:0.772548 valid-loss:1.023824 valid-accuracy:0.771853\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:35 train-loss:1.020719 train-accuracy:0.772573 valid-loss:1.023633 valid-accuracy:0.771865\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:36 train-loss:1.020468 train-accuracy:0.772582 valid-loss:1.023463 valid-accuracy:0.771865\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:37 train-loss:1.020240 train-accuracy:0.772613 valid-loss:1.023329 valid-accuracy:0.771853\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:38 train-loss:1.020007 train-accuracy:0.772620 valid-loss:1.023215 valid-accuracy:0.771911\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:39 train-loss:1.019789 train-accuracy:0.772627 valid-loss:1.023088 valid-accuracy:0.771876\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:40 train-loss:1.019579 train-accuracy:0.772655 valid-loss:1.023015 valid-accuracy:0.771923\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:41 train-loss:1.019387 train-accuracy:0.772671 valid-loss:1.022886 valid-accuracy:0.771923\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:42 train-loss:1.019198 train-accuracy:0.772686 valid-loss:1.022807 valid-accuracy:0.771957\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:43 train-loss:1.018996 train-accuracy:0.772703 valid-loss:1.022670 valid-accuracy:0.771946\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:44 train-loss:1.018803 train-accuracy:0.772714 valid-loss:1.022534 valid-accuracy:0.771969\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:45 train-loss:1.018598 train-accuracy:0.772753 valid-loss:1.022443 valid-accuracy:0.772004\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:46 train-loss:1.018329 train-accuracy:0.772778 valid-loss:1.022296 valid-accuracy:0.771981\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:47 train-loss:1.018139 train-accuracy:0.772803 valid-loss:1.022198 valid-accuracy:0.772004\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:48 train-loss:1.017953 train-accuracy:0.772829 valid-loss:1.022119 valid-accuracy:0.772039\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:49 train-loss:1.017751 train-accuracy:0.772827 valid-loss:1.022050 valid-accuracy:0.772039\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:50 train-loss:1.017558 train-accuracy:0.772863 valid-loss:1.022008 valid-accuracy:0.772085\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 49\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:51 train-loss:1.017375 train-accuracy:0.772882 valid-loss:1.021974 valid-accuracy:0.772062\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:52 train-loss:1.017197 train-accuracy:0.772912 valid-loss:1.021924 valid-accuracy:0.772015\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:53 train-loss:1.017019 train-accuracy:0.772950 valid-loss:1.021835 valid-accuracy:0.772004\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:54 train-loss:1.016832 train-accuracy:0.772963 valid-loss:1.021780 valid-accuracy:0.772004\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:55 train-loss:1.016668 train-accuracy:0.772990 valid-loss:1.021709 valid-accuracy:0.771957\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:56 train-loss:1.016475 train-accuracy:0.773019 valid-loss:1.021639 valid-accuracy:0.771992\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:57 train-loss:1.016289 train-accuracy:0.773029 valid-loss:1.021588 valid-accuracy:0.771957\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:58 train-loss:1.016117 train-accuracy:0.773042 valid-loss:1.021545 valid-accuracy:0.771934\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:59 train-loss:1.015939 train-accuracy:0.773096 valid-loss:1.021513 valid-accuracy:0.771957\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:60 train-loss:1.015777 train-accuracy:0.773121 valid-loss:1.021462 valid-accuracy:0.771934\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:61 train-loss:1.015577 train-accuracy:0.773153 valid-loss:1.021391 valid-accuracy:0.771969\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:62 train-loss:1.015420 train-accuracy:0.773184 valid-loss:1.021340 valid-accuracy:0.771992\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:63 train-loss:1.015213 train-accuracy:0.773238 valid-loss:1.021192 valid-accuracy:0.772027\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:64 train-loss:1.015030 train-accuracy:0.773251 valid-loss:1.021151 valid-accuracy:0.772015\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:65 train-loss:1.014859 train-accuracy:0.773266 valid-loss:1.021062 valid-accuracy:0.772050\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:66 train-loss:1.014692 train-accuracy:0.773271 valid-loss:1.020978 valid-accuracy:0.772039\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:67 train-loss:1.014517 train-accuracy:0.773298 valid-loss:1.020905 valid-accuracy:0.772085\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:68 train-loss:1.014360 train-accuracy:0.773322 valid-loss:1.020868 valid-accuracy:0.772085\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:69 train-loss:1.014176 train-accuracy:0.773363 valid-loss:1.020801 valid-accuracy:0.772143\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:70 train-loss:1.014011 train-accuracy:0.773381 valid-loss:1.020734 valid-accuracy:0.772085\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:71 train-loss:1.013839 train-accuracy:0.773404 valid-loss:1.020662 valid-accuracy:0.772108\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:72 train-loss:1.013609 train-accuracy:0.773440 valid-loss:1.020519 valid-accuracy:0.772085\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:73 train-loss:1.013438 train-accuracy:0.773449 valid-loss:1.020445 valid-accuracy:0.772143\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:74 train-loss:1.013284 train-accuracy:0.773485 valid-loss:1.020440 valid-accuracy:0.772073\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:75 train-loss:1.013116 train-accuracy:0.773512 valid-loss:1.020354 valid-accuracy:0.772108\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:76 train-loss:1.012954 train-accuracy:0.773521 valid-loss:1.020299 valid-accuracy:0.772050\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:77 train-loss:1.012800 train-accuracy:0.773555 valid-loss:1.020247 valid-accuracy:0.772039\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:78 train-loss:1.012650 train-accuracy:0.773604 valid-loss:1.020222 valid-accuracy:0.772004\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:79 train-loss:1.012495 train-accuracy:0.773636 valid-loss:1.020146 valid-accuracy:0.772015\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:80 train-loss:1.012334 train-accuracy:0.773645 valid-loss:1.020097 valid-accuracy:0.772004\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:81 train-loss:1.012181 train-accuracy:0.773690 valid-loss:1.020058 valid-accuracy:0.772027\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:82 train-loss:1.012036 train-accuracy:0.773711 valid-loss:1.020040 valid-accuracy:0.772039\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:83 train-loss:1.011888 train-accuracy:0.773732 valid-loss:1.019998 valid-accuracy:0.772039\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:84 train-loss:1.011745 train-accuracy:0.773778 valid-loss:1.019960 valid-accuracy:0.772027\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:85 train-loss:1.011620 train-accuracy:0.773775 valid-loss:1.019915 valid-accuracy:0.771992\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:86 train-loss:1.011469 train-accuracy:0.773795 valid-loss:1.019881 valid-accuracy:0.772015\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:87 train-loss:1.011328 train-accuracy:0.773829 valid-loss:1.019828 valid-accuracy:0.771969\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:88 train-loss:1.011175 train-accuracy:0.773849 valid-loss:1.019789 valid-accuracy:0.771981\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:89 train-loss:1.010996 train-accuracy:0.773871 valid-loss:1.019692 valid-accuracy:0.772027\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:90 train-loss:1.010862 train-accuracy:0.773887 valid-loss:1.019658 valid-accuracy:0.772097\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:91 train-loss:1.010724 train-accuracy:0.773925 valid-loss:1.019633 valid-accuracy:0.772073\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:92 train-loss:1.010564 train-accuracy:0.773976 valid-loss:1.019582 valid-accuracy:0.772108\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:93 train-loss:1.010414 train-accuracy:0.774002 valid-loss:1.019533 valid-accuracy:0.772108\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:94 train-loss:1.010273 train-accuracy:0.774000 valid-loss:1.019501 valid-accuracy:0.772097\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:95 train-loss:1.010125 train-accuracy:0.774041 valid-loss:1.019452 valid-accuracy:0.772120\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:96 train-loss:1.009995 train-accuracy:0.774076 valid-loss:1.019427 valid-accuracy:0.772155\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:97 train-loss:1.009838 train-accuracy:0.774103 valid-loss:1.019357 valid-accuracy:0.772166\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:98 train-loss:1.009696 train-accuracy:0.774110 valid-loss:1.019367 valid-accuracy:0.772166\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:99 train-loss:1.009561 train-accuracy:0.774168 valid-loss:1.019343 valid-accuracy:0.772143\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 98\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:100 train-loss:1.009346 train-accuracy:0.774215 valid-loss:1.019211 valid-accuracy:0.772201\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:101 train-loss:1.009182 train-accuracy:0.774214 valid-loss:1.019125 valid-accuracy:0.772178\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:102 train-loss:1.009047 train-accuracy:0.774263 valid-loss:1.019083 valid-accuracy:0.772189\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:103 train-loss:1.008922 train-accuracy:0.774287 valid-loss:1.019054 valid-accuracy:0.772143\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:104 train-loss:1.008793 train-accuracy:0.774300 valid-loss:1.019015 valid-accuracy:0.772131\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:105 train-loss:1.008658 train-accuracy:0.774327 valid-loss:1.018986 valid-accuracy:0.772155\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:106 train-loss:1.008512 train-accuracy:0.774349 valid-loss:1.018934 valid-accuracy:0.772236\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:107 train-loss:1.008370 train-accuracy:0.774376 valid-loss:1.018867 valid-accuracy:0.772213\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:108 train-loss:1.008224 train-accuracy:0.774413 valid-loss:1.018863 valid-accuracy:0.772282\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:109 train-loss:1.008072 train-accuracy:0.774452 valid-loss:1.018807 valid-accuracy:0.772224\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:110 train-loss:1.007945 train-accuracy:0.774486 valid-loss:1.018801 valid-accuracy:0.772247\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:111 train-loss:1.007812 train-accuracy:0.774511 valid-loss:1.018786 valid-accuracy:0.772236\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:112 train-loss:1.007681 train-accuracy:0.774519 valid-loss:1.018751 valid-accuracy:0.772270\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:113 train-loss:1.007550 train-accuracy:0.774550 valid-loss:1.018741 valid-accuracy:0.772282\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:114 train-loss:1.007415 train-accuracy:0.774584 valid-loss:1.018733 valid-accuracy:0.772363\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:115 train-loss:1.007269 train-accuracy:0.774609 valid-loss:1.018673 valid-accuracy:0.772375\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:116 train-loss:1.007144 train-accuracy:0.774626 valid-loss:1.018650 valid-accuracy:0.772386\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:117 train-loss:1.006976 train-accuracy:0.774649 valid-loss:1.018564 valid-accuracy:0.772398\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:118 train-loss:1.006833 train-accuracy:0.774670 valid-loss:1.018527 valid-accuracy:0.772375\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:119 train-loss:1.006694 train-accuracy:0.774696 valid-loss:1.018479 valid-accuracy:0.772398\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:120 train-loss:1.006561 train-accuracy:0.774696 valid-loss:1.018452 valid-accuracy:0.772456\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:121 train-loss:1.006420 train-accuracy:0.774734 valid-loss:1.018393 valid-accuracy:0.772456\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:122 train-loss:1.006298 train-accuracy:0.774745 valid-loss:1.018366 valid-accuracy:0.772444\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:123 train-loss:1.006137 train-accuracy:0.774776 valid-loss:1.018283 valid-accuracy:0.772502\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:124 train-loss:1.005997 train-accuracy:0.774795 valid-loss:1.018263 valid-accuracy:0.772525\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:125 train-loss:1.005865 train-accuracy:0.774819 valid-loss:1.018216 valid-accuracy:0.772560\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:126 train-loss:1.005730 train-accuracy:0.774844 valid-loss:1.018167 valid-accuracy:0.772583\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:127 train-loss:1.005607 train-accuracy:0.774816 valid-loss:1.018122 valid-accuracy:0.772653\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:128 train-loss:1.005464 train-accuracy:0.774877 valid-loss:1.018083 valid-accuracy:0.772665\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:129 train-loss:1.005340 train-accuracy:0.774893 valid-loss:1.018070 valid-accuracy:0.772630\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:130 train-loss:1.005218 train-accuracy:0.774919 valid-loss:1.018035 valid-accuracy:0.772653\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:131 train-loss:1.005085 train-accuracy:0.774926 valid-loss:1.018021 valid-accuracy:0.772641\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:132 train-loss:1.004941 train-accuracy:0.774942 valid-loss:1.017969 valid-accuracy:0.772688\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:133 train-loss:1.004832 train-accuracy:0.774944 valid-loss:1.017955 valid-accuracy:0.772676\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:134 train-loss:1.004708 train-accuracy:0.774961 valid-loss:1.017930 valid-accuracy:0.772618\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:135 train-loss:1.004574 train-accuracy:0.774996 valid-loss:1.017901 valid-accuracy:0.772630\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:136 train-loss:1.004447 train-accuracy:0.775014 valid-loss:1.017867 valid-accuracy:0.772665\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:137 train-loss:1.004326 train-accuracy:0.775032 valid-loss:1.017848 valid-accuracy:0.772665\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:138 train-loss:1.004197 train-accuracy:0.775049 valid-loss:1.017805 valid-accuracy:0.772630\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:139 train-loss:1.004060 train-accuracy:0.775086 valid-loss:1.017731 valid-accuracy:0.772618\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:140 train-loss:1.003925 train-accuracy:0.775115 valid-loss:1.017688 valid-accuracy:0.772618\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:141 train-loss:1.003790 train-accuracy:0.775160 valid-loss:1.017624 valid-accuracy:0.772618\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:142 train-loss:1.003681 train-accuracy:0.775167 valid-loss:1.017620 valid-accuracy:0.772618\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:143 train-loss:1.003548 train-accuracy:0.775171 valid-loss:1.017568 valid-accuracy:0.772641\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:144 train-loss:1.003443 train-accuracy:0.775234 valid-loss:1.017542 valid-accuracy:0.772711\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:145 train-loss:1.003319 train-accuracy:0.775240 valid-loss:1.017515 valid-accuracy:0.772699\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:146 train-loss:1.003193 train-accuracy:0.775247 valid-loss:1.017502 valid-accuracy:0.772792\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:147 train-loss:1.003088 train-accuracy:0.775263 valid-loss:1.017502 valid-accuracy:0.772757\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 146\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:148 train-loss:1.002970 train-accuracy:0.775271 valid-loss:1.017500 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:149 train-loss:1.002851 train-accuracy:0.775281 valid-loss:1.017517 valid-accuracy:0.772792\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:150 train-loss:1.002741 train-accuracy:0.775308 valid-loss:1.017470 valid-accuracy:0.772804\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:151 train-loss:1.002634 train-accuracy:0.775340 valid-loss:1.017445 valid-accuracy:0.772792\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:152 train-loss:1.002529 train-accuracy:0.775380 valid-loss:1.017411 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:153 train-loss:1.002416 train-accuracy:0.775397 valid-loss:1.017395 valid-accuracy:0.772757\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:154 train-loss:1.002301 train-accuracy:0.775429 valid-loss:1.017343 valid-accuracy:0.772746\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:155 train-loss:1.002197 train-accuracy:0.775433 valid-loss:1.017340 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:156 train-loss:1.002078 train-accuracy:0.775443 valid-loss:1.017303 valid-accuracy:0.772734\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:157 train-loss:1.001960 train-accuracy:0.775432 valid-loss:1.017251 valid-accuracy:0.772757\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:158 train-loss:1.001840 train-accuracy:0.775463 valid-loss:1.017238 valid-accuracy:0.772757\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:159 train-loss:1.001722 train-accuracy:0.775495 valid-loss:1.017206 valid-accuracy:0.772734\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:160 train-loss:1.001603 train-accuracy:0.775497 valid-loss:1.017178 valid-accuracy:0.772827\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:161 train-loss:1.001480 train-accuracy:0.775506 valid-loss:1.017162 valid-accuracy:0.772850\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:162 train-loss:1.001369 train-accuracy:0.775535 valid-loss:1.017143 valid-accuracy:0.772850\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:163 train-loss:1.001250 train-accuracy:0.775557 valid-loss:1.017111 valid-accuracy:0.772804\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:164 train-loss:1.001145 train-accuracy:0.775594 valid-loss:1.017104 valid-accuracy:0.772827\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:165 train-loss:1.001027 train-accuracy:0.775624 valid-loss:1.017082 valid-accuracy:0.772862\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:166 train-loss:1.000917 train-accuracy:0.775634 valid-loss:1.017074 valid-accuracy:0.772815\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:167 train-loss:1.000814 train-accuracy:0.775673 valid-loss:1.017043 valid-accuracy:0.772896\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:168 train-loss:1.000691 train-accuracy:0.775686 valid-loss:1.017009 valid-accuracy:0.772850\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:169 train-loss:1.000588 train-accuracy:0.775723 valid-loss:1.017014 valid-accuracy:0.772815\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:170 train-loss:1.000478 train-accuracy:0.775772 valid-loss:1.016993 valid-accuracy:0.772815\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:171 train-loss:1.000371 train-accuracy:0.775776 valid-loss:1.016988 valid-accuracy:0.772850\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:172 train-loss:1.000261 train-accuracy:0.775791 valid-loss:1.016990 valid-accuracy:0.772873\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:173 train-loss:1.000149 train-accuracy:0.775833 valid-loss:1.016959 valid-accuracy:0.772908\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:174 train-loss:1.000051 train-accuracy:0.775863 valid-loss:1.016974 valid-accuracy:0.772920\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:175 train-loss:0.999944 train-accuracy:0.775869 valid-loss:1.016984 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:176 train-loss:0.999827 train-accuracy:0.775894 valid-loss:1.016980 valid-accuracy:0.772920\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:177 train-loss:0.999726 train-accuracy:0.775892 valid-loss:1.016988 valid-accuracy:0.772920\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:178 train-loss:0.999605 train-accuracy:0.775918 valid-loss:1.016993 valid-accuracy:0.772920\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:179 train-loss:0.999500 train-accuracy:0.775924 valid-loss:1.016982 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:180 train-loss:0.999394 train-accuracy:0.775941 valid-loss:1.016923 valid-accuracy:0.772862\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:181 train-loss:0.999271 train-accuracy:0.775981 valid-loss:1.016914 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:182 train-loss:0.999172 train-accuracy:0.775978 valid-loss:1.016927 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:183 train-loss:0.999063 train-accuracy:0.776005 valid-loss:1.016908 valid-accuracy:0.772862\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:184 train-loss:0.998946 train-accuracy:0.776001 valid-loss:1.016863 valid-accuracy:0.772769\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:185 train-loss:0.998821 train-accuracy:0.776044 valid-loss:1.016784 valid-accuracy:0.772804\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:186 train-loss:0.998712 train-accuracy:0.776063 valid-loss:1.016790 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:187 train-loss:0.998599 train-accuracy:0.776088 valid-loss:1.016800 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:188 train-loss:0.998492 train-accuracy:0.776133 valid-loss:1.016758 valid-accuracy:0.772954\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:189 train-loss:0.998391 train-accuracy:0.776159 valid-loss:1.016736 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:190 train-loss:0.998286 train-accuracy:0.776178 valid-loss:1.016746 valid-accuracy:0.772896\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:191 train-loss:0.998178 train-accuracy:0.776187 valid-loss:1.016744 valid-accuracy:0.772896\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:192 train-loss:0.998075 train-accuracy:0.776187 valid-loss:1.016746 valid-accuracy:0.772873\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:193 train-loss:0.997958 train-accuracy:0.776215 valid-loss:1.016724 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:194 train-loss:0.997820 train-accuracy:0.776228 valid-loss:1.016670 valid-accuracy:0.772827\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:195 train-loss:0.997717 train-accuracy:0.776248 valid-loss:1.016646 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 194\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:196 train-loss:0.997613 train-accuracy:0.776263 valid-loss:1.016664 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:197 train-loss:0.997521 train-accuracy:0.776282 valid-loss:1.016659 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:198 train-loss:0.997414 train-accuracy:0.776324 valid-loss:1.016662 valid-accuracy:0.772804\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:199 train-loss:0.997311 train-accuracy:0.776317 valid-loss:1.016650 valid-accuracy:0.772862\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:200 train-loss:0.997190 train-accuracy:0.776339 valid-loss:1.016613 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:201 train-loss:0.997077 train-accuracy:0.776352 valid-loss:1.016589 valid-accuracy:0.772827\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:202 train-loss:0.996958 train-accuracy:0.776367 valid-loss:1.016567 valid-accuracy:0.772815\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:203 train-loss:0.996823 train-accuracy:0.776396 valid-loss:1.016522 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:204 train-loss:0.996718 train-accuracy:0.776405 valid-loss:1.016517 valid-accuracy:0.772792\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:205 train-loss:0.996606 train-accuracy:0.776433 valid-loss:1.016526 valid-accuracy:0.772792\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:206 train-loss:0.996490 train-accuracy:0.776464 valid-loss:1.016519 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:207 train-loss:0.996394 train-accuracy:0.776485 valid-loss:1.016522 valid-accuracy:0.772815\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:208 train-loss:0.996303 train-accuracy:0.776511 valid-loss:1.016507 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:209 train-loss:0.996190 train-accuracy:0.776534 valid-loss:1.016494 valid-accuracy:0.772850\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:210 train-loss:0.996070 train-accuracy:0.776583 valid-loss:1.016435 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:211 train-loss:0.995961 train-accuracy:0.776593 valid-loss:1.016434 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:212 train-loss:0.995865 train-accuracy:0.776619 valid-loss:1.016414 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:213 train-loss:0.995748 train-accuracy:0.776634 valid-loss:1.016409 valid-accuracy:0.772827\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:215 train-loss:0.995535 train-accuracy:0.776638 valid-loss:1.016412 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:216 train-loss:0.995437 train-accuracy:0.776664 valid-loss:1.016395 valid-accuracy:0.772792\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:217 train-loss:0.995329 train-accuracy:0.776682 valid-loss:1.016381 valid-accuracy:0.772804\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:218 train-loss:0.995251 train-accuracy:0.776683 valid-loss:1.016386 valid-accuracy:0.772815\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:220 train-loss:0.995067 train-accuracy:0.776705 valid-loss:1.016402 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:221 train-loss:0.994955 train-accuracy:0.776743 valid-loss:1.016384 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:222 train-loss:0.994867 train-accuracy:0.776739 valid-loss:1.016381 valid-accuracy:0.772908\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:223 train-loss:0.994765 train-accuracy:0.776753 valid-loss:1.016373 valid-accuracy:0.772873\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:225 train-loss:0.994579 train-accuracy:0.776794 valid-loss:1.016353 valid-accuracy:0.772850\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:226 train-loss:0.994468 train-accuracy:0.776829 valid-loss:1.016346 valid-accuracy:0.772873\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:227 train-loss:0.994391 train-accuracy:0.776844 valid-loss:1.016367 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:228 train-loss:0.994323 train-accuracy:0.776882 valid-loss:1.016333 valid-accuracy:0.772873\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:229 train-loss:0.994212 train-accuracy:0.776892 valid-loss:1.016318 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:230 train-loss:0.994117 train-accuracy:0.776920 valid-loss:1.016320 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:231 train-loss:0.994015 train-accuracy:0.776932 valid-loss:1.016314 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:232 train-loss:0.993918 train-accuracy:0.776964 valid-loss:1.016296 valid-accuracy:0.772908\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:233 train-loss:0.993825 train-accuracy:0.776986 valid-loss:1.016287 valid-accuracy:0.772943\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:234 train-loss:0.993730 train-accuracy:0.776990 valid-loss:1.016283 valid-accuracy:0.773012\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:235 train-loss:0.993618 train-accuracy:0.777031 valid-loss:1.016288 valid-accuracy:0.772978\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:236 train-loss:0.993497 train-accuracy:0.777054 valid-loss:1.016282 valid-accuracy:0.772954\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:237 train-loss:0.993414 train-accuracy:0.777093 valid-loss:1.016266 valid-accuracy:0.772966\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:238 train-loss:0.993331 train-accuracy:0.777084 valid-loss:1.016253 valid-accuracy:0.772908\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:239 train-loss:0.993249 train-accuracy:0.777092 valid-loss:1.016230 valid-accuracy:0.772943\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:240 train-loss:0.993134 train-accuracy:0.777120 valid-loss:1.016203 valid-accuracy:0.773024\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:241 train-loss:0.993059 train-accuracy:0.777138 valid-loss:1.016214 valid-accuracy:0.773036\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:243 train-loss:0.992889 train-accuracy:0.777158 valid-loss:1.016169 valid-accuracy:0.772989\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:245 train-loss:0.992699 train-accuracy:0.777173 valid-loss:1.016171 valid-accuracy:0.772954\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:246 train-loss:0.992590 train-accuracy:0.777201 valid-loss:1.016159 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:247 train-loss:0.992566 train-accuracy:0.777209 valid-loss:1.016155 valid-accuracy:0.772966\n",
      "[INFO gradient_boosted_trees.cc:1457] Create a snapshot of the model at iteration 246\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:248 train-loss:0.992447 train-accuracy:0.777221 valid-loss:1.016125 valid-accuracy:0.773001\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:250 train-loss:0.992259 train-accuracy:0.777241 valid-loss:1.016162 valid-accuracy:0.772908\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:252 train-loss:0.992088 train-accuracy:0.777263 valid-loss:1.016163 valid-accuracy:0.772896\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:253 train-loss:0.991981 train-accuracy:0.777297 valid-loss:1.016159 valid-accuracy:0.772885\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:254 train-loss:0.991872 train-accuracy:0.777310 valid-loss:1.016144 valid-accuracy:0.772873\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:255 train-loss:0.991809 train-accuracy:0.777325 valid-loss:1.016143 valid-accuracy:0.772862\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:257 train-loss:0.991616 train-accuracy:0.777391 valid-loss:1.016152 valid-accuracy:0.772780\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:258 train-loss:0.991504 train-accuracy:0.777365 valid-loss:1.016148 valid-accuracy:0.772838\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:260 train-loss:0.991318 train-accuracy:0.777396 valid-loss:1.016165 valid-accuracy:0.772827\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:262 train-loss:0.991118 train-accuracy:0.777401 valid-loss:1.016154 valid-accuracy:0.772966\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:263 train-loss:0.991058 train-accuracy:0.777410 valid-loss:1.016153 valid-accuracy:0.772989\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:264 train-loss:0.991006 train-accuracy:0.777435 valid-loss:1.016167 valid-accuracy:0.772978\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:266 train-loss:0.990867 train-accuracy:0.777444 valid-loss:1.016166 valid-accuracy:0.772989\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:267 train-loss:0.990797 train-accuracy:0.777459 valid-loss:1.016161 valid-accuracy:0.772966\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:268 train-loss:0.990699 train-accuracy:0.777495 valid-loss:1.016150 valid-accuracy:0.773001\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:269 train-loss:0.990623 train-accuracy:0.777507 valid-loss:1.016141 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:271 train-loss:0.990448 train-accuracy:0.777540 valid-loss:1.016145 valid-accuracy:0.772943\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:272 train-loss:0.990361 train-accuracy:0.777552 valid-loss:1.016143 valid-accuracy:0.772908\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:273 train-loss:0.990303 train-accuracy:0.777565 valid-loss:1.016104 valid-accuracy:0.772920\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:274 train-loss:0.990199 train-accuracy:0.777571 valid-loss:1.016105 valid-accuracy:0.772931\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:275 train-loss:0.990085 train-accuracy:0.777562 valid-loss:1.016131 valid-accuracy:0.772896\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:277 train-loss:0.989898 train-accuracy:0.777620 valid-loss:1.016067 valid-accuracy:0.773070\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:279 train-loss:0.989766 train-accuracy:0.777659 valid-loss:1.016044 valid-accuracy:0.773047\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:280 train-loss:0.989674 train-accuracy:0.777681 valid-loss:1.016070 valid-accuracy:0.773059\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:282 train-loss:0.989467 train-accuracy:0.777685 valid-loss:1.016084 valid-accuracy:0.773070\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:283 train-loss:0.989377 train-accuracy:0.777677 valid-loss:1.016076 valid-accuracy:0.773070\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:285 train-loss:0.989200 train-accuracy:0.777704 valid-loss:1.016080 valid-accuracy:0.773001\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:286 train-loss:0.989120 train-accuracy:0.777720 valid-loss:1.016097 valid-accuracy:0.773001\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:288 train-loss:0.988948 train-accuracy:0.777752 valid-loss:1.016095 valid-accuracy:0.772966\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:290 train-loss:0.988779 train-accuracy:0.777802 valid-loss:1.016073 valid-accuracy:0.772978\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:292 train-loss:0.988636 train-accuracy:0.777842 valid-loss:1.016062 valid-accuracy:0.772896\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:294 train-loss:0.988470 train-accuracy:0.777852 valid-loss:1.016058 valid-accuracy:0.772978\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:295 train-loss:0.988383 train-accuracy:0.777868 valid-loss:1.016051 valid-accuracy:0.773001\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:296 train-loss:0.988310 train-accuracy:0.777885 valid-loss:1.016048 valid-accuracy:0.773047\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:297 train-loss:0.988249 train-accuracy:0.777896 valid-loss:1.016050 valid-accuracy:0.773059\n",
      "[INFO gradient_boosted_trees.cc:1436] \tnum-trees:299 train-loss:0.988088 train-accuracy:0.777945 valid-loss:1.016087 valid-accuracy:0.773047\n",
      "[INFO gradient_boosted_trees.cc:1434] \tnum-trees:300 train-loss:0.988020 train-accuracy:0.777959 valid-loss:1.016088 valid-accuracy:0.773047\n",
      "[INFO gradient_boosted_trees.cc:1479] Create final snapshot of the model at iteration 300\n",
      "[INFO gradient_boosted_trees.cc:230] Truncates the model to 279 tree(s) i.e. 279  iteration(s).\n",
      "[INFO gradient_boosted_trees.cc:264] Final model num-trees:279 valid-loss:1.016044 valid-accuracy:0.773047\n",
      "[INFO kernel.cc:961] Export model in log directory: /tmp/tmp9b_jj2ix with prefix f26599e5b317482c\n",
      "[INFO kernel.cc:978] Save model in resources\n",
      "[INFO kernel.cc:1176] Loading model from path /tmp/tmp9b_jj2ix/model/ with prefix f26599e5b317482c\n",
      "[INFO abstract_model.cc:1248] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n",
      "[INFO kernel.cc:1022] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 2:59:00.140120\n",
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa595c6bc10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "model_2.fit(train_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnAo-L6Sh_UM"
   },
   "source": [
    "### Post Training Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqqc3nd6h_UM"
   },
   "source": [
    "`model.summary()` shows us the overall structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "zbOF03dqh_UM",
    "outputId": "35fd1c94-79ab-49d4-a3ee-92acbad424d6"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gradient_boosted_trees_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_1 (Functional)        (None, 121)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Type: \"GRADIENT_BOOSTED_TREES\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (121):\n",
      "\tmodel_1/concatenate_1/concat:0.0\n",
      "\tmodel_1/concatenate_1/concat:0.1\n",
      "\tmodel_1/concatenate_1/concat:0.10\n",
      "\tmodel_1/concatenate_1/concat:0.100\n",
      "\tmodel_1/concatenate_1/concat:0.101\n",
      "\tmodel_1/concatenate_1/concat:0.102\n",
      "\tmodel_1/concatenate_1/concat:0.103\n",
      "\tmodel_1/concatenate_1/concat:0.104\n",
      "\tmodel_1/concatenate_1/concat:0.105\n",
      "\tmodel_1/concatenate_1/concat:0.106\n",
      "\tmodel_1/concatenate_1/concat:0.107\n",
      "\tmodel_1/concatenate_1/concat:0.108\n",
      "\tmodel_1/concatenate_1/concat:0.109\n",
      "\tmodel_1/concatenate_1/concat:0.11\n",
      "\tmodel_1/concatenate_1/concat:0.110\n",
      "\tmodel_1/concatenate_1/concat:0.111\n",
      "\tmodel_1/concatenate_1/concat:0.112\n",
      "\tmodel_1/concatenate_1/concat:0.113\n",
      "\tmodel_1/concatenate_1/concat:0.114\n",
      "\tmodel_1/concatenate_1/concat:0.115\n",
      "\tmodel_1/concatenate_1/concat:0.116\n",
      "\tmodel_1/concatenate_1/concat:0.117\n",
      "\tmodel_1/concatenate_1/concat:0.118\n",
      "\tmodel_1/concatenate_1/concat:0.119\n",
      "\tmodel_1/concatenate_1/concat:0.12\n",
      "\tmodel_1/concatenate_1/concat:0.120\n",
      "\tmodel_1/concatenate_1/concat:0.13\n",
      "\tmodel_1/concatenate_1/concat:0.14\n",
      "\tmodel_1/concatenate_1/concat:0.15\n",
      "\tmodel_1/concatenate_1/concat:0.16\n",
      "\tmodel_1/concatenate_1/concat:0.17\n",
      "\tmodel_1/concatenate_1/concat:0.18\n",
      "\tmodel_1/concatenate_1/concat:0.19\n",
      "\tmodel_1/concatenate_1/concat:0.2\n",
      "\tmodel_1/concatenate_1/concat:0.20\n",
      "\tmodel_1/concatenate_1/concat:0.21\n",
      "\tmodel_1/concatenate_1/concat:0.22\n",
      "\tmodel_1/concatenate_1/concat:0.23\n",
      "\tmodel_1/concatenate_1/concat:0.24\n",
      "\tmodel_1/concatenate_1/concat:0.25\n",
      "\tmodel_1/concatenate_1/concat:0.26\n",
      "\tmodel_1/concatenate_1/concat:0.27\n",
      "\tmodel_1/concatenate_1/concat:0.28\n",
      "\tmodel_1/concatenate_1/concat:0.29\n",
      "\tmodel_1/concatenate_1/concat:0.3\n",
      "\tmodel_1/concatenate_1/concat:0.30\n",
      "\tmodel_1/concatenate_1/concat:0.31\n",
      "\tmodel_1/concatenate_1/concat:0.32\n",
      "\tmodel_1/concatenate_1/concat:0.33\n",
      "\tmodel_1/concatenate_1/concat:0.34\n",
      "\tmodel_1/concatenate_1/concat:0.35\n",
      "\tmodel_1/concatenate_1/concat:0.36\n",
      "\tmodel_1/concatenate_1/concat:0.37\n",
      "\tmodel_1/concatenate_1/concat:0.38\n",
      "\tmodel_1/concatenate_1/concat:0.39\n",
      "\tmodel_1/concatenate_1/concat:0.4\n",
      "\tmodel_1/concatenate_1/concat:0.40\n",
      "\tmodel_1/concatenate_1/concat:0.41\n",
      "\tmodel_1/concatenate_1/concat:0.42\n",
      "\tmodel_1/concatenate_1/concat:0.43\n",
      "\tmodel_1/concatenate_1/concat:0.44\n",
      "\tmodel_1/concatenate_1/concat:0.45\n",
      "\tmodel_1/concatenate_1/concat:0.46\n",
      "\tmodel_1/concatenate_1/concat:0.47\n",
      "\tmodel_1/concatenate_1/concat:0.48\n",
      "\tmodel_1/concatenate_1/concat:0.49\n",
      "\tmodel_1/concatenate_1/concat:0.5\n",
      "\tmodel_1/concatenate_1/concat:0.50\n",
      "\tmodel_1/concatenate_1/concat:0.51\n",
      "\tmodel_1/concatenate_1/concat:0.52\n",
      "\tmodel_1/concatenate_1/concat:0.53\n",
      "\tmodel_1/concatenate_1/concat:0.54\n",
      "\tmodel_1/concatenate_1/concat:0.55\n",
      "\tmodel_1/concatenate_1/concat:0.56\n",
      "\tmodel_1/concatenate_1/concat:0.57\n",
      "\tmodel_1/concatenate_1/concat:0.58\n",
      "\tmodel_1/concatenate_1/concat:0.59\n",
      "\tmodel_1/concatenate_1/concat:0.6\n",
      "\tmodel_1/concatenate_1/concat:0.60\n",
      "\tmodel_1/concatenate_1/concat:0.61\n",
      "\tmodel_1/concatenate_1/concat:0.62\n",
      "\tmodel_1/concatenate_1/concat:0.63\n",
      "\tmodel_1/concatenate_1/concat:0.64\n",
      "\tmodel_1/concatenate_1/concat:0.65\n",
      "\tmodel_1/concatenate_1/concat:0.66\n",
      "\tmodel_1/concatenate_1/concat:0.67\n",
      "\tmodel_1/concatenate_1/concat:0.68\n",
      "\tmodel_1/concatenate_1/concat:0.69\n",
      "\tmodel_1/concatenate_1/concat:0.7\n",
      "\tmodel_1/concatenate_1/concat:0.70\n",
      "\tmodel_1/concatenate_1/concat:0.71\n",
      "\tmodel_1/concatenate_1/concat:0.72\n",
      "\tmodel_1/concatenate_1/concat:0.73\n",
      "\tmodel_1/concatenate_1/concat:0.74\n",
      "\tmodel_1/concatenate_1/concat:0.75\n",
      "\tmodel_1/concatenate_1/concat:0.76\n",
      "\tmodel_1/concatenate_1/concat:0.77\n",
      "\tmodel_1/concatenate_1/concat:0.78\n",
      "\tmodel_1/concatenate_1/concat:0.79\n",
      "\tmodel_1/concatenate_1/concat:0.8\n",
      "\tmodel_1/concatenate_1/concat:0.80\n",
      "\tmodel_1/concatenate_1/concat:0.81\n",
      "\tmodel_1/concatenate_1/concat:0.82\n",
      "\tmodel_1/concatenate_1/concat:0.83\n",
      "\tmodel_1/concatenate_1/concat:0.84\n",
      "\tmodel_1/concatenate_1/concat:0.85\n",
      "\tmodel_1/concatenate_1/concat:0.86\n",
      "\tmodel_1/concatenate_1/concat:0.87\n",
      "\tmodel_1/concatenate_1/concat:0.88\n",
      "\tmodel_1/concatenate_1/concat:0.89\n",
      "\tmodel_1/concatenate_1/concat:0.9\n",
      "\tmodel_1/concatenate_1/concat:0.90\n",
      "\tmodel_1/concatenate_1/concat:0.91\n",
      "\tmodel_1/concatenate_1/concat:0.92\n",
      "\tmodel_1/concatenate_1/concat:0.93\n",
      "\tmodel_1/concatenate_1/concat:0.94\n",
      "\tmodel_1/concatenate_1/concat:0.95\n",
      "\tmodel_1/concatenate_1/concat:0.96\n",
      "\tmodel_1/concatenate_1/concat:0.97\n",
      "\tmodel_1/concatenate_1/concat:0.98\n",
      "\tmodel_1/concatenate_1/concat:0.99\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: MEAN_MIN_DEPTH:\n",
      "    1.                            \"__LABEL\"  5.413765 ################\n",
      "    2. \"model_1/concatenate_1/concat:0.120\"  5.388329 ###############\n",
      "    3.  \"model_1/concatenate_1/concat:0.25\"  5.386595 ###############\n",
      "    4.  \"model_1/concatenate_1/concat:0.18\"  5.385901 ###############\n",
      "    5.  \"model_1/concatenate_1/concat:0.40\"  5.383010 ###############\n",
      "    6.  \"model_1/concatenate_1/concat:0.73\"  5.381425 ###############\n",
      "    7.  \"model_1/concatenate_1/concat:0.19\"  5.380435 ###############\n",
      "    8.  \"model_1/concatenate_1/concat:0.89\"  5.380422 ###############\n",
      "    9.  \"model_1/concatenate_1/concat:0.84\"  5.379889 ###############\n",
      "   10. \"model_1/concatenate_1/concat:0.119\"  5.379889 ###############\n",
      "   11.  \"model_1/concatenate_1/concat:0.63\"  5.376982 ###############\n",
      "   12. \"model_1/concatenate_1/concat:0.115\"  5.376767 ###############\n",
      "   13.  \"model_1/concatenate_1/concat:0.88\"  5.375559 ###############\n",
      "   14.  \"model_1/concatenate_1/concat:0.79\"  5.372027 ###############\n",
      "   15.  \"model_1/concatenate_1/concat:0.57\"  5.370105 ###############\n",
      "   16. \"model_1/concatenate_1/concat:0.104\"  5.366361 ###############\n",
      "   17.  \"model_1/concatenate_1/concat:0.83\"  5.366361 ###############\n",
      "   18.  \"model_1/concatenate_1/concat:0.48\"  5.366246 ###############\n",
      "   19.  \"model_1/concatenate_1/concat:0.86\"  5.366014 ###############\n",
      "   20.  \"model_1/concatenate_1/concat:0.71\"  5.365806 ###############\n",
      "   21.  \"model_1/concatenate_1/concat:0.82\"  5.365189 ###############\n",
      "   22. \"model_1/concatenate_1/concat:0.117\"  5.363021 ###############\n",
      "   23.  \"model_1/concatenate_1/concat:0.22\"  5.361706 ###############\n",
      "   24.  \"model_1/concatenate_1/concat:0.99\"  5.360420 ###############\n",
      "   25.  \"model_1/concatenate_1/concat:0.65\"  5.360354 ###############\n",
      "   26.  \"model_1/concatenate_1/concat:0.75\"  5.358308 ###############\n",
      "   27.  \"model_1/concatenate_1/concat:0.58\"  5.356402 ###############\n",
      "   28.  \"model_1/concatenate_1/concat:0.55\"  5.355377 ###############\n",
      "   29.  \"model_1/concatenate_1/concat:0.67\"  5.352771 ###############\n",
      "   30. \"model_1/concatenate_1/concat:0.100\"  5.352718 ###############\n",
      "   31.  \"model_1/concatenate_1/concat:0.32\"  5.352662 ###############\n",
      "   32.  \"model_1/concatenate_1/concat:0.13\"  5.352130 ###############\n",
      "   33.  \"model_1/concatenate_1/concat:0.93\"  5.348556 ##############\n",
      "   34.  \"model_1/concatenate_1/concat:0.17\"  5.347982 ##############\n",
      "   35.  \"model_1/concatenate_1/concat:0.87\"  5.346227 ##############\n",
      "   36. \"model_1/concatenate_1/concat:0.109\"  5.345765 ##############\n",
      "   37. \"model_1/concatenate_1/concat:0.103\"  5.343561 ##############\n",
      "   38.   \"model_1/concatenate_1/concat:0.9\"  5.343353 ##############\n",
      "   39.  \"model_1/concatenate_1/concat:0.68\"  5.342027 ##############\n",
      "   40.  \"model_1/concatenate_1/concat:0.11\"  5.338018 ##############\n",
      "   41. \"model_1/concatenate_1/concat:0.114\"  5.336483 ##############\n",
      "   42.  \"model_1/concatenate_1/concat:0.90\"  5.334778 ##############\n",
      "   43.  \"model_1/concatenate_1/concat:0.74\"  5.334450 ##############\n",
      "   44.  \"model_1/concatenate_1/concat:0.62\"  5.334163 ##############\n",
      "   45.  \"model_1/concatenate_1/concat:0.21\"  5.332276 ##############\n",
      "   46.  \"model_1/concatenate_1/concat:0.45\"  5.330387 ##############\n",
      "   47.  \"model_1/concatenate_1/concat:0.16\"  5.329594 ##############\n",
      "   48.  \"model_1/concatenate_1/concat:0.54\"  5.329247 ##############\n",
      "   49.  \"model_1/concatenate_1/concat:0.50\"  5.327860 ##############\n",
      "   50. \"model_1/concatenate_1/concat:0.110\"  5.326819 ##############\n",
      "   51. \"model_1/concatenate_1/concat:0.116\"  5.320922 ##############\n",
      "   52. \"model_1/concatenate_1/concat:0.102\"  5.319053 ##############\n",
      "   53.  \"model_1/concatenate_1/concat:0.53\"  5.317872 ##############\n",
      "   54.  \"model_1/concatenate_1/concat:0.80\"  5.316413 ##############\n",
      "   55.  \"model_1/concatenate_1/concat:0.81\"  5.313862 ##############\n",
      "   56.  \"model_1/concatenate_1/concat:0.96\"  5.311781 ##############\n",
      "   57.  \"model_1/concatenate_1/concat:0.42\"  5.311179 ##############\n",
      "   58.  \"model_1/concatenate_1/concat:0.66\"  5.309013 ##############\n",
      "   59.   \"model_1/concatenate_1/concat:0.3\"  5.307911 ##############\n",
      "   60.  \"model_1/concatenate_1/concat:0.60\"  5.304350 ##############\n",
      "   61.  \"model_1/concatenate_1/concat:0.29\"  5.302004 ##############\n",
      "   62.  \"model_1/concatenate_1/concat:0.47\"  5.299648 ##############\n",
      "   63.  \"model_1/concatenate_1/concat:0.59\"  5.299533 ##############\n",
      "   64.  \"model_1/concatenate_1/concat:0.14\"  5.299166 ##############\n",
      "   65.  \"model_1/concatenate_1/concat:0.97\"  5.297049 ##############\n",
      "   66. \"model_1/concatenate_1/concat:0.108\"  5.296749 ##############\n",
      "   67.  \"model_1/concatenate_1/concat:0.92\"  5.293094 ##############\n",
      "   68.   \"model_1/concatenate_1/concat:0.8\"  5.292652 ##############\n",
      "   69.  \"model_1/concatenate_1/concat:0.43\"  5.291592 ##############\n",
      "   70.  \"model_1/concatenate_1/concat:0.28\"  5.289936 ##############\n",
      "   71.  \"model_1/concatenate_1/concat:0.98\"  5.289813 ##############\n",
      "   72. \"model_1/concatenate_1/concat:0.112\"  5.286568 ##############\n",
      "   73. \"model_1/concatenate_1/concat:0.111\"  5.278614 #############\n",
      "   74. \"model_1/concatenate_1/concat:0.101\"  5.275014 #############\n",
      "   75.   \"model_1/concatenate_1/concat:0.1\"  5.273287 #############\n",
      "   76.  \"model_1/concatenate_1/concat:0.41\"  5.272477 #############\n",
      "   77.  \"model_1/concatenate_1/concat:0.72\"  5.269471 #############\n",
      "   78.  \"model_1/concatenate_1/concat:0.35\"  5.267968 #############\n",
      "   79.  \"model_1/concatenate_1/concat:0.78\"  5.266350 #############\n",
      "   80.  \"model_1/concatenate_1/concat:0.12\"  5.265032 #############\n",
      "   81.  \"model_1/concatenate_1/concat:0.56\"  5.264500 #############\n",
      "   82.   \"model_1/concatenate_1/concat:0.6\"  5.264219 #############\n",
      "   83.   \"model_1/concatenate_1/concat:0.2\"  5.263690 #############\n",
      "   84.  \"model_1/concatenate_1/concat:0.38\"  5.262765 #############\n",
      "   85.  \"model_1/concatenate_1/concat:0.85\"  5.260711 #############\n",
      "   86.  \"model_1/concatenate_1/concat:0.52\"  5.258065 #############\n",
      "   87.  \"model_1/concatenate_1/concat:0.15\"  5.255250 #############\n",
      "   88.  \"model_1/concatenate_1/concat:0.70\"  5.255127 #############\n",
      "   89.  \"model_1/concatenate_1/concat:0.26\"  5.252431 #############\n",
      "   90. \"model_1/concatenate_1/concat:0.113\"  5.251795 #############\n",
      "   91.  \"model_1/concatenate_1/concat:0.61\"  5.251385 #############\n",
      "   92.  \"model_1/concatenate_1/concat:0.37\"  5.249771 #############\n",
      "   93.  \"model_1/concatenate_1/concat:0.49\"  5.248412 #############\n",
      "   94. \"model_1/concatenate_1/concat:0.107\"  5.239294 #############\n",
      "   95.   \"model_1/concatenate_1/concat:0.5\"  5.238516 #############\n",
      "   96.  \"model_1/concatenate_1/concat:0.36\"  5.238238 #############\n",
      "   97.  \"model_1/concatenate_1/concat:0.51\"  5.237800 #############\n",
      "   98.  \"model_1/concatenate_1/concat:0.77\"  5.234323 #############\n",
      "   99. \"model_1/concatenate_1/concat:0.105\"  5.233933 #############\n",
      "  100.  \"model_1/concatenate_1/concat:0.95\"  5.227482 #############\n",
      "  101.  \"model_1/concatenate_1/concat:0.23\"  5.224264 #############\n",
      "  102.   \"model_1/concatenate_1/concat:0.4\"  5.222475 #############\n",
      "  103.  \"model_1/concatenate_1/concat:0.33\"  5.215130 ############\n",
      "  104.  \"model_1/concatenate_1/concat:0.10\"  5.214245 ############\n",
      "  105.  \"model_1/concatenate_1/concat:0.31\"  5.210772 ############\n",
      "  106.  \"model_1/concatenate_1/concat:0.30\"  5.205944 ############\n",
      "  107.  \"model_1/concatenate_1/concat:0.24\"  5.188576 ############\n",
      "  108.   \"model_1/concatenate_1/concat:0.7\"  5.186668 ############\n",
      "  109.  \"model_1/concatenate_1/concat:0.20\"  5.180791 ############\n",
      "  110.  \"model_1/concatenate_1/concat:0.91\"  5.169094 ############\n",
      "  111.   \"model_1/concatenate_1/concat:0.0\"  5.159491 ############\n",
      "  112. \"model_1/concatenate_1/concat:0.106\"  5.155901 ############\n",
      "  113.  \"model_1/concatenate_1/concat:0.94\"  5.150794 ###########\n",
      "  114.  \"model_1/concatenate_1/concat:0.27\"  5.141595 ###########\n",
      "  115.  \"model_1/concatenate_1/concat:0.76\"  5.126916 ###########\n",
      "  116.  \"model_1/concatenate_1/concat:0.46\"  5.124586 ###########\n",
      "  117.  \"model_1/concatenate_1/concat:0.69\"  5.120330 ###########\n",
      "  118.  \"model_1/concatenate_1/concat:0.64\"  5.093960 ###########\n",
      "  119.  \"model_1/concatenate_1/concat:0.34\"  5.073396 ##########\n",
      "  120.  \"model_1/concatenate_1/concat:0.39\"  5.008979 #########\n",
      "  121.  \"model_1/concatenate_1/concat:0.44\"  4.967036 #########\n",
      "  122. \"model_1/concatenate_1/concat:0.118\"  4.379873 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1. \"model_1/concatenate_1/concat:0.118\" 49.000000 ################\n",
      "    2.  \"model_1/concatenate_1/concat:0.39\" 10.000000 ###\n",
      "    3.  \"model_1/concatenate_1/concat:0.44\" 10.000000 ###\n",
      "    4.  \"model_1/concatenate_1/concat:0.64\"  9.000000 ##\n",
      "    5.  \"model_1/concatenate_1/concat:0.34\"  7.000000 ##\n",
      "    6.  \"model_1/concatenate_1/concat:0.91\"  7.000000 ##\n",
      "    7.  \"model_1/concatenate_1/concat:0.10\"  6.000000 #\n",
      "    8.  \"model_1/concatenate_1/concat:0.24\"  6.000000 #\n",
      "    9.  \"model_1/concatenate_1/concat:0.27\"  6.000000 #\n",
      "   10.  \"model_1/concatenate_1/concat:0.69\"  6.000000 #\n",
      "   11.  \"model_1/concatenate_1/concat:0.76\"  6.000000 #\n",
      "   12.   \"model_1/concatenate_1/concat:0.0\"  5.000000 #\n",
      "   13. \"model_1/concatenate_1/concat:0.106\"  5.000000 #\n",
      "   14. \"model_1/concatenate_1/concat:0.107\"  5.000000 #\n",
      "   15.  \"model_1/concatenate_1/concat:0.23\"  5.000000 #\n",
      "   16.   \"model_1/concatenate_1/concat:0.7\"  5.000000 #\n",
      "   17.  \"model_1/concatenate_1/concat:0.94\"  5.000000 #\n",
      "   18.  \"model_1/concatenate_1/concat:0.26\"  4.000000 #\n",
      "   19.  \"model_1/concatenate_1/concat:0.31\"  4.000000 #\n",
      "   20.  \"model_1/concatenate_1/concat:0.52\"  4.000000 #\n",
      "   21.  \"model_1/concatenate_1/concat:0.72\"  4.000000 #\n",
      "   22.   \"model_1/concatenate_1/concat:0.1\"  3.000000 \n",
      "   23. \"model_1/concatenate_1/concat:0.112\"  3.000000 \n",
      "   24. \"model_1/concatenate_1/concat:0.113\"  3.000000 \n",
      "   25.  \"model_1/concatenate_1/concat:0.12\"  3.000000 \n",
      "   26.  \"model_1/concatenate_1/concat:0.28\"  3.000000 \n",
      "   27.   \"model_1/concatenate_1/concat:0.3\"  3.000000 \n",
      "   28.  \"model_1/concatenate_1/concat:0.30\"  3.000000 \n",
      "   29.  \"model_1/concatenate_1/concat:0.35\"  3.000000 \n",
      "   30.  \"model_1/concatenate_1/concat:0.37\"  3.000000 \n",
      "   31.   \"model_1/concatenate_1/concat:0.4\"  3.000000 \n",
      "   32.  \"model_1/concatenate_1/concat:0.41\"  3.000000 \n",
      "   33.  \"model_1/concatenate_1/concat:0.46\"  3.000000 \n",
      "   34.  \"model_1/concatenate_1/concat:0.49\"  3.000000 \n",
      "   35.  \"model_1/concatenate_1/concat:0.70\"  3.000000 \n",
      "   36.  \"model_1/concatenate_1/concat:0.92\"  3.000000 \n",
      "   37.  \"model_1/concatenate_1/concat:0.95\"  3.000000 \n",
      "   38.  \"model_1/concatenate_1/concat:0.98\"  3.000000 \n",
      "   39. \"model_1/concatenate_1/concat:0.101\"  2.000000 \n",
      "   40. \"model_1/concatenate_1/concat:0.105\"  2.000000 \n",
      "   41. \"model_1/concatenate_1/concat:0.108\"  2.000000 \n",
      "   42. \"model_1/concatenate_1/concat:0.111\"  2.000000 \n",
      "   43.  \"model_1/concatenate_1/concat:0.15\"  2.000000 \n",
      "   44.  \"model_1/concatenate_1/concat:0.29\"  2.000000 \n",
      "   45.  \"model_1/concatenate_1/concat:0.36\"  2.000000 \n",
      "   46.  \"model_1/concatenate_1/concat:0.38\"  2.000000 \n",
      "   47.  \"model_1/concatenate_1/concat:0.42\"  2.000000 \n",
      "   48.  \"model_1/concatenate_1/concat:0.47\"  2.000000 \n",
      "   49.   \"model_1/concatenate_1/concat:0.5\"  2.000000 \n",
      "   50.  \"model_1/concatenate_1/concat:0.50\"  2.000000 \n",
      "   51.  \"model_1/concatenate_1/concat:0.51\"  2.000000 \n",
      "   52.  \"model_1/concatenate_1/concat:0.53\"  2.000000 \n",
      "   53.  \"model_1/concatenate_1/concat:0.59\"  2.000000 \n",
      "   54.  \"model_1/concatenate_1/concat:0.61\"  2.000000 \n",
      "   55.  \"model_1/concatenate_1/concat:0.62\"  2.000000 \n",
      "   56.  \"model_1/concatenate_1/concat:0.66\"  2.000000 \n",
      "   57.  \"model_1/concatenate_1/concat:0.77\"  2.000000 \n",
      "   58.  \"model_1/concatenate_1/concat:0.78\"  2.000000 \n",
      "   59.  \"model_1/concatenate_1/concat:0.81\"  2.000000 \n",
      "   60.  \"model_1/concatenate_1/concat:0.85\"  2.000000 \n",
      "   61.  \"model_1/concatenate_1/concat:0.90\"  2.000000 \n",
      "   62. \"model_1/concatenate_1/concat:0.109\"  1.000000 \n",
      "   63. \"model_1/concatenate_1/concat:0.110\"  1.000000 \n",
      "   64. \"model_1/concatenate_1/concat:0.114\"  1.000000 \n",
      "   65. \"model_1/concatenate_1/concat:0.119\"  1.000000 \n",
      "   66.  \"model_1/concatenate_1/concat:0.14\"  1.000000 \n",
      "   67.   \"model_1/concatenate_1/concat:0.2\"  1.000000 \n",
      "   68.  \"model_1/concatenate_1/concat:0.21\"  1.000000 \n",
      "   69.  \"model_1/concatenate_1/concat:0.43\"  1.000000 \n",
      "   70.   \"model_1/concatenate_1/concat:0.6\"  1.000000 \n",
      "   71.  \"model_1/concatenate_1/concat:0.65\"  1.000000 \n",
      "   72.  \"model_1/concatenate_1/concat:0.74\"  1.000000 \n",
      "   73.  \"model_1/concatenate_1/concat:0.80\"  1.000000 \n",
      "   74.  \"model_1/concatenate_1/concat:0.96\"  1.000000 \n",
      "   75.  \"model_1/concatenate_1/concat:0.97\"  1.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1.  \"model_1/concatenate_1/concat:0.39\" 219.000000 ################\n",
      "    2. \"model_1/concatenate_1/concat:0.118\" 167.000000 ###########\n",
      "    3.  \"model_1/concatenate_1/concat:0.69\" 146.000000 ##########\n",
      "    4.  \"model_1/concatenate_1/concat:0.46\" 143.000000 #########\n",
      "    5.  \"model_1/concatenate_1/concat:0.44\" 127.000000 ########\n",
      "    6.  \"model_1/concatenate_1/concat:0.20\" 124.000000 ########\n",
      "    7.  \"model_1/concatenate_1/concat:0.33\" 121.000000 ########\n",
      "    8.  \"model_1/concatenate_1/concat:0.34\" 116.000000 #######\n",
      "    9.   \"model_1/concatenate_1/concat:0.0\" 115.000000 #######\n",
      "   10.  \"model_1/concatenate_1/concat:0.51\" 110.000000 #######\n",
      "   11.  \"model_1/concatenate_1/concat:0.56\" 110.000000 #######\n",
      "   12.  \"model_1/concatenate_1/concat:0.64\" 106.000000 ######\n",
      "   13.  \"model_1/concatenate_1/concat:0.76\" 106.000000 ######\n",
      "   14. \"model_1/concatenate_1/concat:0.106\" 103.000000 ######\n",
      "   15.   \"model_1/concatenate_1/concat:0.7\" 103.000000 ######\n",
      "   16.  \"model_1/concatenate_1/concat:0.27\" 101.000000 ######\n",
      "   17.  \"model_1/concatenate_1/concat:0.95\" 100.000000 ######\n",
      "   18.  \"model_1/concatenate_1/concat:0.61\" 98.000000 ######\n",
      "   19.  \"model_1/concatenate_1/concat:0.94\" 97.000000 ######\n",
      "   20.   \"model_1/concatenate_1/concat:0.2\" 96.000000 ######\n",
      "   21.   \"model_1/concatenate_1/concat:0.5\" 96.000000 ######\n",
      "   22.   \"model_1/concatenate_1/concat:0.4\" 95.000000 ######\n",
      "   23. \"model_1/concatenate_1/concat:0.105\" 94.000000 #####\n",
      "   24.  \"model_1/concatenate_1/concat:0.30\" 93.000000 #####\n",
      "   25.   \"model_1/concatenate_1/concat:0.6\" 93.000000 #####\n",
      "   26. \"model_1/concatenate_1/concat:0.101\" 91.000000 #####\n",
      "   27.  \"model_1/concatenate_1/concat:0.31\" 87.000000 #####\n",
      "   28.  \"model_1/concatenate_1/concat:0.35\" 82.000000 ####\n",
      "   29.  \"model_1/concatenate_1/concat:0.91\" 82.000000 ####\n",
      "   30.  \"model_1/concatenate_1/concat:0.77\" 81.000000 ####\n",
      "   31.   \"model_1/concatenate_1/concat:0.1\" 80.000000 ####\n",
      "   32.  \"model_1/concatenate_1/concat:0.12\" 79.000000 ####\n",
      "   33.  \"model_1/concatenate_1/concat:0.60\" 79.000000 ####\n",
      "   34.  \"model_1/concatenate_1/concat:0.47\" 77.000000 ####\n",
      "   35.   \"model_1/concatenate_1/concat:0.8\" 77.000000 ####\n",
      "   36.  \"model_1/concatenate_1/concat:0.85\" 77.000000 ####\n",
      "   37.  \"model_1/concatenate_1/concat:0.37\" 76.000000 ####\n",
      "   38.  \"model_1/concatenate_1/concat:0.14\" 75.000000 ####\n",
      "   39.  \"model_1/concatenate_1/concat:0.26\" 75.000000 ####\n",
      "   40.  \"model_1/concatenate_1/concat:0.70\" 75.000000 ####\n",
      "   41.  \"model_1/concatenate_1/concat:0.98\" 75.000000 ####\n",
      "   42.  \"model_1/concatenate_1/concat:0.15\" 74.000000 ####\n",
      "   43.  \"model_1/concatenate_1/concat:0.23\" 74.000000 ####\n",
      "   44.  \"model_1/concatenate_1/concat:0.24\" 74.000000 ####\n",
      "   45.  \"model_1/concatenate_1/concat:0.78\" 74.000000 ####\n",
      "   46.  \"model_1/concatenate_1/concat:0.36\" 73.000000 ####\n",
      "   47.  \"model_1/concatenate_1/concat:0.43\" 72.000000 ####\n",
      "   48.  \"model_1/concatenate_1/concat:0.41\" 70.000000 ####\n",
      "   49. \"model_1/concatenate_1/concat:0.108\" 68.000000 ###\n",
      "   50.  \"model_1/concatenate_1/concat:0.49\" 68.000000 ###\n",
      "   51.  \"model_1/concatenate_1/concat:0.52\" 68.000000 ###\n",
      "   52. \"model_1/concatenate_1/concat:0.113\" 67.000000 ###\n",
      "   53.  \"model_1/concatenate_1/concat:0.74\" 66.000000 ###\n",
      "   54. \"model_1/concatenate_1/concat:0.107\" 65.000000 ###\n",
      "   55.  \"model_1/concatenate_1/concat:0.29\" 65.000000 ###\n",
      "   56.   \"model_1/concatenate_1/concat:0.9\" 65.000000 ###\n",
      "   57.  \"model_1/concatenate_1/concat:0.21\" 62.000000 ###\n",
      "   58.  \"model_1/concatenate_1/concat:0.53\" 62.000000 ###\n",
      "   59.  \"model_1/concatenate_1/concat:0.54\" 62.000000 ###\n",
      "   60. \"model_1/concatenate_1/concat:0.104\" 61.000000 ###\n",
      "   61.  \"model_1/concatenate_1/concat:0.11\" 61.000000 ###\n",
      "   62.  \"model_1/concatenate_1/concat:0.92\" 61.000000 ###\n",
      "   63. \"model_1/concatenate_1/concat:0.103\" 60.000000 ###\n",
      "   64.  \"model_1/concatenate_1/concat:0.72\" 60.000000 ###\n",
      "   65. \"model_1/concatenate_1/concat:0.112\" 58.000000 ###\n",
      "   66.  \"model_1/concatenate_1/concat:0.16\" 58.000000 ###\n",
      "   67.  \"model_1/concatenate_1/concat:0.99\" 58.000000 ###\n",
      "   68. \"model_1/concatenate_1/concat:0.116\" 57.000000 ##\n",
      "   69.  \"model_1/concatenate_1/concat:0.68\" 56.000000 ##\n",
      "   70. \"model_1/concatenate_1/concat:0.102\" 55.000000 ##\n",
      "   71. \"model_1/concatenate_1/concat:0.111\" 55.000000 ##\n",
      "   72.  \"model_1/concatenate_1/concat:0.45\" 55.000000 ##\n",
      "   73.  \"model_1/concatenate_1/concat:0.80\" 55.000000 ##\n",
      "   74.  \"model_1/concatenate_1/concat:0.81\" 54.000000 ##\n",
      "   75.  \"model_1/concatenate_1/concat:0.10\" 53.000000 ##\n",
      "   76.  \"model_1/concatenate_1/concat:0.17\" 53.000000 ##\n",
      "   77.  \"model_1/concatenate_1/concat:0.83\" 53.000000 ##\n",
      "   78. \"model_1/concatenate_1/concat:0.100\" 52.000000 ##\n",
      "   79. \"model_1/concatenate_1/concat:0.109\" 52.000000 ##\n",
      "   80.  \"model_1/concatenate_1/concat:0.22\" 52.000000 ##\n",
      "   81.  \"model_1/concatenate_1/concat:0.42\" 52.000000 ##\n",
      "   82.  \"model_1/concatenate_1/concat:0.50\" 52.000000 ##\n",
      "   83.  \"model_1/concatenate_1/concat:0.59\" 52.000000 ##\n",
      "   84.  \"model_1/concatenate_1/concat:0.82\" 52.000000 ##\n",
      "   85.  \"model_1/concatenate_1/concat:0.13\" 51.000000 ##\n",
      "   86.  \"model_1/concatenate_1/concat:0.32\" 51.000000 ##\n",
      "   87. \"model_1/concatenate_1/concat:0.110\" 50.000000 ##\n",
      "   88.  \"model_1/concatenate_1/concat:0.38\" 50.000000 ##\n",
      "   89.  \"model_1/concatenate_1/concat:0.66\" 50.000000 ##\n",
      "   90.  \"model_1/concatenate_1/concat:0.86\" 50.000000 ##\n",
      "   91. \"model_1/concatenate_1/concat:0.114\" 49.000000 ##\n",
      "   92.  \"model_1/concatenate_1/concat:0.48\" 49.000000 ##\n",
      "   93.  \"model_1/concatenate_1/concat:0.89\" 49.000000 ##\n",
      "   94.  \"model_1/concatenate_1/concat:0.28\" 47.000000 ##\n",
      "   95.  \"model_1/concatenate_1/concat:0.67\" 47.000000 ##\n",
      "   96.  \"model_1/concatenate_1/concat:0.40\" 46.000000 ##\n",
      "   97.  \"model_1/concatenate_1/concat:0.55\" 46.000000 ##\n",
      "   98.  \"model_1/concatenate_1/concat:0.97\" 46.000000 ##\n",
      "   99.  \"model_1/concatenate_1/concat:0.19\" 45.000000 ##\n",
      "  100.  \"model_1/concatenate_1/concat:0.65\" 45.000000 ##\n",
      "  101.  \"model_1/concatenate_1/concat:0.71\" 45.000000 ##\n",
      "  102. \"model_1/concatenate_1/concat:0.117\" 44.000000 #\n",
      "  103.  \"model_1/concatenate_1/concat:0.73\" 44.000000 #\n",
      "  104.  \"model_1/concatenate_1/concat:0.93\" 44.000000 #\n",
      "  105.   \"model_1/concatenate_1/concat:0.3\" 43.000000 #\n",
      "  106.  \"model_1/concatenate_1/concat:0.87\" 43.000000 #\n",
      "  107.  \"model_1/concatenate_1/concat:0.96\" 42.000000 #\n",
      "  108.  \"model_1/concatenate_1/concat:0.57\" 40.000000 #\n",
      "  109.  \"model_1/concatenate_1/concat:0.63\" 40.000000 #\n",
      "  110.  \"model_1/concatenate_1/concat:0.75\" 40.000000 #\n",
      "  111.  \"model_1/concatenate_1/concat:0.90\" 40.000000 #\n",
      "  112.  \"model_1/concatenate_1/concat:0.58\" 39.000000 #\n",
      "  113.  \"model_1/concatenate_1/concat:0.88\" 38.000000 #\n",
      "  114.  \"model_1/concatenate_1/concat:0.84\" 36.000000 #\n",
      "  115. \"model_1/concatenate_1/concat:0.115\" 35.000000 #\n",
      "  116.  \"model_1/concatenate_1/concat:0.79\" 35.000000 #\n",
      "  117.  \"model_1/concatenate_1/concat:0.62\" 34.000000 #\n",
      "  118.  \"model_1/concatenate_1/concat:0.18\" 33.000000 #\n",
      "  119.  \"model_1/concatenate_1/concat:0.25\" 33.000000 #\n",
      "  120. \"model_1/concatenate_1/concat:0.119\" 22.000000 \n",
      "  121. \"model_1/concatenate_1/concat:0.120\" 20.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1. \"model_1/concatenate_1/concat:0.118\" 339968.791401 ################\n",
      "    2.  \"model_1/concatenate_1/concat:0.39\" 1199.815404 \n",
      "    3.  \"model_1/concatenate_1/concat:0.69\" 768.688713 \n",
      "    4.  \"model_1/concatenate_1/concat:0.46\" 638.391007 \n",
      "    5.  \"model_1/concatenate_1/concat:0.20\" 559.438429 \n",
      "    6.  \"model_1/concatenate_1/concat:0.44\" 555.698972 \n",
      "    7.  \"model_1/concatenate_1/concat:0.34\" 554.544041 \n",
      "    8.  \"model_1/concatenate_1/concat:0.33\" 518.277000 \n",
      "    9.  \"model_1/concatenate_1/concat:0.56\" 435.676815 \n",
      "   10.  \"model_1/concatenate_1/concat:0.64\" 433.251336 \n",
      "   11.  \"model_1/concatenate_1/concat:0.76\" 419.493707 \n",
      "   12.   \"model_1/concatenate_1/concat:0.7\" 417.952136 \n",
      "   13.   \"model_1/concatenate_1/concat:0.2\" 408.989349 \n",
      "   14.   \"model_1/concatenate_1/concat:0.0\" 407.890371 \n",
      "   15.  \"model_1/concatenate_1/concat:0.95\" 402.691986 \n",
      "   16. \"model_1/concatenate_1/concat:0.106\" 387.658683 \n",
      "   17.  \"model_1/concatenate_1/concat:0.94\" 379.377236 \n",
      "   18.  \"model_1/concatenate_1/concat:0.51\" 358.842359 \n",
      "   19.  \"model_1/concatenate_1/concat:0.30\" 348.422525 \n",
      "   20.   \"model_1/concatenate_1/concat:0.4\" 327.736425 \n",
      "   21. \"model_1/concatenate_1/concat:0.105\" 321.328681 \n",
      "   22.  \"model_1/concatenate_1/concat:0.61\" 321.309494 \n",
      "   23.  \"model_1/concatenate_1/concat:0.27\" 312.780352 \n",
      "   24.   \"model_1/concatenate_1/concat:0.6\" 309.370783 \n",
      "   25.  \"model_1/concatenate_1/concat:0.31\" 306.626926 \n",
      "   26.  \"model_1/concatenate_1/concat:0.15\" 299.671926 \n",
      "   27.   \"model_1/concatenate_1/concat:0.5\" 292.268396 \n",
      "   28.  \"model_1/concatenate_1/concat:0.35\" 272.709482 \n",
      "   29.  \"model_1/concatenate_1/concat:0.37\" 270.157363 \n",
      "   30. \"model_1/concatenate_1/concat:0.101\" 267.791098 \n",
      "   31.  \"model_1/concatenate_1/concat:0.91\" 267.098256 \n",
      "   32.   \"model_1/concatenate_1/concat:0.8\" 263.186441 \n",
      "   33.  \"model_1/concatenate_1/concat:0.47\" 261.975225 \n",
      "   34.  \"model_1/concatenate_1/concat:0.77\" 259.310083 \n",
      "   35.  \"model_1/concatenate_1/concat:0.70\" 253.675412 \n",
      "   36.  \"model_1/concatenate_1/concat:0.85\" 251.815656 \n",
      "   37.  \"model_1/concatenate_1/concat:0.60\" 251.572246 \n",
      "   38.  \"model_1/concatenate_1/concat:0.12\" 242.001265 \n",
      "   39.   \"model_1/concatenate_1/concat:0.1\" 241.964242 \n",
      "   40.  \"model_1/concatenate_1/concat:0.74\" 235.717556 \n",
      "   41.  \"model_1/concatenate_1/concat:0.24\" 228.886941 \n",
      "   42.  \"model_1/concatenate_1/concat:0.78\" 226.984506 \n",
      "   43.  \"model_1/concatenate_1/concat:0.36\" 226.624126 \n",
      "   44.  \"model_1/concatenate_1/concat:0.23\" 223.260721 \n",
      "   45.  \"model_1/concatenate_1/concat:0.26\" 222.948887 \n",
      "   46.  \"model_1/concatenate_1/concat:0.14\" 222.619592 \n",
      "   47.  \"model_1/concatenate_1/concat:0.98\" 221.521986 \n",
      "   48.  \"model_1/concatenate_1/concat:0.43\" 209.573907 \n",
      "   49.  \"model_1/concatenate_1/concat:0.49\" 207.965044 \n",
      "   50.  \"model_1/concatenate_1/concat:0.41\" 206.753349 \n",
      "   51.  \"model_1/concatenate_1/concat:0.52\" 204.818581 \n",
      "   52. \"model_1/concatenate_1/concat:0.113\" 201.682513 \n",
      "   53. \"model_1/concatenate_1/concat:0.107\" 198.690691 \n",
      "   54.  \"model_1/concatenate_1/concat:0.11\" 190.858114 \n",
      "   55.   \"model_1/concatenate_1/concat:0.9\" 189.471111 \n",
      "   56. \"model_1/concatenate_1/concat:0.108\" 188.608596 \n",
      "   57. \"model_1/concatenate_1/concat:0.102\" 182.957136 \n",
      "   58.  \"model_1/concatenate_1/concat:0.29\" 182.859036 \n",
      "   59.  \"model_1/concatenate_1/concat:0.53\" 174.105415 \n",
      "   60.  \"model_1/concatenate_1/concat:0.92\" 174.032163 \n",
      "   61.  \"model_1/concatenate_1/concat:0.21\" 171.894454 \n",
      "   62. \"model_1/concatenate_1/concat:0.112\" 171.292952 \n",
      "   63. \"model_1/concatenate_1/concat:0.103\" 170.796115 \n",
      "   64.  \"model_1/concatenate_1/concat:0.54\" 168.873904 \n",
      "   65.  \"model_1/concatenate_1/concat:0.45\" 168.555767 \n",
      "   66. \"model_1/concatenate_1/concat:0.104\" 166.471535 \n",
      "   67.  \"model_1/concatenate_1/concat:0.72\" 161.119137 \n",
      "   68. \"model_1/concatenate_1/concat:0.116\" 159.039181 \n",
      "   69.  \"model_1/concatenate_1/concat:0.13\" 157.250462 \n",
      "   70.  \"model_1/concatenate_1/concat:0.68\" 156.411221 \n",
      "   71.  \"model_1/concatenate_1/concat:0.80\" 154.388665 \n",
      "   72. \"model_1/concatenate_1/concat:0.100\" 151.848942 \n",
      "   73.  \"model_1/concatenate_1/concat:0.10\" 151.652906 \n",
      "   74.  \"model_1/concatenate_1/concat:0.16\" 151.148002 \n",
      "   75.  \"model_1/concatenate_1/concat:0.81\" 150.293693 \n",
      "   76.  \"model_1/concatenate_1/concat:0.59\" 149.730890 \n",
      "   77. \"model_1/concatenate_1/concat:0.109\" 148.600515 \n",
      "   78.  \"model_1/concatenate_1/concat:0.99\" 147.458217 \n",
      "   79.  \"model_1/concatenate_1/concat:0.38\" 146.063738 \n",
      "   80.  \"model_1/concatenate_1/concat:0.22\" 146.057141 \n",
      "   81. \"model_1/concatenate_1/concat:0.110\" 145.692639 \n",
      "   82. \"model_1/concatenate_1/concat:0.111\" 145.470904 \n",
      "   83.  \"model_1/concatenate_1/concat:0.17\" 142.464553 \n",
      "   84.  \"model_1/concatenate_1/concat:0.83\" 141.176394 \n",
      "   85.  \"model_1/concatenate_1/concat:0.86\" 139.759306 \n",
      "   86.  \"model_1/concatenate_1/concat:0.82\" 138.415702 \n",
      "   87.  \"model_1/concatenate_1/concat:0.42\" 136.690605 \n",
      "   88.  \"model_1/concatenate_1/concat:0.50\" 135.824315 \n",
      "   89.  \"model_1/concatenate_1/concat:0.32\" 135.793388 \n",
      "   90.  \"model_1/concatenate_1/concat:0.66\" 134.906680 \n",
      "   91.  \"model_1/concatenate_1/concat:0.89\" 133.996429 \n",
      "   92.  \"model_1/concatenate_1/concat:0.97\" 133.510477 \n",
      "   93.  \"model_1/concatenate_1/concat:0.48\" 129.822484 \n",
      "   94.  \"model_1/concatenate_1/concat:0.73\" 128.813665 \n",
      "   95.  \"model_1/concatenate_1/concat:0.67\" 128.284404 \n",
      "   96. \"model_1/concatenate_1/concat:0.114\" 124.773284 \n",
      "   97.  \"model_1/concatenate_1/concat:0.96\" 123.898755 \n",
      "   98.  \"model_1/concatenate_1/concat:0.55\" 123.289782 \n",
      "   99.  \"model_1/concatenate_1/concat:0.71\" 121.820136 \n",
      "  100.  \"model_1/concatenate_1/concat:0.93\" 119.288663 \n",
      "  101.  \"model_1/concatenate_1/concat:0.40\" 117.986440 \n",
      "  102. \"model_1/concatenate_1/concat:0.117\" 117.900859 \n",
      "  103.  \"model_1/concatenate_1/concat:0.19\" 116.478923 \n",
      "  104.   \"model_1/concatenate_1/concat:0.3\" 116.463360 \n",
      "  105.  \"model_1/concatenate_1/concat:0.28\" 115.836042 \n",
      "  106.  \"model_1/concatenate_1/concat:0.65\" 114.527038 \n",
      "  107.  \"model_1/concatenate_1/concat:0.87\" 112.615491 \n",
      "  108.  \"model_1/concatenate_1/concat:0.63\" 105.392267 \n",
      "  109.  \"model_1/concatenate_1/concat:0.90\" 105.382987 \n",
      "  110.  \"model_1/concatenate_1/concat:0.88\" 104.994160 \n",
      "  111.  \"model_1/concatenate_1/concat:0.57\" 102.454817 \n",
      "  112.  \"model_1/concatenate_1/concat:0.79\" 98.644482 \n",
      "  113.  \"model_1/concatenate_1/concat:0.58\" 95.906441 \n",
      "  114. \"model_1/concatenate_1/concat:0.115\" 95.172545 \n",
      "  115.  \"model_1/concatenate_1/concat:0.75\" 94.087906 \n",
      "  116.  \"model_1/concatenate_1/concat:0.84\" 93.057421 \n",
      "  117.  \"model_1/concatenate_1/concat:0.18\" 85.986021 \n",
      "  118.  \"model_1/concatenate_1/concat:0.62\" 83.866753 \n",
      "  119.  \"model_1/concatenate_1/concat:0.25\" 81.101558 \n",
      "  120. \"model_1/concatenate_1/concat:0.119\" 56.764225 \n",
      "  121. \"model_1/concatenate_1/concat:0.120\" 54.198414 \n",
      "\n",
      "\n",
      "\n",
      "Loss: BINOMIAL_LOG_LIKELIHOOD\n",
      "Validation loss value: 1.01604\n",
      "Number of trees per iteration: 1\n",
      "Node format: NOT_SET\n",
      "Number of trees: 279\n",
      "Total number of nodes: 16927\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 279 Average: 60.6703 StdDev: 2.90449\n",
      "Min: 23 Max: 61 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 23, 24)   1   0.36%   0.36%\n",
      "[ 24, 26)   0   0.00%   0.36%\n",
      "[ 26, 28)   0   0.00%   0.36%\n",
      "[ 28, 30)   0   0.00%   0.36%\n",
      "[ 30, 32)   0   0.00%   0.36%\n",
      "[ 32, 34)   0   0.00%   0.36%\n",
      "[ 34, 36)   1   0.36%   0.72%\n",
      "[ 36, 38)   0   0.00%   0.72%\n",
      "[ 38, 40)   0   0.00%   0.72%\n",
      "[ 40, 42)   0   0.00%   0.72%\n",
      "[ 42, 44)   0   0.00%   0.72%\n",
      "[ 44, 46)   0   0.00%   0.72%\n",
      "[ 46, 48)   0   0.00%   0.72%\n",
      "[ 48, 50)   1   0.36%   1.08%\n",
      "[ 50, 52)   1   0.36%   1.43%\n",
      "[ 52, 54)   0   0.00%   1.43%\n",
      "[ 54, 56)   0   0.00%   1.43%\n",
      "[ 56, 58)   1   0.36%   1.79%\n",
      "[ 58, 60)   1   0.36%   2.15%\n",
      "[ 60, 61] 273  97.85% 100.00% ##########\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 8603 Average: 5.41462 StdDev: 0.96841\n",
      "Min: 1 Max: 6 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 1, 2)    7   0.08%   0.08%\n",
      "[ 2, 3)  142   1.65%   1.73%\n",
      "[ 3, 4)  405   4.71%   6.44% #\n",
      "[ 4, 5)  853   9.92%  16.35% ##\n",
      "[ 5, 6) 1512  17.58%  33.93% ###\n",
      "[ 6, 6] 5684  66.07% 100.00% ##########\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 8603 Average: 0 StdDev: 0\n",
      "Min: 0 Max: 0 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 0, 0] 8603 100.00% 100.00% ##########\n",
      "\n",
      "Attribute in nodes:\n",
      "\t219 : model_1/concatenate_1/concat:0.39 [NUMERICAL]\n",
      "\t167 : model_1/concatenate_1/concat:0.118 [NUMERICAL]\n",
      "\t146 : model_1/concatenate_1/concat:0.69 [NUMERICAL]\n",
      "\t143 : model_1/concatenate_1/concat:0.46 [NUMERICAL]\n",
      "\t127 : model_1/concatenate_1/concat:0.44 [NUMERICAL]\n",
      "\t124 : model_1/concatenate_1/concat:0.20 [NUMERICAL]\n",
      "\t121 : model_1/concatenate_1/concat:0.33 [NUMERICAL]\n",
      "\t116 : model_1/concatenate_1/concat:0.34 [NUMERICAL]\n",
      "\t115 : model_1/concatenate_1/concat:0.0 [NUMERICAL]\n",
      "\t110 : model_1/concatenate_1/concat:0.56 [NUMERICAL]\n",
      "\t110 : model_1/concatenate_1/concat:0.51 [NUMERICAL]\n",
      "\t106 : model_1/concatenate_1/concat:0.76 [NUMERICAL]\n",
      "\t106 : model_1/concatenate_1/concat:0.64 [NUMERICAL]\n",
      "\t103 : model_1/concatenate_1/concat:0.7 [NUMERICAL]\n",
      "\t103 : model_1/concatenate_1/concat:0.106 [NUMERICAL]\n",
      "\t101 : model_1/concatenate_1/concat:0.27 [NUMERICAL]\n",
      "\t100 : model_1/concatenate_1/concat:0.95 [NUMERICAL]\n",
      "\t98 : model_1/concatenate_1/concat:0.61 [NUMERICAL]\n",
      "\t97 : model_1/concatenate_1/concat:0.94 [NUMERICAL]\n",
      "\t96 : model_1/concatenate_1/concat:0.5 [NUMERICAL]\n",
      "\t96 : model_1/concatenate_1/concat:0.2 [NUMERICAL]\n",
      "\t95 : model_1/concatenate_1/concat:0.4 [NUMERICAL]\n",
      "\t94 : model_1/concatenate_1/concat:0.105 [NUMERICAL]\n",
      "\t93 : model_1/concatenate_1/concat:0.6 [NUMERICAL]\n",
      "\t93 : model_1/concatenate_1/concat:0.30 [NUMERICAL]\n",
      "\t91 : model_1/concatenate_1/concat:0.101 [NUMERICAL]\n",
      "\t87 : model_1/concatenate_1/concat:0.31 [NUMERICAL]\n",
      "\t82 : model_1/concatenate_1/concat:0.91 [NUMERICAL]\n",
      "\t82 : model_1/concatenate_1/concat:0.35 [NUMERICAL]\n",
      "\t81 : model_1/concatenate_1/concat:0.77 [NUMERICAL]\n",
      "\t80 : model_1/concatenate_1/concat:0.1 [NUMERICAL]\n",
      "\t79 : model_1/concatenate_1/concat:0.60 [NUMERICAL]\n",
      "\t79 : model_1/concatenate_1/concat:0.12 [NUMERICAL]\n",
      "\t77 : model_1/concatenate_1/concat:0.85 [NUMERICAL]\n",
      "\t77 : model_1/concatenate_1/concat:0.8 [NUMERICAL]\n",
      "\t77 : model_1/concatenate_1/concat:0.47 [NUMERICAL]\n",
      "\t76 : model_1/concatenate_1/concat:0.37 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.98 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.70 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.26 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.14 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.78 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.24 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.23 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.15 [NUMERICAL]\n",
      "\t73 : model_1/concatenate_1/concat:0.36 [NUMERICAL]\n",
      "\t72 : model_1/concatenate_1/concat:0.43 [NUMERICAL]\n",
      "\t70 : model_1/concatenate_1/concat:0.41 [NUMERICAL]\n",
      "\t68 : model_1/concatenate_1/concat:0.52 [NUMERICAL]\n",
      "\t68 : model_1/concatenate_1/concat:0.49 [NUMERICAL]\n",
      "\t68 : model_1/concatenate_1/concat:0.108 [NUMERICAL]\n",
      "\t67 : model_1/concatenate_1/concat:0.113 [NUMERICAL]\n",
      "\t66 : model_1/concatenate_1/concat:0.74 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.9 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.29 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.107 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.54 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.53 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.21 [NUMERICAL]\n",
      "\t61 : model_1/concatenate_1/concat:0.92 [NUMERICAL]\n",
      "\t61 : model_1/concatenate_1/concat:0.11 [NUMERICAL]\n",
      "\t61 : model_1/concatenate_1/concat:0.104 [NUMERICAL]\n",
      "\t60 : model_1/concatenate_1/concat:0.72 [NUMERICAL]\n",
      "\t60 : model_1/concatenate_1/concat:0.103 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.99 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.16 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.112 [NUMERICAL]\n",
      "\t57 : model_1/concatenate_1/concat:0.116 [NUMERICAL]\n",
      "\t56 : model_1/concatenate_1/concat:0.68 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.80 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.45 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.111 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.102 [NUMERICAL]\n",
      "\t54 : model_1/concatenate_1/concat:0.81 [NUMERICAL]\n",
      "\t53 : model_1/concatenate_1/concat:0.83 [NUMERICAL]\n",
      "\t53 : model_1/concatenate_1/concat:0.17 [NUMERICAL]\n",
      "\t53 : model_1/concatenate_1/concat:0.10 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.82 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.59 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.50 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.42 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.22 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.109 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.100 [NUMERICAL]\n",
      "\t51 : model_1/concatenate_1/concat:0.32 [NUMERICAL]\n",
      "\t51 : model_1/concatenate_1/concat:0.13 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.86 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.66 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.38 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.110 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.89 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.48 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.114 [NUMERICAL]\n",
      "\t47 : model_1/concatenate_1/concat:0.67 [NUMERICAL]\n",
      "\t47 : model_1/concatenate_1/concat:0.28 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.97 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.55 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.40 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.71 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.65 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.19 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.93 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.73 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.117 [NUMERICAL]\n",
      "\t43 : model_1/concatenate_1/concat:0.87 [NUMERICAL]\n",
      "\t43 : model_1/concatenate_1/concat:0.3 [NUMERICAL]\n",
      "\t42 : model_1/concatenate_1/concat:0.96 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.90 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.75 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.63 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.57 [NUMERICAL]\n",
      "\t39 : model_1/concatenate_1/concat:0.58 [NUMERICAL]\n",
      "\t38 : model_1/concatenate_1/concat:0.88 [NUMERICAL]\n",
      "\t36 : model_1/concatenate_1/concat:0.84 [NUMERICAL]\n",
      "\t35 : model_1/concatenate_1/concat:0.79 [NUMERICAL]\n",
      "\t35 : model_1/concatenate_1/concat:0.115 [NUMERICAL]\n",
      "\t34 : model_1/concatenate_1/concat:0.62 [NUMERICAL]\n",
      "\t33 : model_1/concatenate_1/concat:0.25 [NUMERICAL]\n",
      "\t33 : model_1/concatenate_1/concat:0.18 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.119 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.120 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t49 : model_1/concatenate_1/concat:0.118 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.44 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.39 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.64 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.91 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.34 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.76 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.69 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.27 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.24 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.10 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.94 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.7 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.23 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.107 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.106 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.0 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.72 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.52 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.31 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.26 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.98 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.95 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.92 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.70 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.49 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.46 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.41 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.4 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.37 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.35 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.30 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.3 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.28 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.12 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.113 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.112 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.1 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.90 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.85 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.81 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.78 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.77 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.66 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.62 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.61 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.59 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.53 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.51 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.50 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.5 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.47 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.42 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.38 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.36 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.29 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.15 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.111 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.108 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.105 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.101 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.97 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.96 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.80 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.74 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.65 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.6 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.43 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.21 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.2 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.14 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.119 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.114 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.110 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.109 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t81 : model_1/concatenate_1/concat:0.118 [NUMERICAL]\n",
      "\t31 : model_1/concatenate_1/concat:0.39 [NUMERICAL]\n",
      "\t25 : model_1/concatenate_1/concat:0.44 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.69 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.34 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.64 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.76 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.46 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.0 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.106 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.24 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.94 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.27 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.95 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.5 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.105 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.91 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.33 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.23 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.10 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.56 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.51 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.4 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.36 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.31 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.20 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.7 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.61 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.38 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.28 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.12 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.113 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.107 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.85 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.80 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.77 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.70 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.97 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.8 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.78 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.62 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.59 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.30 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.14 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.111 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.98 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.96 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.72 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.52 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.50 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.49 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.43 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.37 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.26 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.15 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.114 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.93 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.81 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.74 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.6 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.42 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.3 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.2 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.108 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.1 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.92 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.90 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.87 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.66 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.63 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.55 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.53 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.47 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.41 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.35 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.29 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.21 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.16 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.112 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.86 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.79 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.65 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.60 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.54 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.45 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.110 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.11 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.109 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.103 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.102 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.101 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.99 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.71 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.68 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.67 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.57 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.40 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.25 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.17 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.120 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.116 [NUMERICAL]\n",
      "\t2 : model_1/concatenate_1/concat:0.100 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.9 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.89 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.88 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.84 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.82 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.75 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.58 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.48 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.22 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.19 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.13 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.119 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.117 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.115 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.104 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t90 : model_1/concatenate_1/concat:0.118 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.39 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.69 [NUMERICAL]\n",
      "\t43 : model_1/concatenate_1/concat:0.44 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.34 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.20 [NUMERICAL]\n",
      "\t37 : model_1/concatenate_1/concat:0.46 [NUMERICAL]\n",
      "\t36 : model_1/concatenate_1/concat:0.0 [NUMERICAL]\n",
      "\t35 : model_1/concatenate_1/concat:0.64 [NUMERICAL]\n",
      "\t32 : model_1/concatenate_1/concat:0.27 [NUMERICAL]\n",
      "\t29 : model_1/concatenate_1/concat:0.33 [NUMERICAL]\n",
      "\t28 : model_1/concatenate_1/concat:0.76 [NUMERICAL]\n",
      "\t27 : model_1/concatenate_1/concat:0.94 [NUMERICAL]\n",
      "\t27 : model_1/concatenate_1/concat:0.106 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.7 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.105 [NUMERICAL]\n",
      "\t25 : model_1/concatenate_1/concat:0.4 [NUMERICAL]\n",
      "\t23 : model_1/concatenate_1/concat:0.24 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.5 [NUMERICAL]\n",
      "\t21 : model_1/concatenate_1/concat:0.95 [NUMERICAL]\n",
      "\t21 : model_1/concatenate_1/concat:0.31 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.61 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.51 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.6 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.36 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.30 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.15 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.91 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.78 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.77 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.56 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.37 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.23 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.2 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.10 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.85 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.14 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.12 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.49 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.70 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.43 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.41 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.8 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.66 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.53 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.38 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.28 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.26 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.114 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.107 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.1 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.81 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.80 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.74 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.72 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.60 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.52 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.16 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.116 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.113 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.99 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.96 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.87 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.79 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.59 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.35 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.3 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.29 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.112 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.102 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.101 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.98 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.97 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.86 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.54 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.50 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.47 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.45 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.42 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.32 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.93 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.9 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.68 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.62 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.111 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.110 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.108 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.92 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.67 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.63 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.21 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.17 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.115 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.109 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.103 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.100 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.65 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.58 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.22 [NUMERICAL]\n",
      "\t7 : model_1/concatenate_1/concat:0.104 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.88 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.84 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.75 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.13 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.117 [NUMERICAL]\n",
      "\t6 : model_1/concatenate_1/concat:0.11 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.90 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.83 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.82 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.73 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.71 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.55 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.48 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.57 [NUMERICAL]\n",
      "\t4 : model_1/concatenate_1/concat:0.19 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.40 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.25 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.18 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.120 [NUMERICAL]\n",
      "\t3 : model_1/concatenate_1/concat:0.119 [NUMERICAL]\n",
      "\t1 : model_1/concatenate_1/concat:0.89 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t108 : model_1/concatenate_1/concat:0.39 [NUMERICAL]\n",
      "\t105 : model_1/concatenate_1/concat:0.118 [NUMERICAL]\n",
      "\t81 : model_1/concatenate_1/concat:0.69 [NUMERICAL]\n",
      "\t81 : model_1/concatenate_1/concat:0.46 [NUMERICAL]\n",
      "\t64 : model_1/concatenate_1/concat:0.44 [NUMERICAL]\n",
      "\t63 : model_1/concatenate_1/concat:0.0 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.34 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.20 [NUMERICAL]\n",
      "\t51 : model_1/concatenate_1/concat:0.51 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.64 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.33 [NUMERICAL]\n",
      "\t48 : model_1/concatenate_1/concat:0.4 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.94 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.7 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.106 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.27 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.76 [NUMERICAL]\n",
      "\t41 : model_1/concatenate_1/concat:0.2 [NUMERICAL]\n",
      "\t41 : model_1/concatenate_1/concat:0.105 [NUMERICAL]\n",
      "\t39 : model_1/concatenate_1/concat:0.30 [NUMERICAL]\n",
      "\t38 : model_1/concatenate_1/concat:0.31 [NUMERICAL]\n",
      "\t37 : model_1/concatenate_1/concat:0.95 [NUMERICAL]\n",
      "\t36 : model_1/concatenate_1/concat:0.6 [NUMERICAL]\n",
      "\t36 : model_1/concatenate_1/concat:0.101 [NUMERICAL]\n",
      "\t35 : model_1/concatenate_1/concat:0.56 [NUMERICAL]\n",
      "\t34 : model_1/concatenate_1/concat:0.78 [NUMERICAL]\n",
      "\t33 : model_1/concatenate_1/concat:0.5 [NUMERICAL]\n",
      "\t33 : model_1/concatenate_1/concat:0.49 [NUMERICAL]\n",
      "\t32 : model_1/concatenate_1/concat:0.91 [NUMERICAL]\n",
      "\t32 : model_1/concatenate_1/concat:0.61 [NUMERICAL]\n",
      "\t32 : model_1/concatenate_1/concat:0.24 [NUMERICAL]\n",
      "\t31 : model_1/concatenate_1/concat:0.36 [NUMERICAL]\n",
      "\t31 : model_1/concatenate_1/concat:0.23 [NUMERICAL]\n",
      "\t31 : model_1/concatenate_1/concat:0.14 [NUMERICAL]\n",
      "\t30 : model_1/concatenate_1/concat:0.70 [NUMERICAL]\n",
      "\t30 : model_1/concatenate_1/concat:0.60 [NUMERICAL]\n",
      "\t30 : model_1/concatenate_1/concat:0.15 [NUMERICAL]\n",
      "\t30 : model_1/concatenate_1/concat:0.12 [NUMERICAL]\n",
      "\t29 : model_1/concatenate_1/concat:0.77 [NUMERICAL]\n",
      "\t29 : model_1/concatenate_1/concat:0.43 [NUMERICAL]\n",
      "\t29 : model_1/concatenate_1/concat:0.26 [NUMERICAL]\n",
      "\t29 : model_1/concatenate_1/concat:0.10 [NUMERICAL]\n",
      "\t28 : model_1/concatenate_1/concat:0.112 [NUMERICAL]\n",
      "\t27 : model_1/concatenate_1/concat:0.8 [NUMERICAL]\n",
      "\t27 : model_1/concatenate_1/concat:0.52 [NUMERICAL]\n",
      "\t27 : model_1/concatenate_1/concat:0.35 [NUMERICAL]\n",
      "\t27 : model_1/concatenate_1/concat:0.113 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.85 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.53 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.47 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.38 [NUMERICAL]\n",
      "\t26 : model_1/concatenate_1/concat:0.37 [NUMERICAL]\n",
      "\t25 : model_1/concatenate_1/concat:0.16 [NUMERICAL]\n",
      "\t25 : model_1/concatenate_1/concat:0.107 [NUMERICAL]\n",
      "\t24 : model_1/concatenate_1/concat:0.41 [NUMERICAL]\n",
      "\t24 : model_1/concatenate_1/concat:0.21 [NUMERICAL]\n",
      "\t24 : model_1/concatenate_1/concat:0.108 [NUMERICAL]\n",
      "\t24 : model_1/concatenate_1/concat:0.102 [NUMERICAL]\n",
      "\t23 : model_1/concatenate_1/concat:0.98 [NUMERICAL]\n",
      "\t23 : model_1/concatenate_1/concat:0.97 [NUMERICAL]\n",
      "\t23 : model_1/concatenate_1/concat:0.116 [NUMERICAL]\n",
      "\t23 : model_1/concatenate_1/concat:0.11 [NUMERICAL]\n",
      "\t23 : model_1/concatenate_1/concat:0.1 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.9 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.54 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.28 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.111 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.110 [NUMERICAL]\n",
      "\t21 : model_1/concatenate_1/concat:0.99 [NUMERICAL]\n",
      "\t21 : model_1/concatenate_1/concat:0.72 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.81 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.32 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.29 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.92 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.74 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.68 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.45 [NUMERICAL]\n",
      "\t19 : model_1/concatenate_1/concat:0.114 [NUMERICAL]\n",
      "\t18 : model_1/concatenate_1/concat:0.86 [NUMERICAL]\n",
      "\t18 : model_1/concatenate_1/concat:0.66 [NUMERICAL]\n",
      "\t18 : model_1/concatenate_1/concat:0.59 [NUMERICAL]\n",
      "\t18 : model_1/concatenate_1/concat:0.50 [NUMERICAL]\n",
      "\t18 : model_1/concatenate_1/concat:0.42 [NUMERICAL]\n",
      "\t18 : model_1/concatenate_1/concat:0.109 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.87 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.48 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.104 [NUMERICAL]\n",
      "\t17 : model_1/concatenate_1/concat:0.103 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.75 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.62 [NUMERICAL]\n",
      "\t16 : model_1/concatenate_1/concat:0.17 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.93 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.83 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.80 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.79 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.71 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.67 [NUMERICAL]\n",
      "\t15 : model_1/concatenate_1/concat:0.3 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.82 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.63 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.58 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.57 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.40 [NUMERICAL]\n",
      "\t14 : model_1/concatenate_1/concat:0.13 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.96 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.55 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.117 [NUMERICAL]\n",
      "\t13 : model_1/concatenate_1/concat:0.115 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.90 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.88 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.65 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.22 [NUMERICAL]\n",
      "\t12 : model_1/concatenate_1/concat:0.18 [NUMERICAL]\n",
      "\t11 : model_1/concatenate_1/concat:0.19 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.84 [NUMERICAL]\n",
      "\t10 : model_1/concatenate_1/concat:0.73 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.89 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.119 [NUMERICAL]\n",
      "\t9 : model_1/concatenate_1/concat:0.100 [NUMERICAL]\n",
      "\t8 : model_1/concatenate_1/concat:0.25 [NUMERICAL]\n",
      "\t5 : model_1/concatenate_1/concat:0.120 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t219 : model_1/concatenate_1/concat:0.39 [NUMERICAL]\n",
      "\t167 : model_1/concatenate_1/concat:0.118 [NUMERICAL]\n",
      "\t146 : model_1/concatenate_1/concat:0.69 [NUMERICAL]\n",
      "\t143 : model_1/concatenate_1/concat:0.46 [NUMERICAL]\n",
      "\t127 : model_1/concatenate_1/concat:0.44 [NUMERICAL]\n",
      "\t124 : model_1/concatenate_1/concat:0.20 [NUMERICAL]\n",
      "\t121 : model_1/concatenate_1/concat:0.33 [NUMERICAL]\n",
      "\t116 : model_1/concatenate_1/concat:0.34 [NUMERICAL]\n",
      "\t115 : model_1/concatenate_1/concat:0.0 [NUMERICAL]\n",
      "\t110 : model_1/concatenate_1/concat:0.56 [NUMERICAL]\n",
      "\t110 : model_1/concatenate_1/concat:0.51 [NUMERICAL]\n",
      "\t106 : model_1/concatenate_1/concat:0.76 [NUMERICAL]\n",
      "\t106 : model_1/concatenate_1/concat:0.64 [NUMERICAL]\n",
      "\t103 : model_1/concatenate_1/concat:0.7 [NUMERICAL]\n",
      "\t103 : model_1/concatenate_1/concat:0.106 [NUMERICAL]\n",
      "\t101 : model_1/concatenate_1/concat:0.27 [NUMERICAL]\n",
      "\t100 : model_1/concatenate_1/concat:0.95 [NUMERICAL]\n",
      "\t98 : model_1/concatenate_1/concat:0.61 [NUMERICAL]\n",
      "\t97 : model_1/concatenate_1/concat:0.94 [NUMERICAL]\n",
      "\t96 : model_1/concatenate_1/concat:0.5 [NUMERICAL]\n",
      "\t96 : model_1/concatenate_1/concat:0.2 [NUMERICAL]\n",
      "\t95 : model_1/concatenate_1/concat:0.4 [NUMERICAL]\n",
      "\t94 : model_1/concatenate_1/concat:0.105 [NUMERICAL]\n",
      "\t93 : model_1/concatenate_1/concat:0.6 [NUMERICAL]\n",
      "\t93 : model_1/concatenate_1/concat:0.30 [NUMERICAL]\n",
      "\t91 : model_1/concatenate_1/concat:0.101 [NUMERICAL]\n",
      "\t87 : model_1/concatenate_1/concat:0.31 [NUMERICAL]\n",
      "\t82 : model_1/concatenate_1/concat:0.91 [NUMERICAL]\n",
      "\t82 : model_1/concatenate_1/concat:0.35 [NUMERICAL]\n",
      "\t81 : model_1/concatenate_1/concat:0.77 [NUMERICAL]\n",
      "\t80 : model_1/concatenate_1/concat:0.1 [NUMERICAL]\n",
      "\t79 : model_1/concatenate_1/concat:0.60 [NUMERICAL]\n",
      "\t79 : model_1/concatenate_1/concat:0.12 [NUMERICAL]\n",
      "\t77 : model_1/concatenate_1/concat:0.85 [NUMERICAL]\n",
      "\t77 : model_1/concatenate_1/concat:0.8 [NUMERICAL]\n",
      "\t77 : model_1/concatenate_1/concat:0.47 [NUMERICAL]\n",
      "\t76 : model_1/concatenate_1/concat:0.37 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.98 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.70 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.26 [NUMERICAL]\n",
      "\t75 : model_1/concatenate_1/concat:0.14 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.78 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.24 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.23 [NUMERICAL]\n",
      "\t74 : model_1/concatenate_1/concat:0.15 [NUMERICAL]\n",
      "\t73 : model_1/concatenate_1/concat:0.36 [NUMERICAL]\n",
      "\t72 : model_1/concatenate_1/concat:0.43 [NUMERICAL]\n",
      "\t70 : model_1/concatenate_1/concat:0.41 [NUMERICAL]\n",
      "\t68 : model_1/concatenate_1/concat:0.52 [NUMERICAL]\n",
      "\t68 : model_1/concatenate_1/concat:0.49 [NUMERICAL]\n",
      "\t68 : model_1/concatenate_1/concat:0.108 [NUMERICAL]\n",
      "\t67 : model_1/concatenate_1/concat:0.113 [NUMERICAL]\n",
      "\t66 : model_1/concatenate_1/concat:0.74 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.9 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.29 [NUMERICAL]\n",
      "\t65 : model_1/concatenate_1/concat:0.107 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.54 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.53 [NUMERICAL]\n",
      "\t62 : model_1/concatenate_1/concat:0.21 [NUMERICAL]\n",
      "\t61 : model_1/concatenate_1/concat:0.92 [NUMERICAL]\n",
      "\t61 : model_1/concatenate_1/concat:0.11 [NUMERICAL]\n",
      "\t61 : model_1/concatenate_1/concat:0.104 [NUMERICAL]\n",
      "\t60 : model_1/concatenate_1/concat:0.72 [NUMERICAL]\n",
      "\t60 : model_1/concatenate_1/concat:0.103 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.99 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.16 [NUMERICAL]\n",
      "\t58 : model_1/concatenate_1/concat:0.112 [NUMERICAL]\n",
      "\t57 : model_1/concatenate_1/concat:0.116 [NUMERICAL]\n",
      "\t56 : model_1/concatenate_1/concat:0.68 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.80 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.45 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.111 [NUMERICAL]\n",
      "\t55 : model_1/concatenate_1/concat:0.102 [NUMERICAL]\n",
      "\t54 : model_1/concatenate_1/concat:0.81 [NUMERICAL]\n",
      "\t53 : model_1/concatenate_1/concat:0.83 [NUMERICAL]\n",
      "\t53 : model_1/concatenate_1/concat:0.17 [NUMERICAL]\n",
      "\t53 : model_1/concatenate_1/concat:0.10 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.82 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.59 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.50 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.42 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.22 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.109 [NUMERICAL]\n",
      "\t52 : model_1/concatenate_1/concat:0.100 [NUMERICAL]\n",
      "\t51 : model_1/concatenate_1/concat:0.32 [NUMERICAL]\n",
      "\t51 : model_1/concatenate_1/concat:0.13 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.86 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.66 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.38 [NUMERICAL]\n",
      "\t50 : model_1/concatenate_1/concat:0.110 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.89 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.48 [NUMERICAL]\n",
      "\t49 : model_1/concatenate_1/concat:0.114 [NUMERICAL]\n",
      "\t47 : model_1/concatenate_1/concat:0.67 [NUMERICAL]\n",
      "\t47 : model_1/concatenate_1/concat:0.28 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.97 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.55 [NUMERICAL]\n",
      "\t46 : model_1/concatenate_1/concat:0.40 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.71 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.65 [NUMERICAL]\n",
      "\t45 : model_1/concatenate_1/concat:0.19 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.93 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.73 [NUMERICAL]\n",
      "\t44 : model_1/concatenate_1/concat:0.117 [NUMERICAL]\n",
      "\t43 : model_1/concatenate_1/concat:0.87 [NUMERICAL]\n",
      "\t43 : model_1/concatenate_1/concat:0.3 [NUMERICAL]\n",
      "\t42 : model_1/concatenate_1/concat:0.96 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.90 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.75 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.63 [NUMERICAL]\n",
      "\t40 : model_1/concatenate_1/concat:0.57 [NUMERICAL]\n",
      "\t39 : model_1/concatenate_1/concat:0.58 [NUMERICAL]\n",
      "\t38 : model_1/concatenate_1/concat:0.88 [NUMERICAL]\n",
      "\t36 : model_1/concatenate_1/concat:0.84 [NUMERICAL]\n",
      "\t35 : model_1/concatenate_1/concat:0.79 [NUMERICAL]\n",
      "\t35 : model_1/concatenate_1/concat:0.115 [NUMERICAL]\n",
      "\t34 : model_1/concatenate_1/concat:0.62 [NUMERICAL]\n",
      "\t33 : model_1/concatenate_1/concat:0.25 [NUMERICAL]\n",
      "\t33 : model_1/concatenate_1/concat:0.18 [NUMERICAL]\n",
      "\t22 : model_1/concatenate_1/concat:0.119 [NUMERICAL]\n",
      "\t20 : model_1/concatenate_1/concat:0.120 [NUMERICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t8324 : HigherCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t279 : HigherCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t830 : HigherCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t1790 : HigherCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t3305 : HigherCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t8324 : HigherCondition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "GIYEMvdQh_UM"
   },
   "outputs": [],
   "source": [
    "inspector = model_2.make_inspector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBP2GtIQh_UM",
    "outputId": "4e289500-cb71-4639-dedb-ee48cf5f758e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 279 trees\n"
     ]
    }
   ],
   "source": [
    "print(\"Model contains {} trees\".format(inspector.num_trees()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "8FpwTn60h_UN",
    "outputId": "c2795e13-7b60-4fb2-8fcc-098ec026384c"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"model_1/concatenate_1/concat:0.0\" (1; #0),\n",
       " \"model_1/concatenate_1/concat:0.1\" (1; #1),\n",
       " \"model_1/concatenate_1/concat:0.10\" (1; #2),\n",
       " \"model_1/concatenate_1/concat:0.100\" (1; #3),\n",
       " \"model_1/concatenate_1/concat:0.101\" (1; #4),\n",
       " \"model_1/concatenate_1/concat:0.102\" (1; #5),\n",
       " \"model_1/concatenate_1/concat:0.103\" (1; #6),\n",
       " \"model_1/concatenate_1/concat:0.104\" (1; #7),\n",
       " \"model_1/concatenate_1/concat:0.105\" (1; #8),\n",
       " \"model_1/concatenate_1/concat:0.106\" (1; #9),\n",
       " \"model_1/concatenate_1/concat:0.107\" (1; #10),\n",
       " \"model_1/concatenate_1/concat:0.108\" (1; #11),\n",
       " \"model_1/concatenate_1/concat:0.109\" (1; #12),\n",
       " \"model_1/concatenate_1/concat:0.11\" (1; #13),\n",
       " \"model_1/concatenate_1/concat:0.110\" (1; #14),\n",
       " \"model_1/concatenate_1/concat:0.111\" (1; #15),\n",
       " \"model_1/concatenate_1/concat:0.112\" (1; #16),\n",
       " \"model_1/concatenate_1/concat:0.113\" (1; #17),\n",
       " \"model_1/concatenate_1/concat:0.114\" (1; #18),\n",
       " \"model_1/concatenate_1/concat:0.115\" (1; #19),\n",
       " \"model_1/concatenate_1/concat:0.116\" (1; #20),\n",
       " \"model_1/concatenate_1/concat:0.117\" (1; #21),\n",
       " \"model_1/concatenate_1/concat:0.118\" (1; #22),\n",
       " \"model_1/concatenate_1/concat:0.119\" (1; #23),\n",
       " \"model_1/concatenate_1/concat:0.12\" (1; #24),\n",
       " \"model_1/concatenate_1/concat:0.120\" (1; #25),\n",
       " \"model_1/concatenate_1/concat:0.13\" (1; #26),\n",
       " \"model_1/concatenate_1/concat:0.14\" (1; #27),\n",
       " \"model_1/concatenate_1/concat:0.15\" (1; #28),\n",
       " \"model_1/concatenate_1/concat:0.16\" (1; #29),\n",
       " \"model_1/concatenate_1/concat:0.17\" (1; #30),\n",
       " \"model_1/concatenate_1/concat:0.18\" (1; #31),\n",
       " \"model_1/concatenate_1/concat:0.19\" (1; #32),\n",
       " \"model_1/concatenate_1/concat:0.2\" (1; #33),\n",
       " \"model_1/concatenate_1/concat:0.20\" (1; #34),\n",
       " \"model_1/concatenate_1/concat:0.21\" (1; #35),\n",
       " \"model_1/concatenate_1/concat:0.22\" (1; #36),\n",
       " \"model_1/concatenate_1/concat:0.23\" (1; #37),\n",
       " \"model_1/concatenate_1/concat:0.24\" (1; #38),\n",
       " \"model_1/concatenate_1/concat:0.25\" (1; #39),\n",
       " \"model_1/concatenate_1/concat:0.26\" (1; #40),\n",
       " \"model_1/concatenate_1/concat:0.27\" (1; #41),\n",
       " \"model_1/concatenate_1/concat:0.28\" (1; #42),\n",
       " \"model_1/concatenate_1/concat:0.29\" (1; #43),\n",
       " \"model_1/concatenate_1/concat:0.3\" (1; #44),\n",
       " \"model_1/concatenate_1/concat:0.30\" (1; #45),\n",
       " \"model_1/concatenate_1/concat:0.31\" (1; #46),\n",
       " \"model_1/concatenate_1/concat:0.32\" (1; #47),\n",
       " \"model_1/concatenate_1/concat:0.33\" (1; #48),\n",
       " \"model_1/concatenate_1/concat:0.34\" (1; #49),\n",
       " \"model_1/concatenate_1/concat:0.35\" (1; #50),\n",
       " \"model_1/concatenate_1/concat:0.36\" (1; #51),\n",
       " \"model_1/concatenate_1/concat:0.37\" (1; #52),\n",
       " \"model_1/concatenate_1/concat:0.38\" (1; #53),\n",
       " \"model_1/concatenate_1/concat:0.39\" (1; #54),\n",
       " \"model_1/concatenate_1/concat:0.4\" (1; #55),\n",
       " \"model_1/concatenate_1/concat:0.40\" (1; #56),\n",
       " \"model_1/concatenate_1/concat:0.41\" (1; #57),\n",
       " \"model_1/concatenate_1/concat:0.42\" (1; #58),\n",
       " \"model_1/concatenate_1/concat:0.43\" (1; #59),\n",
       " \"model_1/concatenate_1/concat:0.44\" (1; #60),\n",
       " \"model_1/concatenate_1/concat:0.45\" (1; #61),\n",
       " \"model_1/concatenate_1/concat:0.46\" (1; #62),\n",
       " \"model_1/concatenate_1/concat:0.47\" (1; #63),\n",
       " \"model_1/concatenate_1/concat:0.48\" (1; #64),\n",
       " \"model_1/concatenate_1/concat:0.49\" (1; #65),\n",
       " \"model_1/concatenate_1/concat:0.5\" (1; #66),\n",
       " \"model_1/concatenate_1/concat:0.50\" (1; #67),\n",
       " \"model_1/concatenate_1/concat:0.51\" (1; #68),\n",
       " \"model_1/concatenate_1/concat:0.52\" (1; #69),\n",
       " \"model_1/concatenate_1/concat:0.53\" (1; #70),\n",
       " \"model_1/concatenate_1/concat:0.54\" (1; #71),\n",
       " \"model_1/concatenate_1/concat:0.55\" (1; #72),\n",
       " \"model_1/concatenate_1/concat:0.56\" (1; #73),\n",
       " \"model_1/concatenate_1/concat:0.57\" (1; #74),\n",
       " \"model_1/concatenate_1/concat:0.58\" (1; #75),\n",
       " \"model_1/concatenate_1/concat:0.59\" (1; #76),\n",
       " \"model_1/concatenate_1/concat:0.6\" (1; #77),\n",
       " \"model_1/concatenate_1/concat:0.60\" (1; #78),\n",
       " \"model_1/concatenate_1/concat:0.61\" (1; #79),\n",
       " \"model_1/concatenate_1/concat:0.62\" (1; #80),\n",
       " \"model_1/concatenate_1/concat:0.63\" (1; #81),\n",
       " \"model_1/concatenate_1/concat:0.64\" (1; #82),\n",
       " \"model_1/concatenate_1/concat:0.65\" (1; #83),\n",
       " \"model_1/concatenate_1/concat:0.66\" (1; #84),\n",
       " \"model_1/concatenate_1/concat:0.67\" (1; #85),\n",
       " \"model_1/concatenate_1/concat:0.68\" (1; #86),\n",
       " \"model_1/concatenate_1/concat:0.69\" (1; #87),\n",
       " \"model_1/concatenate_1/concat:0.7\" (1; #88),\n",
       " \"model_1/concatenate_1/concat:0.70\" (1; #89),\n",
       " \"model_1/concatenate_1/concat:0.71\" (1; #90),\n",
       " \"model_1/concatenate_1/concat:0.72\" (1; #91),\n",
       " \"model_1/concatenate_1/concat:0.73\" (1; #92),\n",
       " \"model_1/concatenate_1/concat:0.74\" (1; #93),\n",
       " \"model_1/concatenate_1/concat:0.75\" (1; #94),\n",
       " \"model_1/concatenate_1/concat:0.76\" (1; #95),\n",
       " \"model_1/concatenate_1/concat:0.77\" (1; #96),\n",
       " \"model_1/concatenate_1/concat:0.78\" (1; #97),\n",
       " \"model_1/concatenate_1/concat:0.79\" (1; #98),\n",
       " \"model_1/concatenate_1/concat:0.8\" (1; #99),\n",
       " \"model_1/concatenate_1/concat:0.80\" (1; #100),\n",
       " \"model_1/concatenate_1/concat:0.81\" (1; #101),\n",
       " \"model_1/concatenate_1/concat:0.82\" (1; #102),\n",
       " \"model_1/concatenate_1/concat:0.83\" (1; #103),\n",
       " \"model_1/concatenate_1/concat:0.84\" (1; #104),\n",
       " \"model_1/concatenate_1/concat:0.85\" (1; #105),\n",
       " \"model_1/concatenate_1/concat:0.86\" (1; #106),\n",
       " \"model_1/concatenate_1/concat:0.87\" (1; #107),\n",
       " \"model_1/concatenate_1/concat:0.88\" (1; #108),\n",
       " \"model_1/concatenate_1/concat:0.89\" (1; #109),\n",
       " \"model_1/concatenate_1/concat:0.9\" (1; #110),\n",
       " \"model_1/concatenate_1/concat:0.90\" (1; #111),\n",
       " \"model_1/concatenate_1/concat:0.91\" (1; #112),\n",
       " \"model_1/concatenate_1/concat:0.92\" (1; #113),\n",
       " \"model_1/concatenate_1/concat:0.93\" (1; #114),\n",
       " \"model_1/concatenate_1/concat:0.94\" (1; #115),\n",
       " \"model_1/concatenate_1/concat:0.95\" (1; #116),\n",
       " \"model_1/concatenate_1/concat:0.96\" (1; #117),\n",
       " \"model_1/concatenate_1/concat:0.97\" (1; #118),\n",
       " \"model_1/concatenate_1/concat:0.98\" (1; #119),\n",
       " \"model_1/concatenate_1/concat:0.99\" (1; #120)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "inspector.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "gWI2fQa0h_UN",
    "outputId": "31497b4d-370d-4fc9-f803-223fcb86bc39"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'NUM_NODES': [(\"model_1/concatenate_1/concat:0.39\" (1; #54), 219.0),\n",
       "  (\"model_1/concatenate_1/concat:0.118\" (1; #22), 167.0),\n",
       "  (\"model_1/concatenate_1/concat:0.69\" (1; #87), 146.0),\n",
       "  (\"model_1/concatenate_1/concat:0.46\" (1; #62), 143.0),\n",
       "  (\"model_1/concatenate_1/concat:0.44\" (1; #60), 127.0),\n",
       "  (\"model_1/concatenate_1/concat:0.20\" (1; #34), 124.0),\n",
       "  (\"model_1/concatenate_1/concat:0.33\" (1; #48), 121.0),\n",
       "  (\"model_1/concatenate_1/concat:0.34\" (1; #49), 116.0),\n",
       "  (\"model_1/concatenate_1/concat:0.0\" (1; #0), 115.0),\n",
       "  (\"model_1/concatenate_1/concat:0.51\" (1; #68), 110.0),\n",
       "  (\"model_1/concatenate_1/concat:0.56\" (1; #73), 110.0),\n",
       "  (\"model_1/concatenate_1/concat:0.64\" (1; #82), 106.0),\n",
       "  (\"model_1/concatenate_1/concat:0.76\" (1; #95), 106.0),\n",
       "  (\"model_1/concatenate_1/concat:0.106\" (1; #9), 103.0),\n",
       "  (\"model_1/concatenate_1/concat:0.7\" (1; #88), 103.0),\n",
       "  (\"model_1/concatenate_1/concat:0.27\" (1; #41), 101.0),\n",
       "  (\"model_1/concatenate_1/concat:0.95\" (1; #116), 100.0),\n",
       "  (\"model_1/concatenate_1/concat:0.61\" (1; #79), 98.0),\n",
       "  (\"model_1/concatenate_1/concat:0.94\" (1; #115), 97.0),\n",
       "  (\"model_1/concatenate_1/concat:0.2\" (1; #33), 96.0),\n",
       "  (\"model_1/concatenate_1/concat:0.5\" (1; #66), 96.0),\n",
       "  (\"model_1/concatenate_1/concat:0.4\" (1; #55), 95.0),\n",
       "  (\"model_1/concatenate_1/concat:0.105\" (1; #8), 94.0),\n",
       "  (\"model_1/concatenate_1/concat:0.30\" (1; #45), 93.0),\n",
       "  (\"model_1/concatenate_1/concat:0.6\" (1; #77), 93.0),\n",
       "  (\"model_1/concatenate_1/concat:0.101\" (1; #4), 91.0),\n",
       "  (\"model_1/concatenate_1/concat:0.31\" (1; #46), 87.0),\n",
       "  (\"model_1/concatenate_1/concat:0.35\" (1; #50), 82.0),\n",
       "  (\"model_1/concatenate_1/concat:0.91\" (1; #112), 82.0),\n",
       "  (\"model_1/concatenate_1/concat:0.77\" (1; #96), 81.0),\n",
       "  (\"model_1/concatenate_1/concat:0.1\" (1; #1), 80.0),\n",
       "  (\"model_1/concatenate_1/concat:0.12\" (1; #24), 79.0),\n",
       "  (\"model_1/concatenate_1/concat:0.60\" (1; #78), 79.0),\n",
       "  (\"model_1/concatenate_1/concat:0.47\" (1; #63), 77.0),\n",
       "  (\"model_1/concatenate_1/concat:0.8\" (1; #99), 77.0),\n",
       "  (\"model_1/concatenate_1/concat:0.85\" (1; #105), 77.0),\n",
       "  (\"model_1/concatenate_1/concat:0.37\" (1; #52), 76.0),\n",
       "  (\"model_1/concatenate_1/concat:0.14\" (1; #27), 75.0),\n",
       "  (\"model_1/concatenate_1/concat:0.26\" (1; #40), 75.0),\n",
       "  (\"model_1/concatenate_1/concat:0.70\" (1; #89), 75.0),\n",
       "  (\"model_1/concatenate_1/concat:0.98\" (1; #119), 75.0),\n",
       "  (\"model_1/concatenate_1/concat:0.15\" (1; #28), 74.0),\n",
       "  (\"model_1/concatenate_1/concat:0.23\" (1; #37), 74.0),\n",
       "  (\"model_1/concatenate_1/concat:0.24\" (1; #38), 74.0),\n",
       "  (\"model_1/concatenate_1/concat:0.78\" (1; #97), 74.0),\n",
       "  (\"model_1/concatenate_1/concat:0.36\" (1; #51), 73.0),\n",
       "  (\"model_1/concatenate_1/concat:0.43\" (1; #59), 72.0),\n",
       "  (\"model_1/concatenate_1/concat:0.41\" (1; #57), 70.0),\n",
       "  (\"model_1/concatenate_1/concat:0.108\" (1; #11), 68.0),\n",
       "  (\"model_1/concatenate_1/concat:0.49\" (1; #65), 68.0),\n",
       "  (\"model_1/concatenate_1/concat:0.52\" (1; #69), 68.0),\n",
       "  (\"model_1/concatenate_1/concat:0.113\" (1; #17), 67.0),\n",
       "  (\"model_1/concatenate_1/concat:0.74\" (1; #93), 66.0),\n",
       "  (\"model_1/concatenate_1/concat:0.107\" (1; #10), 65.0),\n",
       "  (\"model_1/concatenate_1/concat:0.29\" (1; #43), 65.0),\n",
       "  (\"model_1/concatenate_1/concat:0.9\" (1; #110), 65.0),\n",
       "  (\"model_1/concatenate_1/concat:0.21\" (1; #35), 62.0),\n",
       "  (\"model_1/concatenate_1/concat:0.53\" (1; #70), 62.0),\n",
       "  (\"model_1/concatenate_1/concat:0.54\" (1; #71), 62.0),\n",
       "  (\"model_1/concatenate_1/concat:0.104\" (1; #7), 61.0),\n",
       "  (\"model_1/concatenate_1/concat:0.11\" (1; #13), 61.0),\n",
       "  (\"model_1/concatenate_1/concat:0.92\" (1; #113), 61.0),\n",
       "  (\"model_1/concatenate_1/concat:0.103\" (1; #6), 60.0),\n",
       "  (\"model_1/concatenate_1/concat:0.72\" (1; #91), 60.0),\n",
       "  (\"model_1/concatenate_1/concat:0.112\" (1; #16), 58.0),\n",
       "  (\"model_1/concatenate_1/concat:0.16\" (1; #29), 58.0),\n",
       "  (\"model_1/concatenate_1/concat:0.99\" (1; #120), 58.0),\n",
       "  (\"model_1/concatenate_1/concat:0.116\" (1; #20), 57.0),\n",
       "  (\"model_1/concatenate_1/concat:0.68\" (1; #86), 56.0),\n",
       "  (\"model_1/concatenate_1/concat:0.102\" (1; #5), 55.0),\n",
       "  (\"model_1/concatenate_1/concat:0.111\" (1; #15), 55.0),\n",
       "  (\"model_1/concatenate_1/concat:0.45\" (1; #61), 55.0),\n",
       "  (\"model_1/concatenate_1/concat:0.80\" (1; #100), 55.0),\n",
       "  (\"model_1/concatenate_1/concat:0.81\" (1; #101), 54.0),\n",
       "  (\"model_1/concatenate_1/concat:0.10\" (1; #2), 53.0),\n",
       "  (\"model_1/concatenate_1/concat:0.17\" (1; #30), 53.0),\n",
       "  (\"model_1/concatenate_1/concat:0.83\" (1; #103), 53.0),\n",
       "  (\"model_1/concatenate_1/concat:0.100\" (1; #3), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.109\" (1; #12), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.22\" (1; #36), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.42\" (1; #58), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.50\" (1; #67), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.59\" (1; #76), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.82\" (1; #102), 52.0),\n",
       "  (\"model_1/concatenate_1/concat:0.13\" (1; #26), 51.0),\n",
       "  (\"model_1/concatenate_1/concat:0.32\" (1; #47), 51.0),\n",
       "  (\"model_1/concatenate_1/concat:0.110\" (1; #14), 50.0),\n",
       "  (\"model_1/concatenate_1/concat:0.38\" (1; #53), 50.0),\n",
       "  (\"model_1/concatenate_1/concat:0.66\" (1; #84), 50.0),\n",
       "  (\"model_1/concatenate_1/concat:0.86\" (1; #106), 50.0),\n",
       "  (\"model_1/concatenate_1/concat:0.114\" (1; #18), 49.0),\n",
       "  (\"model_1/concatenate_1/concat:0.48\" (1; #64), 49.0),\n",
       "  (\"model_1/concatenate_1/concat:0.89\" (1; #109), 49.0),\n",
       "  (\"model_1/concatenate_1/concat:0.28\" (1; #42), 47.0),\n",
       "  (\"model_1/concatenate_1/concat:0.67\" (1; #85), 47.0),\n",
       "  (\"model_1/concatenate_1/concat:0.40\" (1; #56), 46.0),\n",
       "  (\"model_1/concatenate_1/concat:0.55\" (1; #72), 46.0),\n",
       "  (\"model_1/concatenate_1/concat:0.97\" (1; #118), 46.0),\n",
       "  (\"model_1/concatenate_1/concat:0.19\" (1; #32), 45.0),\n",
       "  (\"model_1/concatenate_1/concat:0.65\" (1; #83), 45.0),\n",
       "  (\"model_1/concatenate_1/concat:0.71\" (1; #90), 45.0),\n",
       "  (\"model_1/concatenate_1/concat:0.117\" (1; #21), 44.0),\n",
       "  (\"model_1/concatenate_1/concat:0.73\" (1; #92), 44.0),\n",
       "  (\"model_1/concatenate_1/concat:0.93\" (1; #114), 44.0),\n",
       "  (\"model_1/concatenate_1/concat:0.3\" (1; #44), 43.0),\n",
       "  (\"model_1/concatenate_1/concat:0.87\" (1; #107), 43.0),\n",
       "  (\"model_1/concatenate_1/concat:0.96\" (1; #117), 42.0),\n",
       "  (\"model_1/concatenate_1/concat:0.57\" (1; #74), 40.0),\n",
       "  (\"model_1/concatenate_1/concat:0.63\" (1; #81), 40.0),\n",
       "  (\"model_1/concatenate_1/concat:0.75\" (1; #94), 40.0),\n",
       "  (\"model_1/concatenate_1/concat:0.90\" (1; #111), 40.0),\n",
       "  (\"model_1/concatenate_1/concat:0.58\" (1; #75), 39.0),\n",
       "  (\"model_1/concatenate_1/concat:0.88\" (1; #108), 38.0),\n",
       "  (\"model_1/concatenate_1/concat:0.84\" (1; #104), 36.0),\n",
       "  (\"model_1/concatenate_1/concat:0.115\" (1; #19), 35.0),\n",
       "  (\"model_1/concatenate_1/concat:0.79\" (1; #98), 35.0),\n",
       "  (\"model_1/concatenate_1/concat:0.62\" (1; #80), 34.0),\n",
       "  (\"model_1/concatenate_1/concat:0.18\" (1; #31), 33.0),\n",
       "  (\"model_1/concatenate_1/concat:0.25\" (1; #39), 33.0),\n",
       "  (\"model_1/concatenate_1/concat:0.119\" (1; #23), 22.0),\n",
       "  (\"model_1/concatenate_1/concat:0.120\" (1; #25), 20.0)],\n",
       " 'MEAN_MIN_DEPTH': [(\"__LABEL\" (4; #121), 5.413765476227352),\n",
       "  (\"model_1/concatenate_1/concat:0.120\" (1; #25), 5.388329009583811),\n",
       "  (\"model_1/concatenate_1/concat:0.25\" (1; #39), 5.386594705039931),\n",
       "  (\"model_1/concatenate_1/concat:0.18\" (1; #31), 5.38590098322238),\n",
       "  (\"model_1/concatenate_1/concat:0.40\" (1; #56), 5.38301047564925),\n",
       "  (\"model_1/concatenate_1/concat:0.73\" (1; #92), 5.381425192829142),\n",
       "  (\"model_1/concatenate_1/concat:0.19\" (1; #32), 5.380434933728917),\n",
       "  (\"model_1/concatenate_1/concat:0.89\" (1; #109), 5.380422359637618),\n",
       "  (\"model_1/concatenate_1/concat:0.84\" (1; #104), 5.379888727470272),\n",
       "  (\"model_1/concatenate_1/concat:0.119\" (1; #23), 5.37988872747027),\n",
       "  (\"model_1/concatenate_1/concat:0.63\" (1; #81), 5.376982272269152),\n",
       "  (\"model_1/concatenate_1/concat:0.115\" (1; #19), 5.3767669792912915),\n",
       "  (\"model_1/concatenate_1/concat:0.88\" (1; #108), 5.375559389460738),\n",
       "  (\"model_1/concatenate_1/concat:0.79\" (1; #98), 5.372026546871359),\n",
       "  (\"model_1/concatenate_1/concat:0.57\" (1; #74), 5.370105471068908),\n",
       "  (\"model_1/concatenate_1/concat:0.104\" (1; #7), 5.366361152028025),\n",
       "  (\"model_1/concatenate_1/concat:0.83\" (1; #103), 5.366361152028023),\n",
       "  (\"model_1/concatenate_1/concat:0.48\" (1; #64), 5.366245531725096),\n",
       "  (\"model_1/concatenate_1/concat:0.86\" (1; #106), 5.366014291119251),\n",
       "  (\"model_1/concatenate_1/concat:0.71\" (1; #90), 5.36580617457398),\n",
       "  (\"model_1/concatenate_1/concat:0.82\" (1; #102), 5.36518900137078),\n",
       "  (\"model_1/concatenate_1/concat:0.117\" (1; #21), 5.363021009943517),\n",
       "  (\"model_1/concatenate_1/concat:0.22\" (1; #36), 5.361705507830233),\n",
       "  (\"model_1/concatenate_1/concat:0.99\" (1; #120), 5.3604200472315595),\n",
       "  (\"model_1/concatenate_1/concat:0.65\" (1; #83), 5.3603544779457115),\n",
       "  (\"model_1/concatenate_1/concat:0.75\" (1; #94), 5.358308049698127),\n",
       "  (\"model_1/concatenate_1/concat:0.58\" (1; #75), 5.356402389936066),\n",
       "  (\"model_1/concatenate_1/concat:0.55\" (1; #72), 5.35537722325013),\n",
       "  (\"model_1/concatenate_1/concat:0.67\" (1; #85), 5.352771319499586),\n",
       "  (\"model_1/concatenate_1/concat:0.100\" (1; #3), 5.352717956282851),\n",
       "  (\"model_1/concatenate_1/concat:0.32\" (1; #47), 5.352662458537446),\n",
       "  (\"model_1/concatenate_1/concat:0.13\" (1; #26), 5.352129621706929),\n",
       "  (\"model_1/concatenate_1/concat:0.93\" (1; #114), 5.348555625377543),\n",
       "  (\"model_1/concatenate_1/concat:0.17\" (1; #30), 5.347981829722474),\n",
       "  (\"model_1/concatenate_1/concat:0.87\" (1; #107), 5.346227271691049),\n",
       "  (\"model_1/concatenate_1/concat:0.109\" (1; #12), 5.345765322066949),\n",
       "  (\"model_1/concatenate_1/concat:0.103\" (1; #6), 5.343560828291173),\n",
       "  (\"model_1/concatenate_1/concat:0.9\" (1; #110), 5.34335271174591),\n",
       "  (\"model_1/concatenate_1/concat:0.68\" (1; #86), 5.3420269322723675),\n",
       "  (\"model_1/concatenate_1/concat:0.11\" (1; #13), 5.338018230183361),\n",
       "  (\"model_1/concatenate_1/concat:0.114\" (1; #18), 5.336482938747105),\n",
       "  (\"model_1/concatenate_1/concat:0.90\" (1; #111), 5.334777539278958),\n",
       "  (\"model_1/concatenate_1/concat:0.74\" (1; #93), 5.33444994842067),\n",
       "  (\"model_1/concatenate_1/concat:0.62\" (1; #80), 5.334163210069414),\n",
       "  (\"model_1/concatenate_1/concat:0.21\" (1; #35), 5.332276286725673),\n",
       "  (\"model_1/concatenate_1/concat:0.45\" (1; #61), 5.330387290190298),\n",
       "  (\"model_1/concatenate_1/concat:0.16\" (1; #29), 5.329593895697812),\n",
       "  (\"model_1/concatenate_1/concat:0.54\" (1; #71), 5.329247034789035),\n",
       "  (\"model_1/concatenate_1/concat:0.50\" (1; #67), 5.327859591153936),\n",
       "  (\"model_1/concatenate_1/concat:0.110\" (1; #14), 5.326819008427608),\n",
       "  (\"model_1/concatenate_1/concat:0.116\" (1; #20), 5.32092237297842),\n",
       "  (\"model_1/concatenate_1/concat:0.102\" (1; #5), 5.319053178081132),\n",
       "  (\"model_1/concatenate_1/concat:0.53\" (1; #70), 5.317871775755086),\n",
       "  (\"model_1/concatenate_1/concat:0.80\" (1; #100), 5.316413181164339),\n",
       "  (\"model_1/concatenate_1/concat:0.81\" (1; #101), 5.313861826479786),\n",
       "  (\"model_1/concatenate_1/concat:0.96\" (1; #117), 5.311780661027138),\n",
       "  (\"model_1/concatenate_1/concat:0.42\" (1; #58), 5.311179435451925),\n",
       "  (\"model_1/concatenate_1/concat:0.66\" (1; #84), 5.309013481777127),\n",
       "  (\"model_1/concatenate_1/concat:0.3\" (1; #44), 5.30791064196461),\n",
       "  (\"model_1/concatenate_1/concat:0.60\" (1; #78), 5.304350129559146),\n",
       "  (\"model_1/concatenate_1/concat:0.29\" (1; #43), 5.302004339799383),\n",
       "  (\"model_1/concatenate_1/concat:0.47\" (1; #63), 5.299648237240188),\n",
       "  (\"model_1/concatenate_1/concat:0.59\" (1; #76), 5.2995326169372605),\n",
       "  (\"model_1/concatenate_1/concat:0.14\" (1; #27), 5.299166485977995),\n",
       "  (\"model_1/concatenate_1/concat:0.97\" (1; #118), 5.297049092830427),\n",
       "  (\"model_1/concatenate_1/concat:0.108\" (1; #11), 5.296748835797601),\n",
       "  (\"model_1/concatenate_1/concat:0.92\" (1; #113), 5.293093762136431),\n",
       "  (\"model_1/concatenate_1/concat:0.8\" (1; #99), 5.292651879944213),\n",
       "  (\"model_1/concatenate_1/concat:0.43\" (1; #59), 5.2915922316241675),\n",
       "  (\"model_1/concatenate_1/concat:0.28\" (1; #42), 5.2899361317944695),\n",
       "  (\"model_1/concatenate_1/concat:0.98\" (1; #119), 5.28981280347135),\n",
       "  (\"model_1/concatenate_1/concat:0.112\" (1; #16), 5.286567726969252),\n",
       "  (\"model_1/concatenate_1/concat:0.111\" (1; #15), 5.278614077864021),\n",
       "  (\"model_1/concatenate_1/concat:0.101\" (1; #4), 5.275014414713364),\n",
       "  (\"model_1/concatenate_1/concat:0.1\" (1; #1), 5.273286808173241),\n",
       "  (\"model_1/concatenate_1/concat:0.41\" (1; #57), 5.272477466052764),\n",
       "  (\"model_1/concatenate_1/concat:0.72\" (1; #91), 5.269471338176709),\n",
       "  (\"model_1/concatenate_1/concat:0.35\" (1; #50), 5.267968274238685),\n",
       "  (\"model_1/concatenate_1/concat:0.78\" (1; #97), 5.26634958999773),\n",
       "  (\"model_1/concatenate_1/concat:0.12\" (1; #24), 5.265031518544383),\n",
       "  (\"model_1/concatenate_1/concat:0.56\" (1; #73), 5.264499665150927),\n",
       "  (\"model_1/concatenate_1/concat:0.6\" (1; #77), 5.26421856162823),\n",
       "  (\"model_1/concatenate_1/concat:0.2\" (1; #33), 5.263690323030451),\n",
       "  (\"model_1/concatenate_1/concat:0.38\" (1; #53), 5.262765360607053),\n",
       "  (\"model_1/concatenate_1/concat:0.85\" (1; #105), 5.260711173225078),\n",
       "  (\"model_1/concatenate_1/concat:0.52\" (1; #69), 5.258064797257093),\n",
       "  (\"model_1/concatenate_1/concat:0.15\" (1; #28), 5.255250040916911),\n",
       "  (\"model_1/concatenate_1/concat:0.70\" (1; #89), 5.255126712593792),\n",
       "  (\"model_1/concatenate_1/concat:0.26\" (1; #40), 5.252430684299429),\n",
       "  (\"model_1/concatenate_1/concat:0.113\" (1; #17), 5.251794772633341),\n",
       "  (\"model_1/concatenate_1/concat:0.61\" (1; #79), 5.251384962892969),\n",
       "  (\"model_1/concatenate_1/concat:0.37\" (1; #52), 5.2497714173321475),\n",
       "  (\"model_1/concatenate_1/concat:0.49\" (1; #65), 5.2484124954163365),\n",
       "  (\"model_1/concatenate_1/concat:0.107\" (1; #10), 5.239294439113237),\n",
       "  (\"model_1/concatenate_1/concat:0.5\" (1; #66), 5.23851640341324),\n",
       "  (\"model_1/concatenate_1/concat:0.36\" (1; #51), 5.238237908758921),\n",
       "  (\"model_1/concatenate_1/concat:0.51\" (1; #68), 5.2378001109314285),\n",
       "  (\"model_1/concatenate_1/concat:0.77\" (1; #96), 5.234322766087449),\n",
       "  (\"model_1/concatenate_1/concat:0.105\" (1; #8), 5.233932633948064),\n",
       "  (\"model_1/concatenate_1/concat:0.95\" (1; #116), 5.227481898164375),\n",
       "  (\"model_1/concatenate_1/concat:0.23\" (1; #37), 5.22426379973296),\n",
       "  (\"model_1/concatenate_1/concat:0.4\" (1; #55), 5.222475428641058),\n",
       "  (\"model_1/concatenate_1/concat:0.33\" (1; #48), 5.215129795801868),\n",
       "  (\"model_1/concatenate_1/concat:0.10\" (1; #2), 5.214245152253334),\n",
       "  (\"model_1/concatenate_1/concat:0.31\" (1; #46), 5.210772106453692),\n",
       "  (\"model_1/concatenate_1/concat:0.30\" (1; #45), 5.205944405069477),\n",
       "  (\"model_1/concatenate_1/concat:0.24\" (1; #38), 5.188575666230052),\n",
       "  (\"model_1/concatenate_1/concat:0.7\" (1; #88), 5.186668425335643),\n",
       "  (\"model_1/concatenate_1/concat:0.20\" (1; #34), 5.18079056583309),\n",
       "  (\"model_1/concatenate_1/concat:0.91\" (1; #112), 5.169094139291013),\n",
       "  (\"model_1/concatenate_1/concat:0.0\" (1; #0), 5.1594907366942),\n",
       "  (\"model_1/concatenate_1/concat:0.106\" (1; #9), 5.1559013686233905),\n",
       "  (\"model_1/concatenate_1/concat:0.94\" (1; #115), 5.15079352057416),\n",
       "  (\"model_1/concatenate_1/concat:0.27\" (1; #41), 5.141595283141444),\n",
       "  (\"model_1/concatenate_1/concat:0.76\" (1; #95), 5.126915951604675),\n",
       "  (\"model_1/concatenate_1/concat:0.46\" (1; #62), 5.1245858021062265),\n",
       "  (\"model_1/concatenate_1/concat:0.69\" (1; #87), 5.120330397027443),\n",
       "  (\"model_1/concatenate_1/concat:0.64\" (1; #82), 5.093959718336265),\n",
       "  (\"model_1/concatenate_1/concat:0.34\" (1; #49), 5.073395914483505),\n",
       "  (\"model_1/concatenate_1/concat:0.39\" (1; #54), 5.0089787956862475),\n",
       "  (\"model_1/concatenate_1/concat:0.44\" (1; #60), 4.967035603795083),\n",
       "  (\"model_1/concatenate_1/concat:0.118\" (1; #22), 4.37987277984228)],\n",
       " 'NUM_AS_ROOT': [(\"model_1/concatenate_1/concat:0.118\" (1; #22), 49.0),\n",
       "  (\"model_1/concatenate_1/concat:0.39\" (1; #54), 10.0),\n",
       "  (\"model_1/concatenate_1/concat:0.44\" (1; #60), 10.0),\n",
       "  (\"model_1/concatenate_1/concat:0.64\" (1; #82), 9.0),\n",
       "  (\"model_1/concatenate_1/concat:0.34\" (1; #49), 7.0),\n",
       "  (\"model_1/concatenate_1/concat:0.91\" (1; #112), 7.0),\n",
       "  (\"model_1/concatenate_1/concat:0.10\" (1; #2), 6.0),\n",
       "  (\"model_1/concatenate_1/concat:0.24\" (1; #38), 6.0),\n",
       "  (\"model_1/concatenate_1/concat:0.27\" (1; #41), 6.0),\n",
       "  (\"model_1/concatenate_1/concat:0.69\" (1; #87), 6.0),\n",
       "  (\"model_1/concatenate_1/concat:0.76\" (1; #95), 6.0),\n",
       "  (\"model_1/concatenate_1/concat:0.0\" (1; #0), 5.0),\n",
       "  (\"model_1/concatenate_1/concat:0.106\" (1; #9), 5.0),\n",
       "  (\"model_1/concatenate_1/concat:0.107\" (1; #10), 5.0),\n",
       "  (\"model_1/concatenate_1/concat:0.23\" (1; #37), 5.0),\n",
       "  (\"model_1/concatenate_1/concat:0.7\" (1; #88), 5.0),\n",
       "  (\"model_1/concatenate_1/concat:0.94\" (1; #115), 5.0),\n",
       "  (\"model_1/concatenate_1/concat:0.26\" (1; #40), 4.0),\n",
       "  (\"model_1/concatenate_1/concat:0.31\" (1; #46), 4.0),\n",
       "  (\"model_1/concatenate_1/concat:0.52\" (1; #69), 4.0),\n",
       "  (\"model_1/concatenate_1/concat:0.72\" (1; #91), 4.0),\n",
       "  (\"model_1/concatenate_1/concat:0.1\" (1; #1), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.112\" (1; #16), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.113\" (1; #17), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.12\" (1; #24), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.28\" (1; #42), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.3\" (1; #44), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.30\" (1; #45), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.35\" (1; #50), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.37\" (1; #52), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.4\" (1; #55), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.41\" (1; #57), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.46\" (1; #62), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.49\" (1; #65), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.70\" (1; #89), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.92\" (1; #113), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.95\" (1; #116), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.98\" (1; #119), 3.0),\n",
       "  (\"model_1/concatenate_1/concat:0.101\" (1; #4), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.105\" (1; #8), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.108\" (1; #11), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.111\" (1; #15), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.15\" (1; #28), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.29\" (1; #43), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.36\" (1; #51), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.38\" (1; #53), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.42\" (1; #58), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.47\" (1; #63), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.5\" (1; #66), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.50\" (1; #67), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.51\" (1; #68), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.53\" (1; #70), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.59\" (1; #76), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.61\" (1; #79), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.62\" (1; #80), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.66\" (1; #84), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.77\" (1; #96), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.78\" (1; #97), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.81\" (1; #101), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.85\" (1; #105), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.90\" (1; #111), 2.0),\n",
       "  (\"model_1/concatenate_1/concat:0.109\" (1; #12), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.110\" (1; #14), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.114\" (1; #18), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.119\" (1; #23), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.14\" (1; #27), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.2\" (1; #33), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.21\" (1; #35), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.43\" (1; #59), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.6\" (1; #77), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.65\" (1; #83), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.74\" (1; #93), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.80\" (1; #100), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.96\" (1; #117), 1.0),\n",
       "  (\"model_1/concatenate_1/concat:0.97\" (1; #118), 1.0)],\n",
       " 'SUM_SCORE': [(\"model_1/concatenate_1/concat:0.118\" (1; #22),\n",
       "   339968.79140111984),\n",
       "  (\"model_1/concatenate_1/concat:0.39\" (1; #54), 1199.8154038626499),\n",
       "  (\"model_1/concatenate_1/concat:0.69\" (1; #87), 768.688713449573),\n",
       "  (\"model_1/concatenate_1/concat:0.46\" (1; #62), 638.3910069802577),\n",
       "  (\"model_1/concatenate_1/concat:0.20\" (1; #34), 559.4384288608435),\n",
       "  (\"model_1/concatenate_1/concat:0.44\" (1; #60), 555.6989723892431),\n",
       "  (\"model_1/concatenate_1/concat:0.34\" (1; #49), 554.544040545075),\n",
       "  (\"model_1/concatenate_1/concat:0.33\" (1; #48), 518.2770003964179),\n",
       "  (\"model_1/concatenate_1/concat:0.56\" (1; #73), 435.6768150325662),\n",
       "  (\"model_1/concatenate_1/concat:0.64\" (1; #82), 433.2513361174276),\n",
       "  (\"model_1/concatenate_1/concat:0.76\" (1; #95), 419.4937069715911),\n",
       "  (\"model_1/concatenate_1/concat:0.7\" (1; #88), 417.95213616520687),\n",
       "  (\"model_1/concatenate_1/concat:0.2\" (1; #33), 408.9893494754615),\n",
       "  (\"model_1/concatenate_1/concat:0.0\" (1; #0), 407.89037118717965),\n",
       "  (\"model_1/concatenate_1/concat:0.95\" (1; #116), 402.69198560651125),\n",
       "  (\"model_1/concatenate_1/concat:0.106\" (1; #9), 387.65868266077916),\n",
       "  (\"model_1/concatenate_1/concat:0.94\" (1; #115), 379.3772361380404),\n",
       "  (\"model_1/concatenate_1/concat:0.51\" (1; #68), 358.84235895382835),\n",
       "  (\"model_1/concatenate_1/concat:0.30\" (1; #45), 348.42252521160367),\n",
       "  (\"model_1/concatenate_1/concat:0.4\" (1; #55), 327.73642485599066),\n",
       "  (\"model_1/concatenate_1/concat:0.105\" (1; #8), 321.3286808036971),\n",
       "  (\"model_1/concatenate_1/concat:0.61\" (1; #79), 321.3094944350046),\n",
       "  (\"model_1/concatenate_1/concat:0.27\" (1; #41), 312.78035192314974),\n",
       "  (\"model_1/concatenate_1/concat:0.6\" (1; #77), 309.37078272346093),\n",
       "  (\"model_1/concatenate_1/concat:0.31\" (1; #46), 306.6269257708891),\n",
       "  (\"model_1/concatenate_1/concat:0.15\" (1; #28), 299.67192615379304),\n",
       "  (\"model_1/concatenate_1/concat:0.5\" (1; #66), 292.2683956362565),\n",
       "  (\"model_1/concatenate_1/concat:0.35\" (1; #50), 272.7094822340664),\n",
       "  (\"model_1/concatenate_1/concat:0.37\" (1; #52), 270.15736330023674),\n",
       "  (\"model_1/concatenate_1/concat:0.101\" (1; #4), 267.79109822175405),\n",
       "  (\"model_1/concatenate_1/concat:0.91\" (1; #112), 267.0982559242334),\n",
       "  (\"model_1/concatenate_1/concat:0.8\" (1; #99), 263.18644054297783),\n",
       "  (\"model_1/concatenate_1/concat:0.47\" (1; #63), 261.9752253400818),\n",
       "  (\"model_1/concatenate_1/concat:0.77\" (1; #96), 259.31008310604307),\n",
       "  (\"model_1/concatenate_1/concat:0.70\" (1; #89), 253.67541217021721),\n",
       "  (\"model_1/concatenate_1/concat:0.85\" (1; #105), 251.81565647138643),\n",
       "  (\"model_1/concatenate_1/concat:0.60\" (1; #78), 251.57224645673705),\n",
       "  (\"model_1/concatenate_1/concat:0.12\" (1; #24), 242.00126454551014),\n",
       "  (\"model_1/concatenate_1/concat:0.1\" (1; #1), 241.964241569332),\n",
       "  (\"model_1/concatenate_1/concat:0.74\" (1; #93), 235.7175560135504),\n",
       "  (\"model_1/concatenate_1/concat:0.24\" (1; #38), 228.88694144045303),\n",
       "  (\"model_1/concatenate_1/concat:0.78\" (1; #97), 226.98450594678025),\n",
       "  (\"model_1/concatenate_1/concat:0.36\" (1; #51), 226.6241264385144),\n",
       "  (\"model_1/concatenate_1/concat:0.23\" (1; #37), 223.26072073060095),\n",
       "  (\"model_1/concatenate_1/concat:0.26\" (1; #40), 222.9488869481347),\n",
       "  (\"model_1/concatenate_1/concat:0.14\" (1; #27), 222.6195915198134),\n",
       "  (\"model_1/concatenate_1/concat:0.98\" (1; #119), 221.52198574039403),\n",
       "  (\"model_1/concatenate_1/concat:0.43\" (1; #59), 209.5739066160163),\n",
       "  (\"model_1/concatenate_1/concat:0.49\" (1; #65), 207.9650437488708),\n",
       "  (\"model_1/concatenate_1/concat:0.41\" (1; #57), 206.75334924850722),\n",
       "  (\"model_1/concatenate_1/concat:0.52\" (1; #69), 204.81858051994493),\n",
       "  (\"model_1/concatenate_1/concat:0.113\" (1; #17), 201.68251328956217),\n",
       "  (\"model_1/concatenate_1/concat:0.107\" (1; #10), 198.69069137752786),\n",
       "  (\"model_1/concatenate_1/concat:0.11\" (1; #13), 190.85811404492347),\n",
       "  (\"model_1/concatenate_1/concat:0.9\" (1; #110), 189.47111081522462),\n",
       "  (\"model_1/concatenate_1/concat:0.108\" (1; #11), 188.6085957940902),\n",
       "  (\"model_1/concatenate_1/concat:0.102\" (1; #5), 182.95713609534823),\n",
       "  (\"model_1/concatenate_1/concat:0.29\" (1; #43), 182.85903626873073),\n",
       "  (\"model_1/concatenate_1/concat:0.53\" (1; #70), 174.1054151107395),\n",
       "  (\"model_1/concatenate_1/concat:0.92\" (1; #113), 174.0321628583041),\n",
       "  (\"model_1/concatenate_1/concat:0.21\" (1; #35), 171.89445435737457),\n",
       "  (\"model_1/concatenate_1/concat:0.112\" (1; #16), 171.2929517399416),\n",
       "  (\"model_1/concatenate_1/concat:0.103\" (1; #6), 170.796115305252),\n",
       "  (\"model_1/concatenate_1/concat:0.54\" (1; #71), 168.8739037971145),\n",
       "  (\"model_1/concatenate_1/concat:0.45\" (1; #61), 168.55576706558327),\n",
       "  (\"model_1/concatenate_1/concat:0.104\" (1; #7), 166.47153495130897),\n",
       "  (\"model_1/concatenate_1/concat:0.72\" (1; #91), 161.11913679768008),\n",
       "  (\"model_1/concatenate_1/concat:0.116\" (1; #20), 159.03918149114634),\n",
       "  (\"model_1/concatenate_1/concat:0.13\" (1; #26), 157.25046160927286),\n",
       "  (\"model_1/concatenate_1/concat:0.68\" (1; #86), 156.4112209047414),\n",
       "  (\"model_1/concatenate_1/concat:0.80\" (1; #100), 154.38866481094374),\n",
       "  (\"model_1/concatenate_1/concat:0.100\" (1; #3), 151.84894182154585),\n",
       "  (\"model_1/concatenate_1/concat:0.10\" (1; #2), 151.65290568146986),\n",
       "  (\"model_1/concatenate_1/concat:0.16\" (1; #29), 151.14800160298705),\n",
       "  (\"model_1/concatenate_1/concat:0.81\" (1; #101), 150.29369295487913),\n",
       "  (\"model_1/concatenate_1/concat:0.59\" (1; #76), 149.73089003967152),\n",
       "  (\"model_1/concatenate_1/concat:0.109\" (1; #12), 148.6005147201745),\n",
       "  (\"model_1/concatenate_1/concat:0.99\" (1; #120), 147.4582165765953),\n",
       "  (\"model_1/concatenate_1/concat:0.38\" (1; #53), 146.06373808350713),\n",
       "  (\"model_1/concatenate_1/concat:0.22\" (1; #36), 146.05714074887783),\n",
       "  (\"model_1/concatenate_1/concat:0.110\" (1; #14), 145.69263887290617),\n",
       "  (\"model_1/concatenate_1/concat:0.111\" (1; #15), 145.47090443728507),\n",
       "  (\"model_1/concatenate_1/concat:0.17\" (1; #30), 142.4645533103161),\n",
       "  (\"model_1/concatenate_1/concat:0.83\" (1; #103), 141.17639417943428),\n",
       "  (\"model_1/concatenate_1/concat:0.86\" (1; #106), 139.75930644236678),\n",
       "  (\"model_1/concatenate_1/concat:0.82\" (1; #102), 138.41570240142914),\n",
       "  (\"model_1/concatenate_1/concat:0.42\" (1; #58), 136.69060497261353),\n",
       "  (\"model_1/concatenate_1/concat:0.50\" (1; #67), 135.82431509511844),\n",
       "  (\"model_1/concatenate_1/concat:0.32\" (1; #47), 135.79338788051564),\n",
       "  (\"model_1/concatenate_1/concat:0.66\" (1; #84), 134.90667969260835),\n",
       "  (\"model_1/concatenate_1/concat:0.89\" (1; #109), 133.99642926749766),\n",
       "  (\"model_1/concatenate_1/concat:0.97\" (1; #118), 133.51047709949444),\n",
       "  (\"model_1/concatenate_1/concat:0.48\" (1; #64), 129.82248359421965),\n",
       "  (\"model_1/concatenate_1/concat:0.73\" (1; #92), 128.81366537116264),\n",
       "  (\"model_1/concatenate_1/concat:0.67\" (1; #85), 128.28440419240542),\n",
       "  (\"model_1/concatenate_1/concat:0.114\" (1; #18), 124.77328407129039),\n",
       "  (\"model_1/concatenate_1/concat:0.96\" (1; #117), 123.89875515483527),\n",
       "  (\"model_1/concatenate_1/concat:0.55\" (1; #72), 123.28978186252425),\n",
       "  (\"model_1/concatenate_1/concat:0.71\" (1; #90), 121.82013649257988),\n",
       "  (\"model_1/concatenate_1/concat:0.93\" (1; #114), 119.2886633242556),\n",
       "  (\"model_1/concatenate_1/concat:0.40\" (1; #56), 117.98643953174746),\n",
       "  (\"model_1/concatenate_1/concat:0.117\" (1; #21), 117.90085907933599),\n",
       "  (\"model_1/concatenate_1/concat:0.19\" (1; #32), 116.47892328426133),\n",
       "  (\"model_1/concatenate_1/concat:0.3\" (1; #44), 116.46335999276221),\n",
       "  (\"model_1/concatenate_1/concat:0.28\" (1; #42), 115.8360423242134),\n",
       "  (\"model_1/concatenate_1/concat:0.65\" (1; #83), 114.5270376794897),\n",
       "  (\"model_1/concatenate_1/concat:0.87\" (1; #107), 112.6154905290798),\n",
       "  (\"model_1/concatenate_1/concat:0.63\" (1; #81), 105.39226722650528),\n",
       "  (\"model_1/concatenate_1/concat:0.90\" (1; #111), 105.3829866465087),\n",
       "  (\"model_1/concatenate_1/concat:0.88\" (1; #108), 104.99416035611193),\n",
       "  (\"model_1/concatenate_1/concat:0.57\" (1; #74), 102.45481659280267),\n",
       "  (\"model_1/concatenate_1/concat:0.79\" (1; #98), 98.64448233567964),\n",
       "  (\"model_1/concatenate_1/concat:0.58\" (1; #75), 95.90644103916998),\n",
       "  (\"model_1/concatenate_1/concat:0.115\" (1; #19), 95.17254485491776),\n",
       "  (\"model_1/concatenate_1/concat:0.75\" (1; #94), 94.08790638339542),\n",
       "  (\"model_1/concatenate_1/concat:0.84\" (1; #104), 93.05742101566284),\n",
       "  (\"model_1/concatenate_1/concat:0.18\" (1; #31), 85.9860207550264),\n",
       "  (\"model_1/concatenate_1/concat:0.62\" (1; #80), 83.86675301556284),\n",
       "  (\"model_1/concatenate_1/concat:0.25\" (1; #39), 81.10155803739144),\n",
       "  (\"model_1/concatenate_1/concat:0.119\" (1; #23), 56.76422450089012),\n",
       "  (\"model_1/concatenate_1/concat:0.120\" (1; #25), 54.19841369574715)]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_cell_height 300\n",
    "\n",
    "inspector.variable_importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "b2WKf-Zth_UN",
    "outputId": "265f83af-bb85-41f1-8660-f2792fc0bd34"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
       "<div id=\"tree_plot_7ed654ff63874141bae5cc26ca45b344\"></div>\n",
       "<script>\n",
       "/*\n",
       " * Copyright 2021 Google LLC.\n",
       " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       " * you may not use this file except in compliance with the License.\n",
       " * You may obtain a copy of the License at\n",
       " *\n",
       " *     https://www.apache.org/licenses/LICENSE-2.0\n",
       " *\n",
       " * Unless required by applicable law or agreed to in writing, software\n",
       " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       " * See the License for the specific language governing permissions and\n",
       " * limitations under the License.\n",
       " */\n",
       "\n",
       "/**\n",
       " *  Plotting of decision trees generated by TF-DF.\n",
       " *\n",
       " *  A tree is a recursive structure of node objects.\n",
       " *  A node contains one or more of the following components:\n",
       " *\n",
       " *    - A value: Representing the output of the node. If the node is not a leaf,\n",
       " *      the value is only present for analysis i.e. it is not used for\n",
       " *      predictions.\n",
       " *\n",
       " *    - A condition : For non-leaf nodes, the condition (also known as split)\n",
       " *      defines a binary test to branch to the positive or negative child.\n",
       " *\n",
       " *    - An explanation: Generally a plot showing the relation between the label\n",
       " *      and the condition to give insights about the effect of the condition.\n",
       " *\n",
       " *    - Two children : For non-leaf nodes, the children nodes. The first\n",
       " *      children (i.e. \"node.children[0]\") is the negative children (drawn in\n",
       " *      red). The second children is the positive one (drawn in green).\n",
       " *\n",
       " */\n",
       "\n",
       "/**\n",
       " * Plots a single decision tree into a DOM element.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!tree} raw_tree Recursive tree structure.\n",
       " * @param {string} canvas_id Id of the output dom element.\n",
       " */\n",
       "function display_tree(options, raw_tree, canvas_id) {\n",
       "  console.log(options);\n",
       "\n",
       "  // Determine the node placement.\n",
       "  const tree_struct = d3.tree().nodeSize(\n",
       "      [options.node_y_offset, options.node_x_offset])(d3.hierarchy(raw_tree));\n",
       "\n",
       "  // Boundaries of the node placement.\n",
       "  let x_min = Infinity;\n",
       "  let x_max = -x_min;\n",
       "  let y_min = Infinity;\n",
       "  let y_max = -x_min;\n",
       "\n",
       "  tree_struct.each(d => {\n",
       "    if (d.x > x_max) x_max = d.x;\n",
       "    if (d.x < x_min) x_min = d.x;\n",
       "    if (d.y > y_max) y_max = d.y;\n",
       "    if (d.y < y_min) y_min = d.y;\n",
       "  });\n",
       "\n",
       "  // Size of the plot.\n",
       "  const width = y_max - y_min + options.node_x_size + options.margin * 2;\n",
       "  const height = x_max - x_min + options.node_y_size + options.margin * 2 +\n",
       "      options.node_y_offset - options.node_y_size;\n",
       "\n",
       "  const plot = d3.select(canvas_id);\n",
       "\n",
       "  // Tool tip\n",
       "  options.tooltip = plot.append('div')\n",
       "                        .attr('width', 100)\n",
       "                        .attr('height', 100)\n",
       "                        .style('padding', '4px')\n",
       "                        .style('background', '#fff')\n",
       "                        .style('box-shadow', '4px 4px 0px rgba(0,0,0,0.1)')\n",
       "                        .style('border', '1px solid black')\n",
       "                        .style('font-family', 'sans-serif')\n",
       "                        .style('font-size', options.font_size)\n",
       "                        .style('position', 'absolute')\n",
       "                        .style('z-index', '10')\n",
       "                        .attr('pointer-events', 'none')\n",
       "                        .style('display', 'none');\n",
       "\n",
       "  // Create canvas\n",
       "  const svg = plot.append('svg').attr('width', width).attr('height', height);\n",
       "  const graph =\n",
       "      svg.style('overflow', 'visible')\n",
       "          .append('g')\n",
       "          .attr('font-family', 'sans-serif')\n",
       "          .attr('font-size', options.font_size)\n",
       "          .attr(\n",
       "              'transform',\n",
       "              () => `translate(${options.margin},${\n",
       "                  - x_min + options.node_y_offset / 2 + options.margin})`);\n",
       "\n",
       "  // Plot bounding box.\n",
       "  if (options.show_plot_bounding_box) {\n",
       "    svg.append('rect')\n",
       "        .attr('width', width)\n",
       "        .attr('height', height)\n",
       "        .attr('fill', 'none')\n",
       "        .attr('stroke-width', 1.0)\n",
       "        .attr('stroke', 'black');\n",
       "  }\n",
       "\n",
       "  // Draw the edges.\n",
       "  display_edges(options, graph, tree_struct);\n",
       "\n",
       "  // Draw the nodes.\n",
       "  display_nodes(options, graph, tree_struct);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Draw the nodes of the tree.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!graph} graph D3 search handle containing the graph.\n",
       " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
       " *     data, etc.).\n",
       " */\n",
       "function display_nodes(options, graph, tree_struct) {\n",
       "  const nodes = graph.append('g')\n",
       "                    .selectAll('g')\n",
       "                    .data(tree_struct.descendants())\n",
       "                    .join('g')\n",
       "                    .attr('transform', d => `translate(${d.y},${d.x})`);\n",
       "\n",
       "  nodes.append('rect')\n",
       "      .attr('x', 0.5)\n",
       "      .attr('y', 0.5)\n",
       "      .attr('width', options.node_x_size)\n",
       "      .attr('height', options.node_y_size)\n",
       "      .attr('stroke', 'lightgrey')\n",
       "      .attr('stroke-width', 1)\n",
       "      .attr('fill', 'white')\n",
       "      .attr('y', -options.node_y_size / 2);\n",
       "\n",
       "  // Brackets on the right of condition nodes without children.\n",
       "  non_leaf_node_without_children =\n",
       "      nodes.filter(node => node.data.condition != null && node.children == null)\n",
       "          .append('g')\n",
       "          .attr('transform', `translate(${options.node_x_size},0)`);\n",
       "\n",
       "  non_leaf_node_without_children.append('path')\n",
       "      .attr('d', 'M0,0 C 10,0 0,10 10,10')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.0)\n",
       "      .attr('stroke', '#F00');\n",
       "\n",
       "  non_leaf_node_without_children.append('path')\n",
       "      .attr('d', 'M0,0 C 10,0 0,-10 10,-10')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.0)\n",
       "      .attr('stroke', '#0F0');\n",
       "\n",
       "  const node_content = nodes.append('g').attr(\n",
       "      'transform',\n",
       "      `translate(0,${options.node_padding - options.node_y_size / 2})`);\n",
       "\n",
       "  node_content.append(node => create_node_element(options, node));\n",
       "}\n",
       "\n",
       "/**\n",
       " * Creates the D3 content for a single node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!node} node Node to draw.\n",
       " * @return {!d3} D3 content.\n",
       " */\n",
       "function create_node_element(options, node) {\n",
       "  // Output accumulator.\n",
       "  let output = {\n",
       "    // Content to draw.\n",
       "    content: d3.create('svg:g'),\n",
       "    // Vertical offset to the next element to draw.\n",
       "    vertical_offset: 0\n",
       "  };\n",
       "\n",
       "  // Conditions.\n",
       "  if (node.data.condition != null) {\n",
       "    display_condition(options, node.data.condition, output);\n",
       "  }\n",
       "\n",
       "  // Values.\n",
       "  if (node.data.value != null) {\n",
       "    display_value(options, node.data.value, output);\n",
       "  }\n",
       "\n",
       "  // Explanations.\n",
       "  if (node.data.explanation != null) {\n",
       "    display_explanation(options, node.data.explanation, output);\n",
       "  }\n",
       "\n",
       "  return output.content.node();\n",
       "}\n",
       "\n",
       "\n",
       "/**\n",
       " * Adds a single line of text inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {string} text Text to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_node_text(options, text, output) {\n",
       "  output.content.append('text')\n",
       "      .attr('x', options.node_padding)\n",
       "      .attr('y', output.vertical_offset)\n",
       "      .attr('alignment-baseline', 'hanging')\n",
       "      .text(text);\n",
       "  output.vertical_offset += 10;\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a single line of text inside of a node with a tooltip.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {string} text Text to display.\n",
       " * @param {string} tooltip Text in the Tooltip.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_node_text_with_tooltip(options, text, tooltip, output) {\n",
       "  const item = output.content.append('text')\n",
       "                   .attr('x', options.node_padding)\n",
       "                   .attr('alignment-baseline', 'hanging')\n",
       "                   .text(text);\n",
       "\n",
       "  add_tooltip(options, item, () => tooltip);\n",
       "  output.vertical_offset += 10;\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a tooltip to a dom element.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!dom} target Dom element to equip with a tooltip.\n",
       " * @param {!func} get_content Generates the html content of the tooltip.\n",
       " */\n",
       "function add_tooltip(options, target, get_content) {\n",
       "  function show(d) {\n",
       "    options.tooltip.style('display', 'block');\n",
       "    options.tooltip.html(get_content());\n",
       "  }\n",
       "\n",
       "  function hide(d) {\n",
       "    options.tooltip.style('display', 'none');\n",
       "  }\n",
       "\n",
       "  function move(d) {\n",
       "    options.tooltip.style('display', 'block');\n",
       "    options.tooltip.style('left', (d.pageX + 5) + 'px');\n",
       "    options.tooltip.style('top', d.pageY + 'px');\n",
       "  }\n",
       "\n",
       "  target.on('mouseover', show);\n",
       "  target.on('mouseout', hide);\n",
       "  target.on('mousemove', move);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a condition inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!condition} condition Condition to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_condition(options, condition, output) {\n",
       "  threshold_format = d3.format('r');\n",
       "\n",
       "  if (condition.type === 'IS_MISSING') {\n",
       "    display_node_text(options, `${condition.attribute} is missing`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'IS_TRUE') {\n",
       "    display_node_text(options, `${condition.attribute} is true`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'NUMERICAL_IS_HIGHER_THAN') {\n",
       "    format = d3.format('r');\n",
       "    display_node_text(\n",
       "        options,\n",
       "        `${condition.attribute} >= ${threshold_format(condition.threshold)}`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'CATEGORICAL_IS_IN') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `${condition.attribute} in [...]`,\n",
       "        `${condition.attribute} in [${condition.mask}]`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'CATEGORICAL_SET_CONTAINS') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `${condition.attribute} intersect [...]`,\n",
       "        `${condition.attribute} intersect [${condition.mask}]`, output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (condition.type === 'NUMERICAL_SPARSE_OBLIQUE') {\n",
       "    display_node_text_with_tooltip(\n",
       "        options, `Sparse oblique split...`,\n",
       "        `[${condition.attributes}]*[${condition.weights}]>=${\n",
       "            threshold_format(condition.threshold)}`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  display_node_text(\n",
       "      options, `Non supported condition ${condition.type}`, output);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds a value inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!value} value Value to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_value(options, value, output) {\n",
       "  if (value.type === 'PROBABILITY') {\n",
       "    const left_margin = 0;\n",
       "    const right_margin = 50;\n",
       "    const plot_width = options.node_x_size - options.node_padding * 2 -\n",
       "        left_margin - right_margin;\n",
       "\n",
       "    let cusum = Array.from(d3.cumsum(value.distribution));\n",
       "    cusum.unshift(0);\n",
       "    const distribution_plot = output.content.append('g').attr(\n",
       "        'transform', `translate(0,${output.vertical_offset + 0.5})`);\n",
       "\n",
       "    distribution_plot.selectAll('rect')\n",
       "        .data(value.distribution)\n",
       "        .join('rect')\n",
       "        .attr('height', 10)\n",
       "        .attr(\n",
       "            'x',\n",
       "            (d, i) =>\n",
       "                (cusum[i] * plot_width + left_margin + options.node_padding))\n",
       "        .attr('width', (d, i) => d * plot_width)\n",
       "        .style('fill', (d, i) => d3.schemeSet1[i]);\n",
       "\n",
       "    const num_examples =\n",
       "        output.content.append('g')\n",
       "            .attr('transform', `translate(0,${output.vertical_offset})`)\n",
       "            .append('text')\n",
       "            .attr('x', options.node_x_size - options.node_padding)\n",
       "            .attr('alignment-baseline', 'hanging')\n",
       "            .attr('text-anchor', 'end')\n",
       "            .text(`(${value.num_examples})`);\n",
       "\n",
       "    const distribution_details = d3.create('ul');\n",
       "    distribution_details.selectAll('li')\n",
       "        .data(value.distribution)\n",
       "        .join('li')\n",
       "        .append('span')\n",
       "        .text(\n",
       "            (d, i) =>\n",
       "                'class ' + i + ': ' + d3.format('.3%')(value.distribution[i]));\n",
       "\n",
       "    add_tooltip(options, distribution_plot, () => distribution_details.html());\n",
       "    add_tooltip(options, num_examples, () => 'Number of examples');\n",
       "\n",
       "    output.vertical_offset += 10;\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  if (value.type === 'REGRESSION') {\n",
       "    display_node_text(\n",
       "        options,\n",
       "        'value: ' + d3.format('r')(value.value) + ` (` +\n",
       "            d3.format('.6')(value.num_examples) + `)`,\n",
       "        output);\n",
       "    return;\n",
       "  }\n",
       "\n",
       "  display_node_text(options, `Non supported value ${value.type}`, output);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Adds an explanation inside of a node.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!explanation} explanation Explanation to display.\n",
       " * @param {!output} output Output display accumulator.\n",
       " */\n",
       "function display_explanation(options, explanation, output) {\n",
       "  // Margin before the explanation.\n",
       "  output.vertical_offset += 10;\n",
       "\n",
       "  display_node_text(\n",
       "      options, `Non supported explanation ${explanation.type}`, output);\n",
       "}\n",
       "\n",
       "\n",
       "/**\n",
       " * Draw the edges of the tree.\n",
       " * @param {!options} options Dictionary of configurations.\n",
       " * @param {!graph} graph D3 search handle containing the graph.\n",
       " * @param {!tree_struct} tree_struct Structure of the tree (node placement,\n",
       " *     data, etc.).\n",
       " */\n",
       "function display_edges(options, graph, tree_struct) {\n",
       "  // Draw an edge between a parent and a child node with a bezier.\n",
       "  function draw_single_edge(d) {\n",
       "    return 'M' + (d.source.y + options.node_x_size) + ',' + d.source.x + ' C' +\n",
       "        (d.source.y + options.node_x_size + options.edge_rounding) + ',' +\n",
       "        d.source.x + ' ' + (d.target.y - options.edge_rounding) + ',' +\n",
       "        d.target.x + ' ' + d.target.y + ',' + d.target.x;\n",
       "  }\n",
       "\n",
       "  graph.append('g')\n",
       "      .attr('fill', 'none')\n",
       "      .attr('stroke-width', 1.2)\n",
       "      .selectAll('path')\n",
       "      .data(tree_struct.links())\n",
       "      .join('path')\n",
       "      .attr('d', draw_single_edge)\n",
       "      .attr(\n",
       "          'stroke', d => (d.target === d.source.children[0]) ? '#0F0' : '#F00');\n",
       "}\n",
       "\n",
       "display_tree({\"margin\": 10, \"node_x_size\": 160, \"node_y_size\": 28, \"node_x_offset\": 180, \"node_y_offset\": 33, \"font_size\": 10, \"edge_rounding\": 20, \"node_padding\": 2, \"show_plot_bounding_box\": false}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.0, \"num_examples\": 775804.0, \"standard_deviation\": 0.4999979619924199}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.118\", \"threshold\": 0.5}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.08733168244361877, \"num_examples\": 484580.0, \"standard_deviation\": 0.45050500391187576}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.118\", \"threshold\": 1.5}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.1028747707605362, \"num_examples\": 375200.0, \"standard_deviation\": 0.42963752419689}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.34\", \"threshold\": -85327494512640.0}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.1036636233329773, \"num_examples\": 354632.0, \"standard_deviation\": 0.4284573493676016}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.118\", \"threshold\": 3.5}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.08926175534725189, \"num_examples\": 20568.0, \"standard_deviation\": 0.44813608911060215}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.95\", \"threshold\": 84.29150390625}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.03401292487978935, \"num_examples\": 109380.0, \"standard_deviation\": 0.4929602162692135}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.20\", \"threshold\": -89.63700103759766}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": 0.03253459557890892, \"num_examples\": 99703.0, \"standard_deviation\": 0.49357271460973323}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.2\", \"threshold\": 197.7050018310547}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": 0.049219511449337006, \"num_examples\": 9677.0, \"standard_deviation\": 0.4849679561230729}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.44\", \"threshold\": -0.0018669499550014734}}]}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14531490206718445, \"num_examples\": 291224.0, \"standard_deviation\": 0.34202685649030556}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.33\", \"threshold\": 0.0005893349880352616}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14835745096206665, \"num_examples\": 108996.0, \"standard_deviation\": 0.3337269167745342}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.51\", \"threshold\": 56.39849853515625}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14883308112621307, \"num_examples\": 105459.0, \"standard_deviation\": 0.33239535930249264}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.7\", \"threshold\": 656170.0}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.13410840928554535, \"num_examples\": 3537.0, \"standard_deviation\": 0.36948660877102024}}]}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14349374175071716, \"num_examples\": 182228.0, \"standard_deviation\": 0.34681660337981285}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.44\", \"threshold\": 0.0007854950381442904}, \"children\": [{\"value\": {\"type\": \"REGRESSION\", \"value\": -0.14454403519630432, \"num_examples\": 150857.0, \"standard_deviation\": 0.34406828975858006}, \"condition\": {\"type\": \"NUMERICAL_IS_HIGHER_THAN\", \"attribute\": \"model_1/concatenate_1/concat:0.2\", \"threshold\": 305.864990234375}}, {\"value\": {\"type\": \"REGRESSION\", \"value\": -0.13843542337417603, \"num_examples\": 31371.0, \"standard_deviation\": 0.35947166496869254}}]}]}]}, \"#tree_plot_7ed654ff63874141bae5cc26ca45b344\")\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(model_2, tree_idx=0, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfOWIkEPh_UN"
   },
   "source": [
    "### Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "TbMOstUCh_UN",
    "outputId": "42151398-c445-41d8-8eb4-dfd80c5fd661"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logs = inspector.training_logs()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Accuracy (out-of-bag)')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Logloss (out-of-bag)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8BULXPWh_UO"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's evaluate the model using the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUJlvjQIh_UO",
    "outputId": "9e90b39c-12bd-4c45-f6f8-a2e82c2a3f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 16s 131ms/step - loss: 0.0000e+00 - auc_2: 0.8150\n",
      "loss: 0.0\n",
      "auc_2: 0.8149550557136536\n"
     ]
    }
   ],
   "source": [
    "evaluation = model_2.evaluate(valid_ds, return_dict = True)\n",
    "for name, value in evaluation.items():\n",
    "    print(\"{}: {}\".format(name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F_OSUlrh_UP"
   },
   "source": [
    "This model as you noticed created the additional features differently. While the pandas based method counts the nan values and calculates std and var on the whole training dataset, this model calculates and counts them for each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HusgjngbKZIE"
   },
   "source": [
    "## Test Set Prediction\n",
    "\n",
    "Although the [Tabular Playground Series - Sep 2021](https://www.kaggle.com/c/tabular-playground-series-sep-2021) has officially ended, you can still perform a Late Submission and compare the results with others on the leaderboard, especially if you want to tweak the notebook and apply some changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MXRh6KpKRKT",
    "outputId": "f1e1d850-f79c-4947-a3ee-78991f37ddfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1\n"
     ]
    }
   ],
   "source": [
    "#@title Select which model to use for prediction\n",
    "\n",
    "model = model_1 #@param [\"model_1\", \"model_2\"] {type: \"raw\"}\n",
    "\n",
    "if model == model_1:\n",
    "    print(\"model_1\")\n",
    "else:\n",
    "    print(\"model_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "MJsb6PDlL0mQ"
   },
   "outputs": [],
   "source": [
    "test_file_path = os.path.join(DOWNLOAD_LOCATION, \"test.csv\")\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "ids = test_data.pop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "CrLgttvYOr8h"
   },
   "outputs": [],
   "source": [
    "if model == model_1:\n",
    "    test_data['nan'] = test_data[features].isnull().sum(axis=1)\n",
    "    test_data['std'] = test_data[features].std(axis=1)\n",
    "    test_data['var'] = test_data[features].var(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Rxmtu2qaL3DM"
   },
   "outputs": [],
   "source": [
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP9kEc9hL7kW",
    "outputId": "0957d4ea-f863-412a-cbe1-72dc1fda0b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494/494 [==============================] - 60s 119ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mLc3Hk7lL-f9",
    "outputId": "7317438d-e2ab-4f51-eda5-ce80be565fe6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7975caa4-56de-4e50-b08d-ff4685aec283\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>957919</td>\n",
       "      <td>0.630406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>957920</td>\n",
       "      <td>0.115652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>957921</td>\n",
       "      <td>0.620445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>957922</td>\n",
       "      <td>0.120009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>957923</td>\n",
       "      <td>0.137160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7975caa4-56de-4e50-b08d-ff4685aec283')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7975caa4-56de-4e50-b08d-ff4685aec283 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7975caa4-56de-4e50-b08d-ff4685aec283');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       id     claim\n",
       "0  957919  0.630406\n",
       "1  957920  0.115652\n",
       "2  957921  0.620445\n",
       "3  957922  0.120009\n",
       "4  957923  0.137160"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'id': ids,\n",
    "                       'claim': preds.squeeze()})\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "h5M4vOopMAuf"
   },
   "outputs": [],
   "source": [
    "output_filename = \"test_prediction_output.csv\"\n",
    "output.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "N87Fs6NBPjhp",
    "outputId": "e8d85d7e-6823-44f7-cad9-cab30008f92c"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_d0211dc7-8566-4940-bbf8-29020e855713\", \"test_prediction_output.csv\", 9122713)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('test_prediction_output.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPnTl4WQQHz"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "*   [KerasTuner + TF Decision Forest](https://www.kaggle.com/ekaterinadranitsyna/kerastuner-tf-decision-forest?linkId=133421702) by [Ekaterina Dranitsyna](https://www.kaggle.com/ekaterinadranitsyna)\n",
    "*   [TensorFlow Decision Forests tutorials](https://www.tensorflow.org/decision_forests/tutorials) which are a set of 3 very interesting (beginner, intermediate and advanced levels) tutorials.\n",
    "*   The [TensorFlow Forum](https://discuss.tensorflow.org/) where one can get in touch with the TensorFlow community. Check it out if you haven't yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
